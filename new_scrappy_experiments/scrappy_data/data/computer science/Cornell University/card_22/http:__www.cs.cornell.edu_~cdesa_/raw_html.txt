"<html><head> \n    <meta http-equiv=\"Content-type\" content=\"text/html;charset=UTF-8\"> \n    <title>Chris De Sa</title> \n\n    <!-- copied this email-hiding script from Silias Boyd-Wickizer\n         https://pdos.csail.mit.edu/~sbw/ -->\n    <script type=\"text/javascript\">\n      function fill_email() {\n      document.getElementById(\"email\").innerHTML =\n        \"cd\" + \"esa@cs.cor\" + \"nell.edu\";\n        var cns = document.getElementsByClassName(\"conferencename\");\n        for (i = 0; i < cns.length; i++) {\n          cns[i].classList.add((i % 2 == 0) ? \"conferencenameblue\" : \"conferencenamered\");\n        }\n      }\n\n      function toggle_abstract(elem) {\n        var as = elem.parentNode.parentNode.getElementsByClassName(\"abstract\");\n        if (as.length > 0) {\n          var a = as[0];\n          a.className = \"abstract-show\"\n        }\n        else {\n          as = elem.parentNode.parentNode.getElementsByClassName(\"abstract-show\");\n          var a = as[0];\n          a.className = \"abstract\"\n        }\n      }\n\n      function show_all_abstracts() {\n        var as = [...document.getElementsByClassName(\"abstract\")];\n        for (i = 0; i < as.length; i+=1) {\n          var a = as[i];\n          a.className = \"abstract-show\";\n        }\n      }\n\n      function hide_all_abstracts() {\n        var as = [...document.getElementsByClassName(\"abstract-show\")];\n        for (i = 0; i < as.length; i+=1) {\n          var a = as[i];\n          a.className = \"abstract\";\n        }\n      }\n    </script>\n    \n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML\" id=\"\">\n    </script>\n\n    <style>\n      body {\n        font-family: Garamond, Georgia, serif;\n        width: 986px;\n        margin: auto;\n        background-color: #EEF3FF;\n      }\n\n      #blog {\n        margin-top: 0px;\n        margin-bottom: 30px;\n        margin-left: 20px;\n        margin-right: 20px;\n        line-height: 22px;\n      }\n\n      #teaching {\n        margin-top: 0px;\n        margin-bottom: 30px;\n        margin-left: 20px;\n        margin-right: 20px;\n        line-height: 22px;\n      }\n\n      #pubs {\n        margin-top: 30px;\n        margin-bottom: 0px;\n        margin-left: 20px;\n        margin-right: 20px;\n        line-height: 22px;\n      }\n\n      #headertab {\n        border: 0;\n        padding: 0px;\n      }\n\n      #headertab td {\n        padding: 10px;\n        vertical-align: top;\n      }\n\n      .coursetitle {\n        font-weight: bold;\n      }\n\n      .papertitle {\n        font-weight: bold;\n      }\n\n      .abstract {\n        display: none;\n      }\n\n      .description {\n        text-align: justify;\n      }\n\n      .abstract-show {\n        margin: 10px;\n        padding: 7px;\n        border: 1px dotted #A09040;\n        text-align: justify;\n      }\n\n      .bestpaper {\n        font-weight: normal;\n        background: #99DD77;\n        padding: 3px;\n        margin-left: 6px;\n      }\n\n      .papers {\n        border-collapse: separate;\n        border-spacing: 8px 2px;\n      }\n\n      .conferencename {\n        padding: 4px;\n      }\n\n      .conferencenameblue {\n        background-color: #1034A6;\n      }\n\n      .conferencenamered {\n        background-color: #800020;\n      }\n\n      .conferencename div {\n        writing-mode: vertical-lr;\n        transform: rotate(180deg);\n        font-weight: normal;\n        color: white;\n      }\n\n      img {\n        margin-top: 16px;\n      }\n    </style>\n  <style type=\"text/css\">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}\n.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}\n.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}\n.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}\n.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}\n</style><style type=\"text/css\">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n#MathJax_About.MathJax_MousePost {outline: none}\n.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n.MathJax_MenuItem {padding: 1px 2em; background: transparent}\n.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}\n.MathJax_MenuActive .MathJax_MenuArrow {color: white}\n.MathJax_MenuArrow.RTL {left: .5em; right: auto}\n.MathJax_MenuCheck {position: absolute; left: .7em}\n.MathJax_MenuCheck.RTL {right: .7em; left: auto}\n.MathJax_MenuRadioCheck {position: absolute; left: .7em}\n.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}\n.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}\n.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}\n.MathJax_MenuDisabled {color: GrayText}\n.MathJax_MenuActive {background-color: #606872; color: white}\n.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}\n.MathJax_ContextMenu:focus {outline: none}\n.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}\n#MathJax_AboutClose {top: .2em; right: .2em}\n.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}\n.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}\n.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}\n.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}\n.MathJax_MenuClose:hover span {background-color: #CCC!important}\n.MathJax_MenuClose:hover:focus {outline: none}\n</style><style type=\"text/css\">.MathJax_Preview .MJXf-math {color: inherit!important}\n</style><style type=\"text/css\">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}\n.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}\n</style><style type=\"text/css\">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}\n#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}\n#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}\n</style><style type=\"text/css\">.MathJax_Preview {color: #888}\n#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}\n#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}\n.MathJax_Error {color: #CC0000; font-style: italic}\n</style><style type=\"text/css\">.MJXp-script {font-size: .8em}\n.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}\n.MJXp-bold {font-weight: bold}\n.MJXp-italic {font-style: italic}\n.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-largeop {font-size: 150%}\n.MJXp-largeop.MJXp-int {vertical-align: -.2em}\n.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}\n.MJXp-display {display: block; text-align: center; margin: 1em 0}\n.MJXp-math span {display: inline-block}\n.MJXp-box {display: block!important; text-align: center}\n.MJXp-box:after {content: \" \"}\n.MJXp-rule {display: block!important; margin-top: .1em}\n.MJXp-char {display: block!important}\n.MJXp-mo {margin: 0 .15em}\n.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}\n.MJXp-denom {display: inline-table!important; width: 100%}\n.MJXp-denom > * {display: table-row!important}\n.MJXp-surd {vertical-align: top}\n.MJXp-surd > * {display: block!important}\n.MJXp-script-box > *  {display: table!important; height: 50%}\n.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}\n.MJXp-script-box > *:last-child > * {vertical-align: bottom}\n.MJXp-script-box > * > * > * {display: block!important}\n.MJXp-mphantom {visibility: hidden}\n.MJXp-munderover {display: inline-table!important}\n.MJXp-over {display: inline-block!important; text-align: center}\n.MJXp-over > * {display: block!important}\n.MJXp-munderover > * {display: table-row!important}\n.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}\n.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}\n.MJXp-mtr {display: table-row!important}\n.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}\n.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}\n.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}\n.MJXp-mlabeledtr {display: table-row!important}\n.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}\n.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}\n.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}\n.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}\n.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}\n.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}\n.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}\n.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}\n.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}\n.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}\n.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}\n.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}\n.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}\n.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}\n</style></head>\n  \n  <body onload=\"fill_email()\"><div id=\"MathJax_Message\" style=\"\">Processing math: 100%</div>\n    <table id=\"headertab\">\n      <tbody><tr>\n        <td><img src=\"img/chris.desa2019.jpg\"></td>\n        <td>\n          <h1>Chris De Sa</h1>\n          <p><span id=\"email\">cdesa@cs.cornell.edu</span> \u2014 Gates Hall, Room 450</p>\n\n          <p class=\"description\">\n            I am an Assistant Professor in the Computer Science\n            department at Cornell University.\n            I am a member of the <a href=\"http://machinelearning.cis.cornell.edu/index.php\">Cornell Machine Learning Group</a>.\n            My research interests include algorithmic, software, and hardware techniques for high-performance machine learning, with a focus on relaxed-consistency variants of stochastic algorithms such as asynchronous and low-precision stochastic gradient descent (SGD) and Markov chain Monte Carlo. My work builds towards using these techniques to construct data analytics and machine learning frameworks, including for deep learning, that are efficient, parallel, and distributed.\n          </p>\n\n          <p class=\"description\">\n            I graduated from Stanford University in 2017, where I was advised by\n            <a href=\"http://arsenalfc.stanford.edu/kunle\">Kunle Olukotun</a>\n            and by\n            <a href=\"http://cs.stanford.edu/people/chrismre/\">Chris R\u200c\u00e9</a>.\n          </p>\n        </td>\n      </tr>\n    </tbody></table>\n\n    <div id=\"teaching\">\n    <h2>Teaching</h2>\n\n    <p><span class=\"coursetitle\">CS 4780</span> <a href=\"http://www.cs.cornell.edu/courses/cs4780/2018sp/\">Machine Learning</a> (Spring 2018)</p>\n    <p><span class=\"coursetitle\">CS 4787</span> <a href=\"http://www.cs.cornell.edu/courses/cs4787/2019sp/\">Principles of Large-Scale Machine Learning</a> (Spring 2019)</p>\n    <p><span class=\"coursetitle\">CS 6787</span> <a href=\"http://www.cs.cornell.edu/courses/cs6787/2019fa/\">Advanced Machine Learning Systems</a> (Fall 2018, <a href=\"http://www.cs.cornell.edu/courses/cs6787/2018fa/\">Fall 2018</a>, <a href=\"http://www.cs.cornell.edu/courses/cs6787/2017fa/\">Fall 2017</a>)</p>\n\n    <p><span class=\"coursetitle\">Office Hours</span> Wednesdays 2:00-3:00 PM in Gates 450.</p>\n\n    </div>\n    \n    <div id=\"pubs\">\n    <h2>Publications</h2>\n    \n    <p><a href=\"papers/cdesa-cv.pdf\">CV</a> \u2014 <a href=\"https://scholar.google.com/citations?user=v7EjGHkAAAAJ\">Google Scholar</a> \u2014 <a href=\"javascript:void(0)\" onclick=\"show_all_abstracts()\">Show All Abstracts</a> \u2014 <a href=\"javascript:void(0)\" onclick=\"hide_all_abstracts()\">Hide All Abstracts</a></p>\n\n    <table class=\"papers\">\n      <tbody><tr><td rowspan=\"4\" class=\"conferencename conferencenameblue\"><div>NeurIPS 2019</div></td><td>\n        <div class=\"papertitle\">\n          Channel Gating Neural Networks\n        </div>\n        <div class=\"authors\">\n          Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, G. Edward Suh\n        </div>\n        <div class=\"journal\">\n          In <i>NeurIPS: Proceedings of the 32th Neural Information Processing Systems Conference</i>, December 2019. (to appear)\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1805.12549\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          This paper introduces channel gating, a dynamic, fine-grained, and highly hardware-efficient pruning scheme to reduce the compute cost for convolutional neural networks (CNNs). Channel gating identifies regions in the features that contribute less to the classification result, and skips the computation on a subset of the input channels for these ineffective regions. Unlike static network pruning, channel gating optimizes CNN inference at run-time by exploiting input-specific characteristics, which allows substantially reducing the compute cost with almost no accuracy loss. We experimentally show that applying channel gating in state-of-the-art networks achieves a 2.7-8.0x reduction in FLOPs with minimal accuracy loss on CIFAR-10. Combining our method with knowledge distillation reduces the compute cost of ResNet-18 by 2.6x without accuracy degradation on ImageNet. We further demonstrate that channel gating can be realized in hardware in an efficient manner. Our approach exhibits sparsity patterns that are well-suited to dense systolic arrays with minimal additional hardware. We have designed an accelerator for channel gating networks, which can be implemented using either FPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our accelerator achieves an encouraging speedup of 2.4x on average, with a theoretical FLOP reduction of 2.8x.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Numerically Accurate Hyperbolic Embeddings Using Tiling-Based Models\n          <span class=\"bestpaper\">Spotlight</span>\n        </div>\n        <div class=\"authors\">\n          Tao Yu, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>NeurIPS: Proceedings of the 32th Neural Information Processing Systems Conference</i>, December 2019. (to appear)\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n        </div>\n        <div class=\"abstract\">\n          Hyperbolic embeddings achieve excellent performance when embedding hierarchical data structures like synonym or type hierarchies, but they can be limited by numerical error when ordinary floating point numbers are used to represent points in hyperbolic space.\n          Standard models such as the Poincar\u00e9 disk and the Lorentz model have unbounded error as points get far from the origin.\n          To address this, we propose a new model with which uses an integer-based tiling to represent <i>any</i> point in the space with provably bounded numerical error. This allows us to learn high-precision embeddings without using BigFloats, and enables us to store the resulting embeddings with fewer bits. We evaluate our tiling-based model empirically, and show that it can both compress hyperbolic embeddings (down to 2% of a Poincar\u00e9 embedding on WordNet) and learn more accurate embeddings on real-world datasets.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Poisson-Minibatching for Gibbs Sampling with Convergence Rate Guarantees\n          <span class=\"bestpaper\">Spotlight</span>\n        </div>\n        <div class=\"authors\">\n          Ruqi Zhang, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>NeurIPS: Proceedings of the 32th Neural Information Processing Systems Conference</i>, December 2019. (to appear)\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n        </div>\n        <div class=\"abstract\">\n          Gibbs sampling is a Markov chain Monte Carlo method that is often used for learning and inference on graphical models.\n          Minibatching, in which a small random subset of the graph is used at each iteration, can help make Gibbs sampling scale to large graphical models by reducing its computational cost.\n          In this paper, we propose a new auxiliary-variable minibatched Gibbs sampling method, <i>Poisson-minibatching Gibbs</i>, which both produces unbiased samples and has a theoretical guarantee on its convergence rate.\n          In comparison to previous minibatched Gibbs algorithms, Poisson-minibatching Gibbs supports fast sampling from continuous state spaces and avoids the need for a Metropolis-Hastings correction on discrete state spaces.\n          We demonstrate the effectiveness of our method on multiple applications and in comparison with both plain Gibbs and previous minibatched methods.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Dimension-Free Bounds for Low-Precision Training\n        </div>\n        <div class=\"authors\">\n          Zheng Li, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>NeurIPS: Proceedings of the 32th Neural Information Processing Systems Conference</i>, December 2019. (to appear)\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n        </div>\n        <div class=\"abstract\">\n          Low-precision training is a promising way of decreasing the time and energy cost of training machine learning models.\n          Previous work has analyzed low-precision training algorithms, such as low-precision stochastic gradient descent, and derived theoretical bounds on their convergence rates.\n          These bounds tend to depend on the dimension of the model d in that the number of bits needed to achieve a particular error bound increases as d increases.\n          In this paper, we derive new bounds for low-precision training algorithms that do not contain the dimension d, which lets us better understand what affects the convergence of these algorithms as parameters scale.\n          Our methods also generalize naturally to let us prove new convergence bounds on low-precision training with other quantization schemes, such as low-precision floating-point computation and logarithmic quantization.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>EMC2@NeurIPS 2019</div></td><td>\n        <div class=\"papertitle\">\n          QPyTorch: A Low-Precision Arithmetic Simulation Framework\n        </div>\n        <div class=\"authors\">\n          Tianyi Zhang, Zhiqiu Lin, Guandao Yang, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>EMC<sup>2</sup>: Workshop on Energy Efficient ML and Cognitive Computing, at NeurIPS</i>, December 2019. (to appear)\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1910.04540\">Arxiv</a>]\n          [<a href=\"https://github.com/Tiiiger/QPyTorch\">Download on GitHub</a>]\n          [<a href=\"https://pypi.org/project/qtorch/\">Install via Pip</a>]\n        </div>\n        <div class=\"abstract\">\n          Low-precision training reduces computational cost and produces efficient models. Recent research in developing new low-precision training algorithms often relies on simulation to empirically evaluate the statistical effects of quantization while avoiding the substantial overhead of building specific hardware. To support this empirical research, we introduce QPyTorch, a low-precision arithmetic simulation framework. Built natively in PyTorch, QPyTorch provides a convenient interface that minimizes the efforts needed to reliably convert existing codes to study low-precision training. QPyTorch is general, and supports a variety of combinations of precisions, number formats, and rounding options. Additionally, it leverages an efficient fused-kernel approach to reduce simulator overhead, which enables simulation of large-scale, realistic problems.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>MICRO 2019</div></td><td>\n        <div class=\"papertitle\">\n          Boosting the Performance of CNN Accelerators with Dynamic Fine-Grained Channel Gating\n        </div>\n        <div class=\"authors\">\n          Weizhe Hua, Yuan Zhou, Christopher De Sa, Zhiru Zhang, G. Edward Suh\n        </div>\n        <div class=\"journal\">\n          In <i>MICRO '52 Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</i>, October 2019.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://dl.acm.org/citation.cfm?id=3358283\">Paper</a>]\n        </div>\n        <div class=\"abstract\">\n          This paper proposes a new fine-grained dynamic pruning technique for CNN inference, named channel gating, and presents an accelerator architecture that can effectively exploit the dynamic sparsity. Intuitively, channel gating identifies the regions in the feature map of each CNN layer that contribute less to the classification result and turns off a subset of channels for computing the activations in these less important regions. Unlike static network pruning, which removes redundant weights or neurons prior to inference, channel gating exploits dynamic sparsity specific to each input at run time and in a structured manner. To maximize compute savings while minimizing accuracy loss, channel gating learns the gating thresholds together with weights automatically through training. Experimental results show that the proposed approach can significantly speed up state-of-the-art networks with a marginal accuracy loss, and enable a trade-off between performance and accuracy. This paper also shows that channel gating can be supported with a small set of extensions to a CNN accelerator, and implements a prototype for quantized ResNet-18 models. The accelerator shows an average speedup of 2.3\u00d7 for ImageNet when the theoretical FLOP reduction is 2.8\u00d7, indicating that the hardware can effectively exploit the dynamic sparsity exposed by channel gating.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>SIGOPS 2019</div></td><td>\n        <div class=\"papertitle\">\n          Cloud-Hosted Intelligence for Real-time IoT Applications\n        </div>\n        <div class=\"authors\">\n          Ken Birman, Bharath Hariharan, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>SIGOPS Operating Systems Review 53</i>, 1, 7\u201313, July 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/sigops2019_reactiveedge.pdf\">Paper</a>]\n          [<a href=\"https://doi.org/10.1145/3352020.3352023\">DOI</a>]\n        </div>\n        <div class=\"abstract\">\n          Deploying machine learning into IoT cloud settings will require an evolution of the cloud infrastructure. In this white paper, we justify this assertion and identify new capabilities needed for real-time intelligent systems. We also outline our initial efforts to create a new edge architecture more suitable for ML. Although the work is still underway, several components exist, and we review them. We then point to open technical problems that will need to be solved as we progress further in this direction.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"4\" class=\"conferencename conferencenameblue\"><div>ICML 2019</div></td><td>\n        <div class=\"papertitle\">\n          Improving Neural Network Quantization without Retraining using Outlier Channel Splitting\n        </div>\n        <div class=\"authors\">\n          Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: the Thirty-sixth International Conference on Machine Learning</i>, June 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v97/zhao19c.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1901.09504\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Quantization can improve the execution latency and energy efficiency of neural networks on both commodity GPUs and specialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied topic of quantizing a floating-point model without (re)training. DNN weights and activations follow a bell-shaped distribution post-training, while practical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by clipping the outliers or using specialized hardware. In this work, we propose outlier channel splitting (OCS), which duplicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Experimental evaluation on ImageNet classification and language modeling shows that OCS can outperform state-of-the-art clipping techniques with only minor overhead.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Distributed Learning with Sublinear Communication\n        </div>\n        <div class=\"authors\">\n          Jayadev Acharya, Christopher De Sa, Dylan J. Foster, Karthik Sridharan\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: the Thirty-sixth International Conference on Machine Learning</i>, June 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v97/acharya19b.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1902.11259\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          In distributed statistical learning, <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-1\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-2\">N</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-1\"> N </script> samples are split across <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-3\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-4\">m</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-2\"> m </script> machines and a learner wishes to use minimal communication to learn as well as if the examples were on a single machine. This model has received substantial interest in machine learning due to its scalability and potential for parallel speedup. However, in high-dimensional settings, where the number examples is smaller than the number of features (\"dimension\"), the speedup afforded by distributed learning may be overshadowed by the cost of communicating a single example. This paper investigates the following question: When is it possible to learn a d-dimensional model in the distributed setting with total communication sublinear in <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-5\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-6\">d</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-3\"> d </script>? \n          <p>\n          Starting with a negative result, we show that for learning <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-7\"><span class=\"MJXp-msubsup\" id=\"MJXp-Span-8\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-9\" style=\"margin-right: 0.05em;\">\u2113</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-10\" style=\"vertical-align: -0.4em;\">1</span></span></span></span><script type=\"math/tex\" id=\"MathJax-Element-4\"> \\ell_1 </script>-bounded or sparse linear models, no algorithm can obtain optimal error until communication is linear in dimension. Our main result is that that by slightly relaxing the standard boundedness assumptions for linear models, we can obtain distributed algorithms that enjoy optimal error with communication logarithmic in dimension. This result is based on a family of algorithms that combine mirror descent with randomized sparsification/quantization of iterates, and extends to the general stochastic convex optimization model.\n        </p></div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          A Kernel Theory of Modern Data Augmentation\n        </div>\n        <div class=\"authors\">\n          Tri Dao, Albert Gu, Alexander J. Ratner, Virginia Smith, Christopher De Sa, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: the Thirty-sixth International Conference on Machine Learning</i>, June 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v97/dao19b.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1803.06084\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          SWALP : Stochastic Weight Averaging in Low Precision Training\n        </div>\n        <div class=\"authors\">\n          Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: the Thirty-sixth International Conference on Machine Learning</i>, June 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v97/yang19d.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1904.11943\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>CVPR 2019</div></td><td>\n        <div class=\"papertitle\">\n          Building Efficient Deep Neural Networks with Unitary Group Convolutions\n        </div>\n        <div class=\"authors\">\n          Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang\n        </div>\n        <div class=\"journal\">\n          In <i>CVPR: The Conference on Computer Vision and Pattern Recognition</i>, June 2019.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1811.07755\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          We propose unitary group convolutions (UGConvs), a building block for CNNs which compose a group convolution with unitary transforms in feature space to learn a richer set of representations than group convolution alone. UGConvs generalize two disparate ideas in CNN architecture, channel shuffling (i.e. ShuffleNet) and block-circulant networks (i.e. CirCNN), and provide unifying insights that lead to a deeper understanding of each technique. We experimentally demonstrate that dense unitary transforms can outperform channel shuffling in DNN accuracy. On the other hand, different dense transforms exhibit comparable accuracy performance. Based on these observations we propose HadaNet, a UGConv network using Hadamard transforms. HadaNets achieve similar accuracy to circulant networks with lower computation complexity, and better accuracy than ShuffleNets with the same number of parameters and floating-point multiplies.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>ICDT 2019</div></td><td>\n        <div class=\"papertitle\">\n          A Formal Framework For Probabilistic Unclean Databases\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Ihab F. Ilyas, Benny Kimelfeld, Christopher R\u00e9, Theodoros Rekatsinas\n        </div>\n        <div class=\"journal\">\n          In the <i>22nd International Conference on Database Theory (ICDT 2019)</i>, March 2019.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://arxiv.org/abs/1801.06750\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Most theoretical frameworks that focus on data errors and inconsistencies follow logic-based reasoning. Yet, practical data cleaning tools need to incorporate statistical reasoning to be effective in real-world data cleaning tasks. Motivated by these empirical successes, we propose a formal framework for unclean databases, where two types of statistical knowledge are incorporated: The first represents a belief of how intended (clean) data is generated, and the second represents a belief of how noise is introduced in the actual observed database instance. To capture this noisy channel model, we introduce the concept of a Probabilistic Unclean Database (PUD), a triple that consists of a probabilistic database that we call the intention, a probabilistic data transformator that we call the realization and captures how noise is introduced, and a dirty observed database instance that we call the observation. We define three computational problems in the PUD framework: cleaning (infer the most probable clean instance given a PUD), probabilistic query answering (compute the probability of an answer tuple over the unclean observed instance), and learning (estimate the most likely intention and realization models of a PUD given a collection of training data). We illustrate the PUD framework on concrete representations of the intention and realization, show that they generalize traditional concepts of repairs such as cardinality and value repairs, draw connection to consistent query answering, and prove tractability results. We further show that parameters can be learned in practical instantiations, and in fact, prove that under certain conditions we can learn a PUD directly from a single dirty database instance without any need for clean examples.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>SPIE 2019</div></td><td>\n        <div class=\"papertitle\">\n          Addressing sensor drift in a proprioceptive optical foam system\n        </div>\n        <div class=\"authors\">\n          Ilse M. Van Meerbeek, Jose A. Barreiros, Robert F. Shepherd, Christopher M. De Sa\n        </div>\n        <div class=\"journal\">\n          In <i>Proc. SPIE 10970, Sensors and Smart Structures Technologies for Civil, Mechanical, and Aerospace Systems 2019</i>, March 2019.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10970/109700F/Addressing-sensor-drift-in-a-proprioceptive-optical-foam-system/10.1117/12.2515349.short\">Paper</a>]\n        </div>\n        <div class=\"abstract\">\n          We previously reported an elastomeric, optical foam sensor that can sense different types of deformation. The elastomeric foam is embedded with optical fibers that shine light into the foam while simultaneously transmitting scattered light exiting the foam. We applied machine learning techniques to the optical fiber data to form a prediction model that predicts whether the foam is being twisted or bent (classification), as well as the magnitude and direction of the deformation (regression). The best classification model had 100% accuracy on new data points, and the best regression models had a mean absolute error of 0.06 degrees on new data points. This kind of proprioceptive ability could give soft robots much more information about their physical state and therefore improve our ability to control them; however, prediction error increases with time due to drift in the optical fiber outputs. This paper presents an attempt to address this drift. We applied a technique based on work presented by Di Carlo et. al. This unsupervised technique uses the evolutionary optimization process \"covariance matrix adaptation evolution strategy\" (CMA-ES) to compute a correction factor that can be applied to unobserved, drifted data points. The best solutions reduced classification error by 49% and regression mean absolute error by 36%.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>Sci. Robot. 2018</div></td><td>\n        <div class=\"papertitle\">\n          Soft optoelectronic sensory foams with proprioception\n        </div>\n        <div class=\"authors\">\n          Ilse Van Meerbeek, Christopher De Sa, Robert Shepherd\n        </div>\n        <div class=\"journal\">\n          In <i>Science Robotics</i>, November 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://robotics.sciencemag.org/content/3/24/eaau2489\">Paper</a>]\n          [<a href=\"http://news.cornell.edu/stories/2018/12/its-alive-fiber-optic-sensors-could-help-soft-robots-feel-adapt\">Article</a>]\n        </div>\n        <div class=\"abstract\">\n          In a step toward soft robot proprioception, and therefore better control, this paper presents an internally illuminated elastomer foam that has been trained to detect its own deformation through machine learning techniques. Optical fibers transmitted light into the foam and simultaneously received diffuse waves from internal reflection. The diffuse reflected light was interpreted by machine learning techniques to predict whether the foam was twisted clockwise, twisted counterclockwise, bent up, or bent down. Machine learning techniques were also used to predict the magnitude of the deformation type. On new data points, the model predicted the type of deformation with 100% accuracy and the magnitude of the deformation with a mean absolute error of 0.06\u00b0. This capability may impart soft robots with more complete proprioception, enabling them to be reliably controlled and responsive to external stimuli.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"2\" class=\"conferencename conferencenamered\"><div>ICML 2018</div></td><td>\n        <div class=\"papertitle\">\n          Minibatch Gibbs Sampling on Large Graphical Models\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Vincent Chen, Wing Wong\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: Proceedings of the 35rd International Conference on Machine Learning</i>, July 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v80/de-sa18a.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1806.06086\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Gibbs sampling is the de facto Markov chain Monte Carlo method used for inference and learning on large scale graphical models. For complicated factor graphs with lots of factors, the performance of Gibbs sampling can be limited by the computational cost of executing a single update step of the Markov chain. This cost is proportional to the degree of the graph, the number of factors adjacent to each variable. In this paper, we show how this cost can be reduced by using minibatching: subsampling the factors to form an estimate of their sum. We introduce several minibatched variants of Gibbs, show that they can be made unbiased, prove bounds on their convergence rates, and show that under some conditions they can result in asymptotic single-update-run-time speedups over plain Gibbs sampling.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Representation Tradeoffs for Hyperbolic Embeddings\n        </div>\n        <div class=\"authors\">\n          Frederic Sala, Chris De Sa, Albert Gu, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: Proceedings of the 35rd International Conference on Machine Learning</i>, July 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://proceedings.mlr.press/v80/sala18a.html\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1804.03329\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyperbolic space with arbitrarily low distortion without optimization. On WordNet, this algorithm obtains a mean-average-precision of 0.989 with only two dimensions, outperforming existing work by 0.11 points. We provide bounds characterizing the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based implementation that can handle incomplete information.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>PODC 2018</div></td><td>\n        <div class=\"papertitle\">\n          The Convergence of Stochastic Gradient Descent in Asynchronous Shared Memory\n        </div>\n        <div class=\"authors\">\n          Dan Alistarh, Christopher De Sa, Nikola Konstantinov\n        </div>\n        <div class=\"journal\">\n          In <i>Principles of Distributed Computing (PODC 2018)</i>, July 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1803.08841\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          <p>Stochastic Gradient Descent (SGD) is a fundamental algorithm in machine learning, representing the optimization backbone for training several classic models, from regression to neural networks. Given the recent practical focus on distributed machine learning, significant work has been dedicated to the convergence properties of this algorithm under the inconsistent and noisy updates arising from execution in a distributed environment. However, surprisingly, the convergence properties of this classic algorithm in the standard shared-memory model are still not well-understood. \n          </p><p>\n          In this work, we address this gap, and provide new convergence bounds for lock-free concurrent stochastic gradient descent, executing in the classic asynchronous shared memory model, against a strong adaptive adversary. Our results give improved upper and lower bounds on the \"price of asynchrony\" when executing the fundamental SGD algorithm in a concurrent setting. They show that this classic optimization tool can converge faster and with a wider range of parameters than previously known under asynchronous iterations. At the same time, we exhibit a fundamental trade-off between the maximum delay in the system and the rate at which SGD can converge, which governs the set of parameters under which this algorithm can still work efficiently.</p>\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>AISTATS 2018</div></td><td>\n        <div class=\"papertitle\">\n          Accelerated Stochastic Power Iteration\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Bryan He, Ioannis Mitliagkas, Christopher R\u00e9, Peng Xu\n        </div>\n        <div class=\"journal\">\n          In <i>AISTATS: The 21st International Conference on Artificial Intelligence and Statistics</i>, April 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1707.02670\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Principal component analysis (PCA) is one of the most powerful tools in machine learning. The simplest method for PCA, the power iteration, requires <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-11\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-12\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-13\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-14\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mn\" id=\"MJXp-Span-15\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-16\"><span class=\"MJXp-mo\" id=\"MJXp-Span-17\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-mi\" id=\"MJXp-Span-18\">\u0394</span><span class=\"MJXp-mo\" id=\"MJXp-Span-19\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-5\"> \\mathcal O(1/\\Delta) </script> full-data passes to recover the principal component of a matrix with eigen-gap <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-20\"><span class=\"MJXp-mi\" id=\"MJXp-Span-21\">\u0394</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-6\"> \\Delta </script>. Lanczos, a significantly more complex method, achieves an accelerated rate of <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-22\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-23\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-24\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-25\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mn\" id=\"MJXp-Span-26\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-27\"><span class=\"MJXp-mo\" id=\"MJXp-Span-28\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msqrt\" id=\"MJXp-Span-29\"><span class=\"MJXp-surd\"><span style=\"font-size: 134%; margin-top: 0.104em;\">\u221a</span></span><span class=\"MJXp-root\"><span class=\"MJXp-rule\" style=\"border-top: 0.08em solid;\"></span><span class=\"MJXp-box\"><span class=\"MJXp-mi\" id=\"MJXp-Span-30\">\u0394</span></span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-31\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-7\"> \\mathcal O(1/\\sqrt{\\Delta}) </script> passes. Modern applications, however, motivate methods that only ingest a subset of available data, known as the stochastic setting. In the online stochastic setting, simple algorithms like Oja's iteration achieve the optimal sample complexity <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-32\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-33\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-34\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-35\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-36\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-37\" style=\"margin-right: 0.05em;\">\u03c3</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-38\" style=\"vertical-align: 0.5em;\">2</span></span><span class=\"MJXp-mrow\" id=\"MJXp-Span-39\"><span class=\"MJXp-mo\" id=\"MJXp-Span-40\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-41\"><span class=\"MJXp-mi\" id=\"MJXp-Span-42\" style=\"margin-right: 0.05em;\">\u0394</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-43\" style=\"vertical-align: 0.5em;\">2</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-44\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-8\"> \\mathcal O(\\sigma^2/\\Delta^2) </script>. Unfortunately, they are fully sequential, and also require <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-45\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-46\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-47\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-48\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-49\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-50\" style=\"margin-right: 0.05em;\">\u03c3</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-51\" style=\"vertical-align: 0.5em;\">2</span></span><span class=\"MJXp-mrow\" id=\"MJXp-Span-52\"><span class=\"MJXp-mo\" id=\"MJXp-Span-53\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-54\"><span class=\"MJXp-mi\" id=\"MJXp-Span-55\" style=\"margin-right: 0.05em;\">\u0394</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-56\" style=\"vertical-align: 0.5em;\">2</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-57\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-9\"> \\mathcal O(\\sigma^2/\\Delta^2) </script> iterations, far from the <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-58\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-59\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-60\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-61\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mn\" id=\"MJXp-Span-62\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-63\"><span class=\"MJXp-mo\" id=\"MJXp-Span-64\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msqrt\" id=\"MJXp-Span-65\"><span class=\"MJXp-surd\"><span style=\"font-size: 134%; margin-top: 0.104em;\">\u221a</span></span><span class=\"MJXp-root\"><span class=\"MJXp-rule\" style=\"border-top: 0.08em solid;\"></span><span class=\"MJXp-box\"><span class=\"MJXp-mi\" id=\"MJXp-Span-66\">\u0394</span></span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-67\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-10\"> \\mathcal O(1/\\sqrt{\\Delta}) </script> rate of Lanczos. We propose a simple variant of the power iteration with an added momentum term, that achieves both the optimal sample and iteration complexity. In the full-pass setting, standard analysis shows that momentum achieves the accelerated rate, <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-68\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-69\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-70\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-71\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mn\" id=\"MJXp-Span-72\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-73\"><span class=\"MJXp-mo\" id=\"MJXp-Span-74\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msqrt\" id=\"MJXp-Span-75\"><span class=\"MJXp-surd\"><span style=\"font-size: 134%; margin-top: 0.104em;\">\u221a</span></span><span class=\"MJXp-root\"><span class=\"MJXp-rule\" style=\"border-top: 0.08em solid;\"></span><span class=\"MJXp-box\"><span class=\"MJXp-mi\" id=\"MJXp-Span-76\">\u0394</span></span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-77\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-11\"> \\mathcal O(1/\\sqrt{\\Delta}) </script>. We demonstrate empirically that naively applying momentum to a stochastic method, does not result in acceleration. We perform a novel, tight variance analysis that reveals the \"breaking-point variance\" beyond which this acceleration does not occur. By combining this insight with modern variance reduction techniques, we construct stochastic PCA algorithms, for the online and offline setting, that achieve an accelerated iteration complexity <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-78\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-79\"><span class=\"MJXp-mi MJXp-cal\" id=\"MJXp-Span-80\">O</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-81\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mn\" id=\"MJXp-Span-82\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-83\"><span class=\"MJXp-mo\" id=\"MJXp-Span-84\" style=\"margin-left: 0.111em; margin-right: 0.111em;\">/</span></span><span class=\"MJXp-msqrt\" id=\"MJXp-Span-85\"><span class=\"MJXp-surd\"><span style=\"font-size: 134%; margin-top: 0.104em;\">\u221a</span></span><span class=\"MJXp-root\"><span class=\"MJXp-rule\" style=\"border-top: 0.08em solid;\"></span><span class=\"MJXp-box\"><span class=\"MJXp-mi\" id=\"MJXp-Span-86\">\u0394</span></span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-87\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-12\"> \\mathcal O(1/\\sqrt{\\Delta}) </script>. Due to the embarassingly parallel nature of our methods, this acceleration translates directly to wall-clock time if deployed in a parallel environment. Our approach is very general, and applies to many non-convex optimization problems that can now be accelerated using the same technique.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>SODA 2018</div></td><td>\n        <div class=\"papertitle\">\n          A Two Pronged Progress in Structured Dense Matrix Multiplication\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, Atri Rudra\n        </div>\n        <div class=\"journal\">\n          In <i>SODA: ACM-SIAM Symposium on Discrete Algorithms (SODA18)</i>, January 2018.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1611.01569\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Matrix-vector multiplication is one of the most fundamental computing\n          primitives. Given a matrix <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-88\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-89\">A</span><span class=\"MJXp-mo\" id=\"MJXp-Span-90\" style=\"margin-left: 0.333em; margin-right: 0.333em;\">\u2208</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-91\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-92\" style=\"margin-right: 0.05em;\"><span class=\"MJXp-mi undefined\" id=\"MJXp-Span-93\">F</span></span><span class=\"MJXp-mrow MJXp-script\" id=\"MJXp-Span-94\" style=\"vertical-align: 0.5em;\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-95\">N</span><span class=\"MJXp-mo\" id=\"MJXp-Span-96\">\u00d7</span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-97\">N</span></span></span></span></span><script type=\"math/tex\" id=\"MathJax-Element-13\"> A\\in\\mathbb{F}^{N\\times N} </script> and a vector <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-98\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-99\">b</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-14\"> b </script>, it is\n          known that in the worst case <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-100\"><span class=\"MJXp-mi\" id=\"MJXp-Span-101\">\u0398</span><span class=\"MJXp-mo\" id=\"MJXp-Span-102\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-103\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-104\" style=\"margin-right: 0.05em;\">N</span><span class=\"MJXp-mn MJXp-script\" id=\"MJXp-Span-105\" style=\"vertical-align: 0.5em;\">2</span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-106\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-15\"> \\Theta(N^2) </script> operations over <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-107\"><span class=\"MJXp-mrow\" id=\"MJXp-Span-108\"><span class=\"MJXp-mi undefined\" id=\"MJXp-Span-109\">F</span></span></span></span><script type=\"math/tex\" id=\"MathJax-Element-16\"> \\mathbb{F} </script> are\n          needed to compute <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-110\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-111\">A</span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-112\">b</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-17\"> Ab </script>. A broad question is to identify classes of structured\n          dense matrices that can be represented with <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-113\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-114\">O</span><span class=\"MJXp-mo\" id=\"MJXp-Span-115\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-116\">N</span><span class=\"MJXp-mo\" id=\"MJXp-Span-117\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-18\"> O(N) </script> parameters, and for which\n          matrix-vector multiplication can be performed sub-quadratically. One such class\n          of structured matrices is the orthogonal polynomial transforms, whose rows\n          correspond to a family of orthogonal polynomials. Other well known classes\n          include the Toeplitz, Hankel, Vandermonde, Cauchy matrices and their extensions\n          that are all special cases of a displacement rank property. In this paper, we\n          make progress on two fronts:\n          <br>1. We introduce the notion of recurrence width of matrices. For matrices with\n          constant recurrence width, we design algorithms to compute <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-118\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-119\">A</span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-120\">b</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-19\"> Ab </script> and <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-121\"><span class=\"MJXp-msubsup\" id=\"MJXp-Span-122\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-123\" style=\"margin-right: 0.05em;\">A</span><span class=\"MJXp-mi MJXp-italic MJXp-script\" id=\"MJXp-Span-124\" style=\"vertical-align: 0.5em;\">T</span></span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-125\">b</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-20\"> A^Tb </script> in a\n          near-linear number of operations. This notion of width is finer than all the\n          above classes of structured matrices and thus we can compute multiplication for\n          all of them using the same core algorithm.\n          <br>2. We additionally adapt this algorithm to an algorithm for a much more\n          general class of matrices with displacement structure: those with low\n          displacement rank with respect to quasiseparable matrices. This class includes\n          Toeplitz-plus-Hankel-like matrices, Discrete Cosine/Sine Transforms, and more,\n          and captures all previously known matrices with displacement structure that we\n          are aware of.\n          <br>Our work unifies, generalizes, and simplifies existing state-of-the-art\n          results in structured matrix-vector multiplication. Finally, we show how\n          applications in areas such as multipoint evaluations of multivariate\n          polynomials and computing linear sequences can be reduced to problems involving\n          low recurrence width matrices.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>NeurIPS 2017</div></td><td>\n        <div class=\"papertitle\">\n          Gaussian Quadrature for Kernel Features\n          <span class=\"bestpaper\">Spotlight</span>\n        </div>\n        <div class=\"authors\">\n          Tri Dao, Christopher De Sa, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>NIPS: Proceedings of the 30th Neural Information Processing Systems Conference</i>, December 2017.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1709.02605\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Kernel methods have recently attracted resurgent interest, matching the\n          performance of deep neural networks in tasks such as speech recognition. The\n          random Fourier features map is a technique commonly used to scale up kernel\n          machines, but employing the randomized feature map means that\n          <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-126\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-127\">O</span><span class=\"MJXp-mo\" id=\"MJXp-Span-128\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-129\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-130\" style=\"margin-right: 0.05em;\">\u03f5</span><span class=\"MJXp-mrow MJXp-script\" id=\"MJXp-Span-131\" style=\"vertical-align: 0.5em;\"><span class=\"MJXp-mo\" id=\"MJXp-Span-132\">\u2212</span><span class=\"MJXp-mn\" id=\"MJXp-Span-133\">2</span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-134\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-21\"> O(\\epsilon^{-2})</script>  samples are required to achieve an approximation error of at\n          most <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-135\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-136\">\u03f5</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-22\"> \\epsilon</script> . In this paper, we investigate some alternative schemes for\n          constructing feature maps that are deterministic, rather than random, by\n          approximating the kernel in the frequency domain using Gaussian quadrature. We\n          show that deterministic feature maps can be constructed, for any <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-137\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-138\">\u03b3</span><span class=\"MJXp-mo\" id=\"MJXp-Span-139\" style=\"margin-left: 0.333em; margin-right: 0.333em;\">&gt;</span><span class=\"MJXp-mn\" id=\"MJXp-Span-140\">0</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-23\"> \\gamma > 0</script> ,\n          to achieve error <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-141\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-142\">\u03f5</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-24\"> \\epsilon</script>  with <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-143\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-144\">O</span><span class=\"MJXp-mo\" id=\"MJXp-Span-145\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-146\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-147\" style=\"margin-right: 0.05em;\">e</span><span class=\"MJXp-mrow MJXp-script\" id=\"MJXp-Span-148\" style=\"vertical-align: 0.5em;\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-149\">\u03b3</span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-150\" style=\"margin-left: 0.267em; margin-right: 0.267em;\">+</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-151\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-152\" style=\"margin-right: 0.05em;\">\u03f5</span><span class=\"MJXp-mrow MJXp-script\" id=\"MJXp-Span-153\" style=\"vertical-align: 0.5em;\"><span class=\"MJXp-mo\" id=\"MJXp-Span-154\">\u2212</span><span class=\"MJXp-mn\" id=\"MJXp-Span-155\">1</span><span class=\"MJXp-mrow\" id=\"MJXp-Span-156\"><span class=\"MJXp-mo\" id=\"MJXp-Span-157\">/</span></span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-158\">\u03b3</span></span></span><span class=\"MJXp-mo\" id=\"MJXp-Span-159\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-25\"> O(e^{\\gamma} + \\epsilon^{-1/\\gamma})</script>  samples\n          as <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-160\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-161\">\u03f5</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-26\"> \\epsilon </script> goes to 0. We validate our methods on datasets in different\n          domains, such as MNIST and TIMIT, showing that deterministic features are\n          faster to generate and achieve comparable accuracy to the state-of-the-art\n          kernel methods based on random Fourier features.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>ISCA 2017</div></td><td>\n        <div class=\"papertitle\">\n          Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent\n        </div>\n        <div class=\"authors\">\n          Chris De Sa, Matt Feldman, Christopher R\u00e9, and Kunle Olukotun\n        </div>\n        <div class=\"journal\">\n          In <i>ISCA: 44th International Symposium on Computer Architecture</i>, June 2017.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/isca2017_buckwild.pdf\">Paper</a>]\n        </div>\n        <div class=\"abstract\">\n          Stochastic gradient descent (SGD) is one of the most popular numerical algorithms used in machine learning and other domains.  Since this is likely to continue for the foreseeable future, it is important to study techniques that can make it run fast on parallel hardware. In this paper, we provide the first analysis of a technique called BUCKWILD that uses both asynchronous execution and low-precision computation.  We introduce the DMGC model, the first conceptualization of the parameter space that exists when implementing low-precision SGD, and show that it provides a way to both classify these algorithms and model their  performance.  We leverage this insight to propose and analyze techniques to improve the speed of low-precision SGD.  First, we propose software optimizations that can increase throughput on existing CPUs by up to 11x.  Second, we propose architectural changes, including a new cache technique we call an obstinate cache, that increase throughput beyond the limits of current-generation hardware.  We also implement and analyze low-precision SGD on the FPGA, which is a promising alternative to the CPU for future SGD systems.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>HILDA 2017</div></td><td>\n        <div class=\"papertitle\">\n          Flipper: A Systematic Approach to Debugging Training Sets\n        </div>\n        <div class=\"authors\">\n          Paroma Varma, Dan Iter, Christopher De Sa, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>HILDA: Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics, at SIGMOD</i>, May 2017.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://dl.acm.org/citation.cfm?id=3077263\">Link</a>]\n        </div>\n        <div class=\"abstract\">\n          As machine learning methods gain popularity across different fields, acquiring labeled training datasets has become the primary bottleneck in the machine learning pipeline. Recently generative models have been used to create and label large amounts of training data, albeit noisily. The output of these generative models is then used to train a discriminative model of choice, such as logistic regression or a complex neural network. However, any errors in the generative model can propagate to the subsequent model being trained. Unfortunately, these generative models are not easily interpretable and are therefore difficult to debug for users. To address this, we present our vision for Flipper, a framework that presents users with high-level information about why their training set is inaccurate and informs their decisions as they improve their generative model manually. We present potential tools within the Flipper framework, inspired by observing biomedical experts working with generative models, which allow users to analyze the errors in their training data in a systematic fashion. Finally, we discuss a prototype of Flipper and report results of a user study where users create a training set for a classification task and improve the discriminative model's accuracy by 2.4 points in less than an hour with feedback from Flipper.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"3\" class=\"conferencename conferencenameblue\"><div>NeurIPS 2016</div></td><td>\n        <div class=\"papertitle\">\n          Data Programming: Creating Large Training Sets, Quickly\n        </div>\n        <div class=\"authors\">\n          Alex Ratner, Chris De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>NIPS: Proceedings of the 29th Neural Information Processing Systems Conference</i>, December 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://arxiv.org/abs/1605.07723\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users provide a set of labeling functions, which are programs that heuristically label large subsets of data points, albeit noisily. By viewing these labeling functions as implicitly describing a generative model for this noise, we show that we can recover the parameters of this model to \"denoise\" the training set. Then, we show how to modify a discriminative loss function to make it noise-aware. We demonstrate our method over a range of discriminative models including logistic regression and LSTMs. We establish theoretically that we can recover the parameters of these generative models in a handful of settings. Experimentally, on the 2014 TAC-KBP relation extraction challenge, we show that data programming would have obtained a winning score, and also show that applying data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points over a supervised LSTM baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way to create machine learning models for non-experts.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much\n        </div>\n        <div class=\"authors\">\n          Bryan He, Christopher De Sa, Ioannis Mitliagkas, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>NIPS: Proceedings of the 29th Neural Information Processing Systems Conference</i>, December 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://arxiv.org/abs/1606.03432\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Socratic Learning: Empowering the Generative Model\n        </div>\n        <div class=\"authors\">\n          Paroma Varma, Rose Yu, Dan Iter, Christopher De Sa, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>FiLM-NIPS: Future of Interactive Learning Machines at NIPS</i>, December 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://arxiv.org/abs/1610.08123\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Modern machine learning techniques, such as deep learning, often use discriminative models that require large amounts of labeled data. An alternative approach is to use a generative model, which leverages heuristics from domain experts to train on unlabeled data. Domain experts often prefer to use generative models because they \"tell a story\" about their data. Unfortunately, generative models are typically less accurate than discriminative models. Several recent approaches combine both types of model to exploit their strengths. In this setting, a misspecified generative model can hurt the performance of subsequent discriminative training. To address this issue, we propose a framework called Socratic learning that automatically uses information from the discriminative model to correct generative model misspecification. Furthermore, this process provides users with interpretable feedback about how to improve their generative model. We evaluate Socratic learning on real-world relation extraction tasks and observe an immediate improvement in classification accuracy that could otherwise require several weeks of effort by domain experts.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"2\" class=\"conferencename conferencenamered\"><div>ICML 2016</div></td><td>\n        <div class=\"papertitle\">\n          Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling\n          <span class=\"bestpaper\">Best Paper Award</span>\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: Proceedings of the 33rd International Conference on\n          Machine Learning</i>, June 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/icml2016_hogwild_gibbs.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1602.07415\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Gibbs sampling is a Markov chain Monte Carlo technique commonly used\n          for estimating marginal distributions.\n          To speed up Gibbs sampling, there has recently been interest in parallelizing\n          it by executing asynchronously.  While empirical results suggest\n          that many models can be efficiently sampled asynchronously, traditional\n          Markov chain analysis does not apply to the asynchronous case, and thus\n          asynchronous Gibbs sampling is poorly understood.\n          In this paper, we derive a better understanding of the two main challenges\n          of asynchronous Gibbs: bias and mixing time. \n          We show experimentally that our theoretical results match practical outcomes.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Parallel SGD: When does Averaging Help?\n        </div>\n        <div class=\"authors\">\n          Jian Zhang, Christopher De Sa, Ioannis Mitiliagkas, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>OptML: Optimization Methods for the Next Generation of Machine Learning\n          </i>, workshop at <i>ICML</i>, June 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n        </div>\n        <div class=\"abstract\">\n          Consider a number of workers running SGD independently on the\n          same pool of data and averaging the models every once in a while \u2014 a\n          common but not well understood practice. We study model averaging\n          as a variance-reducing mechanism and describe two ways in which the\n          frequency of averaging affects convergence. For convex objectives, we show\n          the benefit of frequent averaging depends on the gradient variance envelope.\n          For non-convex objectives, we illustrate that this benefit depends on the\n          presence of multiple globally optimal points. We complement our findings\n          with multicore experiments on both synthetic and real data.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>SIGMOD 2016</div></td><td>\n        <div class=\"papertitle\">\n          DeepDive: Declarative Knowledge Base Construction\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Alex Ratner, Christopher R\u00e9, Jaeho Shin,\n          Feiran Wang, Sen Wu, and Ce Zhang\n        </div>\n        <div class=\"journal\">\n          Research highlight in <i>SIGMOD Record</i>, April 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/sigmodrecord2016_deepdive_highlight.pdf\">Paper</a>]\n          [<a href=\"http://sigmod.org/sigmodrecord/2016/04/19/deepdive-declarative-knowledge-base-construction/\">On the web</a>]\n        </div>\n        <div class=\"abstract\">\n          The dark data extraction or knowledge base construction (KBC) problem is to populate a SQL database with information from unstructured data sources including emails, webpages, and pdf reports. KBC is a long-standing problem in industry and research that encompasses problems of data extraction, cleaning, and integration. We describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems. The key idea in DeepDive is that statistical inference and machine learning are key tools to attack classical data problems in extraction, cleaning, and integration in a unified and more effective manner. DeepDive programs are declarative in that one cannot write probabilistic inference algorithms; instead, one interacts by defining features or rules about the domain. A key reason for this design choice is to enable domain experts to build their own KBC systems. We present the applications, abstractions, and techniques of DeepDive employed to accelerate construction of KBC systems.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>ASPLOS 2016</div></td><td>\n        <div class=\"papertitle\">\n          Generating Configurable Hardware from Parallel Patterns\n        </div>\n        <div class=\"authors\">\n          Raghu Prabhakar, David Koeplinger, Kevin J. Brown, HyoukJoong Lee, Christopher De Sa, Christos Kozyrakis, and Kunle Olukotun\n        </div>\n        <div class=\"journal\">\n          In <i>ASPLOS: 21st Int'l Conference on Architectural Support\n          for Programming Languages and Operating Systems</i>, April 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/asplos16_prabhakar.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1511.06968\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          In recent years the computing landscape has seen an increasing shift towards specialized accelerators. Field programmable gate arrays (FPGAs) are particularly promising as they offer significant performance and energy improvements compared to CPUs for a wide class of applications and are far more flexible than fixed-function ASICs. However, FPGAs are difficult to program. Traditional programming models for reconfigurable logic use low-level hardware description languages like Verilog and VHDL, which have none of the productivity features of modern software development languages but produce very efficient designs, and low-level software languages like C and OpenCL coupled with high-level synthesis (HLS) tools that typically produce designs that are far less efficient. Functional languages with parallel patterns are a better fit for hardware generation because they both provide high-level abstractions to programmers with little experience in hardware design and avoid many of the problems faced when generating hardware from imperative languages. In this paper, we identify two optimizations that are important when using parallel patterns to generate hardware: tiling and metapipelining. We present a general representation of tiled parallel patterns, and provide rules for automatically tiling patterns and generating metapipelines. We demonstrate experimentally that these optimizations result in speedups up to 40x on a set of benchmarks from the data analytics domain.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>CGO 2016</div></td><td>\n        <div class=\"papertitle\">\n          Have Abstraction and Eat Performance, Too: Optimized Heterogeneous Computing with Parallel Patterns\n        </div>\n        <div class=\"authors\">\n          Kevin J. Brown, HyoukJoong Lee, Tiark Rompf, Arvind K. Sujeeth,\n          Christopher De Sa, Christopher Aberger, and Kunle Olukotun\n        </div>\n        <div class=\"journal\">\n          In <i>CGO: International Symposium on Code Generation and Optimization</i>, March 2016.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/cgo16_brown.pdf\">Paper</a>]\n        </div>\n        <div class=\"abstract\">\n          High performance in modern computing platforms requires\n          programs to be parallel, distributed, and run on heterogeneous\n          hardware. However programming such architectures\n          is extremely difficult due to the need to implement the application\n          using multiple programming models and combine\n          them together in ad-hoc ways. To optimize distributed applications\n          both for modern hardware and for modern programmers\n          we need a programming model that is sufficiently\n          expressive to support a variety of parallel applications, sufficiently\n          performant to surpass hand-optimized sequential implementations,\n          and sufficiently portable to support a variety\n          of heterogeneous hardware. Unfortunately existing systems\n          tend to fall short of these requirements.\n          <p>\n          In this paper we introduce the Distributed Multiloop Language\n          (DMLL), a new intermediate language based on common\n          parallel patterns that captures the necessary semantic\n          knowledge to efficiently target distributed heterogeneous architectures.\n          We show straightforward analyses that determine\n          what data to distribute based on its usage as well as\n          powerful transformations of nested patterns that restructure\n          computation to enable distribution and optimize for heterogeneous\n          devices. We present experimental results for a range\n          of applications spanning multiple domains and demonstrate\n          highly efficient execution compared to manually-optimized\n          counterparts in multiple distributed programming models.\n        </p></div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"2\" class=\"conferencename conferencenamered\"><div>NeurIPS 2015</div></td><td>\n        <div class=\"papertitle\">\n          Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width\n          <span class=\"bestpaper\">Spotlight</span>\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>NIPS: Proceedings of the 28th Neural Information Processing Systems Conference</i>, December 2015.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/nips2015_hierarchy_width.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1510.00756\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width\u2014regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td>\n        <div class=\"papertitle\">\n          Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>NIPS: Proceedings of the 28th Neural Information Processing Systems Conference</i>, December 2015.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/nips2015_hogwild.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1506.06438\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenameblue\"><div>VLDB 2015</div></td><td>\n        <div class=\"papertitle\">\n          Incremental Knowledge Base Construction Using DeepDive\n          <span class=\"bestpaper\">Best of Issue</span>\n        </div>\n        <div class=\"authors\">\n          Jaeho Shin, Sen Wu, Feiran Wang, Ce Zhang, Christopher De Sa, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>VLDB: Proceedings of the 41st International Conference on Very Large Data Bases</i>, September 2015.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"http://i.stanford.edu/hazy/papers/inc.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1502.00731\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Populating a database with unstructured information is a\n          long-standing problem in industry and research that encompasses\n          problems of extraction, cleaning, and integration. Recent\n          names used for this problem include dealing with dark\n          data and knowledge base construction (KBC). In this work,\n          we describe DeepDive, a system that combines database and\n          machine learning ideas to help develop KBC systems, and we\n          present techniques to make the KBC process more efficient.\n          We observe that the KBC process is iterative, and we develop\n          techniques to incrementally produce inference results\n          for KBC systems. We propose two methods for incremental\n          inference, based respectively on sampling and variational\n          techniques. We also study the tradeoff space of these methods\n          and develop a simple rule-based optimizer. DeepDive\n          includes all of these contributions, and we evaluate DeepDive\n          on five KBC systems, showing that it can speed up\n          KBC inference tasks by up to two orders of magnitude with\n          negligible impact on quality.\n        </div>\n        <p>\n      </p></td></tr>\n      <tr><td rowspan=\"1\" class=\"conferencename conferencenamered\"><div>ICML 2015</div></td><td>\n        <div class=\"papertitle\">\n          Global Convergence of Stochastic Gradient Descent for Some Nonconvex Matrix Problems\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Kunle Olukotun, and Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          In <i>ICML: Proceedings of the 32nd International Conference on\n          Machine Learning</i>, July 2015.\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/icml2015_alecton.pdf\">Paper</a>]\n          [<a href=\"http://arxiv.org/abs/1411.1134\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Stochastic gradient descent (SGD) on a low-rank factorization is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within <span class=\"MathJax_Preview\" style=\"color: inherit;\"><span class=\"MJXp-math\" id=\"MJXp-Span-162\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-163\">O</span><span class=\"MJXp-mo\" id=\"MJXp-Span-164\" style=\"margin-left: 0em; margin-right: 0em;\">(</span><span class=\"MJXp-msubsup\" id=\"MJXp-Span-165\"><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-166\" style=\"margin-right: 0.05em;\">\u03f5</span><span class=\"MJXp-mrow MJXp-script\" id=\"MJXp-Span-167\" style=\"vertical-align: 0.5em;\"><span class=\"MJXp-mo\" id=\"MJXp-Span-168\">\u2212</span><span class=\"MJXp-mn\" id=\"MJXp-Span-169\">1</span></span></span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-170\">n</span><span class=\"MJXp-mi\" id=\"MJXp-Span-171\">log</span><span class=\"MJXp-mo\" id=\"MJXp-Span-172\" style=\"margin-left: 0em; margin-right: 0em;\"></span><span class=\"MJXp-mi MJXp-italic\" id=\"MJXp-Span-173\">n</span><span class=\"MJXp-mo\" id=\"MJXp-Span-174\" style=\"margin-left: 0em; margin-right: 0em;\">)</span></span></span><script type=\"math/tex\" id=\"MathJax-Element-27\">O(\\epsilon^{-1} n \\log n)</script> steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show experiments to illustrate the runtime and convergence of the algorithm.\n        </div>\n        <p>\n      </p></td></tr>\n    </tbody></table>\n\n    <h2>Manuscripts</h2>\n\n    <ul>\n      <li>\n        <div class=\"papertitle\">\n          PipeMare: Asynchronous Pipeline Parallel DNN Training\n        </div>\n        <div class=\"authors\">\n          Bowen Yang, Jian Zhang, Jonathan Li, Christopher R\u00e9, Christopher R. Aberger, Christopher De Sa\n        </div>\n        <div class=\"journal\">\n          Manuscript updated October 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1910.05124\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Recently there has been a flurry of interest around using pipeline parallelism while training neural networks. Pipeline parallelism enables larger models to be partitioned spatially across chips and within a chip, leading to both lower network communication and overall higher hardware utilization. Unfortunately, to preserve statistical efficiency, existing pipeline-parallelism techniques sacrifice hardware efficiency by introducing bubbles into the pipeline and/or incurring extra memory costs. In this paper, we investigate to what extent these sacrifices are necessary. Theoretically, we derive a simple but robust training method, called PipeMare, that tolerates asynchronous updates during pipeline-parallel execution. Using this, we show empirically, on a ResNet network and a Transformer network, that PipeMare can achieve final model qualities that match those of synchronous training techniques (at most 0.9% worse test accuracy and 0.3 better test BLEU score) while either using up to 2.0X less weight and optimizer memory or being up to 3.3X faster than other pipeline parallel training techniques. To the best of our knowledge we are the first to explore these techniques and fine-grained pipeline parallelism (e.g. the number of pipeline stages equals to the number of layers) during neural network training.\n        </div>\n        <p>\n      </p></li>\n      <li>\n        <div class=\"papertitle\">\n          Overwrite Quantization: Opportunistic Outlier Handling for Neural Network Accelerators\n        </div>\n        <div class=\"authors\">\n          Ritchie Zhao, Christopher De Sa, Zhiru Zhang\n        </div>\n        <div class=\"journal\">\n          Manuscript updated October 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1910.06909\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Outliers in weights and activations pose a key challenge for fixed-point quantization of neural networks. While outliers can be addressed by fine-tuning, this is not practical for machine learning (ML) service providers (e.g., Google, Microsoft) who often receive customers' models without the training data. Specialized hardware for handling outliers can enable low-precision DNNs, but incurs nontrivial area overhead. In this paper, we propose overwrite quantization (OverQ), a novel hardware technique which opportunistically increases bitwidth for outliers by letting them overwrite adjacent values. An FPGA prototype shows OverQ can significantly improve ResNet-18 accuracy at 4 bits while incurring relatively little increase in resource utilization.\n        </div>\n        <p>\n      </p></li>\n      <li>\n        <div class=\"papertitle\">\n          High-Accuracy Low-Precision Training\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R. Aberger, Kunle Olukotun, Christopher R\u00e9\n        </div>\n        <div class=\"journal\">\n          Manuscript updated December 2018\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/arxiv2018_lpsvrg.pdf\">Updated Manuscript</a>]\n          [<a href=\"http://www.cs.cornell.edu/~cdesa/blog/2018-03-09-halp/halp.html\">Blog</a>]\n        </div>\n        <div class=\"abstract\">\n          There is currently an arms race to design low-precision hardware accelerators capable of training machine learning models. This is because purpose-built, low-precision hardware accelerators can lower both the time and energy needed to complete a task. In contrast, traditional hardware architectures are over-provisioned, in terms of numerical precision, for machine learning tasks. Unfortunately, the statistical effects of low-precision computation <i>during training</i> are still not well understood. As a result, it is difficult to reach the statistical accuracies of traditional architectures on these new accelerators which have limited support for higher precision computation. This is due to a tradeoff with standard low-precision training algorithms: as the number of bits is decreased, noise that limits statistical accuracy is increased. In this paper we argue that one can reap the hardware benefits of low-precision accelerators while maintaining the statistical accuracies of traditional, higher-precision data formats. To do this we introduce a training algorithm called High-Accuracy Low-Precision (HALP). HALP is a low-precision stochastic gradient descent variant that uses entirely low-precision computation in its inner loop while infrequently recentering this computation with higher-precision computation done in an outer loop. HALP uses three techniques to reduce noise: (1) a known variance reduction method based on stochastic variance-reduced gradient (SVRG); (2) a novel bit centering technique that uses infrequent high-precision computation to reduce quantization noise; and (3) a novel dynamic bias adjustment technique to prevent overflow and underflow. On strongly convex problems, we show both theoretically and empirically that HALP converges at the same linear rate as full-precision SVRG. Inspired by these results, we show on two neural network applications (CNN and LSTM) that HALP can empirically compete with higher-precision training algorithms.\n        </div>\n        <p>\n      </p></li>\n      <li>\n        <div class=\"papertitle\">\n          SysML: The New Frontier of Machine Learning Systems\n        </div>\n        <div class=\"authors\">\n          Alexander Ratner <i>et al</i>\n        </div>\n        <div class=\"journal\">\n          On <i>arxiv</i> March 2019\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1904.03257\">Arxiv</a>]\n          [<a href=\"https://www.sysml.cc/\">Conference Website</a>]\n        </div>\n        <div class=\"abstract\">\n          Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, SysML, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.\n        </div>\n        <p>\n      </p></li>\n      <!-- <li>\n        <div class=\"papertitle\">\n          Channel Gating Neural Networks\n        </div>\n        <div class=\"authors\">\n          Weizhe Hua, Christopher De Sa, Zhiru Zhang, G. Edward Suh\n        </div>\n        <div class=\"journal\">\n          On <i>arxiv</i> May 2018\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"https://arxiv.org/abs/1805.12549\">Arxiv</a>]\n        </div>\n        <div class=\"abstract\">\n          Employing deep neural networks to obtain state-of-the-art performance on computer vision tasks can consume billions of floating point operations and several Joules of energy per evaluation. Network pruning, which statically removes unnecessary features and weights, has emerged as a promising way to reduce this computation cost. In this paper, we propose channel gating, a dynamic, fine-grained, training-based computation-cost-reduction scheme. Channel gating works by identifying the regions in the features which contribute less to the classification result and turning off a subset of the channels for computing the pixels within these uninteresting regions. Unlike static network pruning, the channel gating optimizes computations exploiting characteristics specific to each input at run-time. We show experimentally that applying channel gating in state-of-the-art networks can achieve 66% and 60% reduction in FLOPs with 0.22% and 0.29% accuracy loss on the CIFAR-10 and CIFAR-100 datasets, respectively.\n        </div>\n        <p />\n      </li> -->\n      <!-- <li>\n        <div class=\"papertitle\">\n          High-Accuracy Low-Precision Training\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R. Aberger, Kunle Olukotun, Christopher R&eacute;\n        </div>\n        <div class=\"journal\">\n          On <i>arxiv</i>, March 2018\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/arxiv2018_lpsvrg.pdf\">Paper</a>]\n          [<a href=\"https://arxiv.org/abs/1803.03383\">Arxiv</a>]\n          [<a href=\"http://www.cs.cornell.edu/~cdesa/blog/2018-03-09-halp/halp.html\">Blog</a>]\n        </div>\n        <div class=\"abstract\">\n          Low-precision computation is often used to lower the time and energy cost of machine learning, and recently hardware accelerators have been developed to support it. Still, it has been used primarily for inference - not training. Previous low-precision training algorithms suffered from a fundamental tradeoff: as the number of bits of precision is lowered, quantization noise is added to the model, which limits statistical accuracy. To address this issue, we describe a simple low-precision stochastic gradient descent variant called HALP. HALP converges at the same theoretical rate as full-precision algorithms despite the noise introduced by using low precision throughout execution. The key idea is to use SVRG to reduce gradient variance, and to combine this with a novel technique called bit centering to reduce quantization error. We show that on the CPU, HALP can run up to \\( 4 \\times \\) faster than full-precision SVRG and can match its convergence trajectory. We implemented HALP in TensorQuant, and show that it exceeds the validation performance of plain low-precision SGD on two deep learning tasks.\n        </div>\n        <p />\n      </li> -->\n      <!-- <li>\n        <div class=\"papertitle\">\n          Representation Tradeoffs for Hyperbolic Embeddings\n        </div>\n        <div class=\"authors\">\n          Christopher De Sa, Albert Gu, Christopher R&eacute;, Frederic Sala\n        </div>\n        <div class=\"journal\">\n          <i>preprint</i>\n        </div>\n        <div class=\"links\">\n          [<a href=\"javascript:void(0)\" onclick=\"toggle_abstract(this)\">Abstract</a>]\n          [<a href=\"papers/arxiv2018_hyperbolic.pdf\">Paper</a>]\n        </div>\n        <div class=\"abstract\">\n          Hyperbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures like synonym or type hierarchies. Given a tree, we give a combinatorial construction that embeds the tree in hyperbolic space with arbitrarily low distortion without using optimization. On WordNet, our combinatorial embedding obtains a mean-average-precision of \\( 0.99 \\) with only two dimensions, while Nickel et al.'s recent construction obtains \\( 0.87 \\) using \\( 200 \\) dimensions.  We provide upper and lower bounds that allow us to characterize the precision-dimensionality tradeoff inherent in any hyperbolic embedding. To embed general metric spaces, we propose a hyperbolic generalization of multidimensional scaling (h-MDS). We show how to perform exact recovery of hyperbolic points from distances, provide a perturbation analysis, and give a recovery result that allows us to reduce dimensionality. The h-MDS approach offers consistently low distortion even with few dimensions across several datasets. Finally, we extract lessons from the algorithms and theory above to design a PyTorch-based implementation that can handle incomplete information and is scalable.  \n        </div>\n        <p />\n      </li> -->\n    </ul>\n    \n    </div>\n    \n\n    <div id=\"blog\">\n    <h2>Blog Posts</h2>\n\n    <ul>\n      <li>[March 9, 2018.] <a href=\"http://www.cs.cornell.edu/~cdesa/blog/2018-03-09-halp/halp.html\">HALP: High-Accuracy Low-Precision Training</a>.</li>\n    </ul>\n\n    </div>\n  \n\n\n</body></html>"