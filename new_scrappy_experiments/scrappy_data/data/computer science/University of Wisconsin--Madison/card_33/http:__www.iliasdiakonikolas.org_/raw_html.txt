"<html><head>\n\t\t<title>Ilias Diakonikolas Homepage</title>\n\t\t<meta name=\"description\" content=\"Ilias Diakonikolas\">\n\t\t<meta name=\"keywords\" content=\"Ilias, Diakonikolas, Columbia, Berkeley, Computer Science, algorithms, robust statistics, learning, testing, threshold functions, distributions, density estimation, hypothesis testing\">\n\n            <style>\n             body {font-family: Helvetica, Verdana, Arial, sans-serif;}\n\t     a:link {text-decoration:none; color:#003399;}\n\t     a:visited {text-decoration:none; color:#003399;}\n\t     .abstract_link { cursor: pointer; text-decoration: none; color:#003399; font-size:15px}\n\t     .abstract {width:700px; font-size:10pt; text-align:justify; margin-top:10px;}\n             </style>\n\n\t     <script type=\"text/javascript\" async=\"\" src=\"http://www.google-analytics.com/ga.js\"></script><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" id=\"\">\n\t     </script>\n\t     <script type=\"text/x-mathjax-config;executed=true\">\n\t       MathJax.Hub.Config({\n\t       tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}\n\t       });\n\t     </script>\n<script type=\"text/javascript\" src=\"path-to-mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML\"></script>\n\n<script src=\"http://code.jquery.com/jquery-1.10.1.min.js\"></script>\n\n<script>\n  function reveal_abstract(id) {\n    var element = document.getElementById(\"abstract\" + id);\n    var style = element.style;\n    if (style.display == \"none\") {\n      style.display = \"block\";\n    } else {\n      style.display = \"none\";\n    }\n  }\n</script>\n\n<script type=\"text/javascript\">\n\n  var _gaq = _gaq || [];\n  _gaq.push(['_setAccount', 'UA-37451300-1']);\n  _gaq.push(['_setDomainName', 'iliasdiakonikolas.org']);\n  _gaq.push(['_trackPageview']);\n\n  (function() {\n    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n  })();\n\n</script>\n\n\t<style type=\"text/css\">.MathJax_Preview {color: #888}\n#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}\n#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}\n.MathJax_Error {color: #CC0000; font-style: italic}\n</style></head>\n<body style=\"\"><div id=\"MathJax_Message\" style=\"display: none;\"></div>\n\n<h1>Ilias Diakonikolas</h1>\n\n\n<table>\n<tbody><tr>\n\n<td width=\"5\" valign=\"top\">\n<img border=\"2\" src=\"http://www.iliasdiakonikolas.org/photo.jpg\" ,=\"\" width=\"260\">\n\n\n\n</td><td> \n\n</td><td>\n\n<font size=\"4\"> I am a faculty member in the CS department at <a href=\"https://www.cs.wisc.edu/\">UW Madison</a>.\n\n</font><br><br>\n\n<font size=\"4\">\nPrior to joining UW, I was Andrew and Erna Viterbi Early Career Chair\nin Computer Science at <a href=\"http://www.cs.usc.edu/\">USC</a>,\nand a faculty member at the <a href=\"https://www.ed.ac.uk/informatics\">University of Edinburgh</a>. Prior to that,\nI spent two years at <a href=\"http://www.berkeley.edu\">UC Berkeley</a> as the <a href=\"https://simonsfoundation.org/\">Simons Postdoctoral Fellow</a> \nin <a href=\"http://theory.cs.berkeley.edu/\">Theoretical Computer Science</a>. \nI obtained my Ph.D. in <a href=\"http://www.cs.columbia.edu\">Computer Science</a> at <a href=\"http://www.columbia.edu\">Columbia University</a>,\nadvised by <a href=\"http://www.cs.columbia.edu/~mihalis\">Mihalis Yannakakis</a>.\nI did my undergraduate studies in Greece, at the <a href=\"https://www.ntua.gr/en/\">National Technical University of Athens</a>. <br> <br>\n\n<strong>Research:</strong> My main research interests are in algorithms and machine learning. \nA major goal of my work is to understand the tradeoff between statistical efficiency, computational efficiency, and robustness \nfor fundamental problems in statistics and machine learning.\nAreas of current interest include high-dimensional robust statistics/learning, nonparametric estimation, and distribution testing. \nI also have strong interests in applied probability, algorithmic game theory, and their connections to machine learning. \n\n\n\n<br> <br> \n\n<strong>Funding:</strong> My research has been supported by an NSF CAREER Award, an NSF AITF Award, a DARPA grant, a Sloan Research Fellowship, a Google Faculty Research Award, a Marie Curie Career Integration Grant, \nand an EPSRC grant.\n\n<br> <br>\n\nA short professional biography can be found <a href=\"http://www.iliasdiakonikolas.org/bio.html\">here</a>.\n\n\n\n<br> <br>\n\n<strong>Interested in working with me?</strong> Apply to our <a href=\"https://www.cs.wisc.edu/graduate/ms-and-phd-program/\">Ph.D. program</a>.\n\n\n</font>\n\n<font size=\"4\"> \n\n<p>University of Wisconsin-Madison <br>\nDepartment of Computer Sciences <br>\n1210 W. Dayton St. <br>\nMadison, WI 53706 <br>\nfullname_at_gmail.com\n</p></font><p><font size=\"4\">\n\n</font>\n\n\n</p></td></tr>\n</tbody></table>\n\n\n\n\n\n\n<h2>Recent and Upcoming Activities</h2>\n\n<font size=\"4\"> \t\n\t<ul>\n\n\n<li> Recent survey (with Daniel Kane) on algorithmic aspects of high-dimensional robust statistics. \nSee <a href=\"https://arxiv.org/abs/1911.05911\">here</a> for the arXiv version and \n<a href=\"http://www.iliasdiakonikolas.org/bwca-robust.pdf\">here</a> \nfor the shorter book chapter version. Comments welcome!\n</li>\n\n\n<li> <a href=\"https://simons.berkeley.edu/news/research-vignette-foundations-data-science\">Research Vignette: Algorithmic High-Dimensional Robust Statistics</a> (with S. Vempala and D. Woodruff),\nUC Berkeley Simons Institute newsletter, September 2019 issue.\n</li>\n\n<li> Simons Institute <a href=\"http://www.iliasdiakonikolas.org/simons-tutorial-robust.html\">Tutorial on Algorithmic High-Dimensional Robust Statistics</a>.\nSee <a href=\"https://www.youtube.com/watch?v=kOUDbhycAl0&amp;t=57s\">here</a> and <a href=\"https://www.youtube.com/watch?v=mWfEXgmMxSc\">here</a> for talk videos.\n</li>\n\n\n<li> Recent <a href=\"http://www.iliasdiakonikolas.org/isit-robust-tutorial.html\">tutorial</a> on High-Dimensional Robust Statistics at <a href=\"https://2019.ieee-isit.org/\">ISIT 2019</a>.\nSee <a href=\"http://www.iliasdiakonikolas.org/isit-robust-tutorial.html\">here</a> for the slides!\n\n</li><li> Recent <a href=\"http://www.iliasdiakonikolas.org/stoc-robust-tutorial.html\">tutorial</a> on Algorithmic High-Dimensional Robust Statistics (with Daniel Kane) at <a href=\"http://acm-stoc.org/stoc2019/\">STOC 2019</a>.\nSee <a href=\"http://www.iliasdiakonikolas.org/stoc-robust-tutorial.html\">here</a> for the slides!\n\n\n<br><br>\n\n\n\n</li><li><a href=\"http://ita.ucsd.edu/workshop/19/?year=19\">ITA 2019</a> session on \n<a href=\"http://www.iliasdiakonikolas.org/ita19-robust-learning.html\">Robust High-Dimensional Learning</a>.\n</li>\n\n<li> Videos of the talks from our Simons Workshop on <a href=\"https://simons.berkeley.edu/data-science-2018-2\">Robust and High-Dimensional Statistics</a>\nare available <a href=\"https://simons.berkeley.edu/workshops/schedule/6682\">here</a>.\n</li>\n\n<li> Video of a <a href=\"https://www.youtube.com/watch?time_continue=4&amp;v=IIp4CDab2N4\">recent talk</a> on \nComputational-Statistical Tradeoffs in Robust High-Dimensional Estimation.\n</li>\n\n\n<li><a href=\"http://www.ttic.edu/summer-workshop-2018/\">TTIC Summer 2018 workshop</a> on Robust High-Dimensional Statistics: \nSee <a href=\"http://www.iliasdiakonikolas.org/tti-robust.html\">here</a> for slides of all talks!\n</li>\n\n\t </ul>\n</font> \n\n\n    \n\n<script>\nfunction topic() {\n  var classToName = {\n    \"robust_statistics\" : \"Robust High-Dimensional Statistics/Learning\",\n    \"robust_PAC\" : \"Robust PAC Learning\",\n    \"nonparametric_statistics\" : \"Nonparametric Estimation\",\n    \"hypothesis_testing\" : \"Hypothesis Testing/Distribution Testing\",\n    \"applied_probability\" : \"Applied Probability with Algorithmic Applications\",\n    \"multi_obj_opt\" : \"Multiobjective Optimization\",\n    \"game_theory\" : \"Game Theory\",\n    \"property_testing\" : \"Property Testing\",\n    \"other_topics\" : \"Other Topics\"\n  }\n  var dict = {}\n  $(\".paper\").each(function() {\n    var category = $(this).context.className.split(\"paper \")[1];\n    if (!(category in dict)) {\n      dict[category] = new Array();\n    }\n    dict[category].push($(this).context.outerHTML);\n  });\n  str=\"\";\n  for (var key in classToName) {\n    console.log(dict[key].length);\n    str += \"<h3 style='color: #474747'><i>\" + classToName[key] + \"</i></h3>\";\n    str += \"<ol>\"\n    for (var i = 0 ; i < dict[key].length; i++) {\n      str += dict[key][i];\n      str += \"<br>\";\n    }\n    str += \"</ol>\"\n  }\n  $(\"#pubs\").html(str);\n}\n\nfunction chronologically() {\n  var dict = {};\n  var max_id = 0;\n  $(\".paper\").each(function(){\n    var strindex = $(this).context.id.split(\"paper\")[1];\n    var index = parseInt(strindex);\n    dict[index] = $(this).context.outerHTML;\n    if (index > max_id) {\n      max_id = index;\n    }\n  });\n  str = \"<ol>\";\n  for (var i = max_id; i >= 0; i--) {\n    if (i in dict) {\n      str += dict[i];\n      str += \"<br>\"\n    }\n  }\n  str += \"</ol>\";\n  $(\"#pubs\").html(str);\n}\n</script>\n\n\n<br>\n\n\n<h2>Expository Articles</h2>\n\n<ul>\n\n<li>\n\n\n  Recent Advances in Algorithmic High-Dimensional Robust Statistics\n  <a onclick=\"reveal_abstract(-2)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"https://arxiv.org/abs/1911.05911\" class=\"abstract_link\">[arxiv]</a> \n  <br> I. Diakonikolas and D. M. Kane.   \n  <br> A <a href=\"http://www.iliasdiakonikolas.org/bwca-robust.pdf\" class=\"abstract_link\">shorter version</a> \n       to appear as an Invited Book Chapter in <i>Beyond the Worst-Case Analysis of Algorithms</i>, Cambridge University Press, December 2019  \n  <div id=\"abstract-2\" class=\"abstract\" style=\"display:none\">\n  Learning in the presence of outliers is a fundamental problem in statistics.\n  Until recently, all known efficient unsupervised learning algorithms were very sensitive to outliers in high dimensions.\n  In particular, even for the task of robust mean estimation under natural distributional assumptions,\n  no efficient algorithm was known.\n  Recent work in theoretical computer science gave the first efficient robust estimators for a\n  number of fundamental statistical tasks, including mean and covariance estimation.\n  Since then, there has been a flurry of research activity on algorithmic\n  high-dimensional robust estimation in a range of settings.\n  In this survey article, we introduce the core ideas and algorithmic techniques\n  in the emerging area of algorithmic high-dimensional robust statistics\n  with a focus on robust mean estimation. We also provide an overview\n  of the approaches that have led to computationally efficient robust estimators\n  for a range of broader statistical tasks and discuss new directions and opportunities for future work.\n  </div>\n\n\n</li>\n\n<br>\n\n\n<li>\n\n  Learning Structured Distributions\n  <a onclick=\"reveal_abstract(-1)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/distribution-learning-survey.pdf\" class=\"abstract_link\">[pdf]</a> \n  <br>  I. Diakonikolas.\n  <br> Invited Book Chapter, in <i>Handbook of Big Data</i>, Chapman and Hall/CRC, February 2016.\n  <div id=\"abstract-1\" class=\"abstract\" style=\"display:none\">\nEstimating distributions from samples is a paradigmatic and fundamental unsupervised learning problem\nthat has been studied in statistics since the late nineteenth century, starting with the pioneering\nwork of Karl Pearson. During the past couple of decades, there has been a large body of work \nin computer science on this topic with a focus on {\\em computational efficiency.}\n\n<p>\n\nThe area of distribution estimation is well-motivated in its own right, and\nhas seen a recent surge of research activity, in part due to the ubiquity of structured distributions\nin the natural and social sciences. Such structural properties of distributions\nare sometimes direct consequences of the underlying application problem, \nor they are a plausible explanation of the model under investigation.\n\n</p><p>\n\nIn this chapter, we give a survey of both classical and modern techniques for distribution estimation, \nwith a focus on recent algorithmic ideas developed in theoretical computer science.\nThese ideas have led to computationally and statistically efficient algorithms for learning broad families of models.\nFor the sake of concreteness, we illustrate these ideas with specific examples. \nFinally, we highlight outstanding challenges and research directions for future work.\n</p></div>\n\n</li>\n\n<br>\n\n</ul>\n\n\n<h2>Research Publications\n<a class=\"abstract_link\" onclick=\"chronologically()\">[chronologically]</a><a class=\"abstract_link\" onclick=\"topic()\">[by topic]</a></h2>\n\n\n\n<div id=\"pubs\">\n<ol>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper82\">\nPrivate Testing of Distributions via Sample Permutations\n<br>  M. Aliakbarpour, I. Diakonikolas, D. Kane, R. Rubinfeld\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019) \n\n</li>\n\n<br>\n\n<li class=\"paper nonparametric_statistics\" id=\"paper81\">\nEfficient Algorithms for Multidimensional Segmented Regression\n<br> I. Diakonikolas, J. Li, A. Voloshinov\n<br> Manuscript, 2019\n\n</li>\n\n<br>\n\n<li class=\"paper robust_PAC\" id=\"paper80\">\nNearly Tight Bounds for Robust Proper Learning of Halfspaces with a Margin\n<a onclick=\"reveal_abstract(80)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1908.11335\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, P. Manurangsi\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019)\n<br> Selected for <strong>Spotlight Presentation</strong> at NeurIPS 2019\n\n<div id=\"abstract80\" class=\"abstract\" style=\"display:none\">\nWe study the problem of {\\em properly} learning large margin halfspaces in the agnostic PAC model. \nIn more detail, we study the complexity of properly learning $d$-dimensional halfspaces\non the unit ball within misclassification error $\\alpha \\cdot \\mathrm{OPT}_{\\gamma} + \\epsilon$, \nwhere $\\mathrm{OPT}_{\\gamma}$ is the optimal $\\gamma$-margin error rate \nand $\\alpha \\geq 1$ is the approximation ratio.\nWe give learning algorithms and computational hardness results\nfor this problem, for all values of the approximation ratio $\\alpha \\geq 1$, \nthat are nearly-matching for a range of parameters. \nSpecifically, for the natural setting that $\\alpha$ is any constant bigger \nthan one, we provide an essentially tight complexity characterization.\nOn the positive side, we give an $\\alpha = 1.01$-approximate proper learner \nthat uses $O(1/(\\epsilon^2\\gamma^2))$ samples (which is optimal) and runs in time\n$\\mathrm{poly}(d/\\epsilon) \\cdot 2^{\\tilde{O}(1/\\gamma^2)}$. On the negative side,  \nwe show that {\\em any} constant factor approximate proper learner has runtime \n$\\mathrm{poly}(d/\\epsilon) \\cdot 2^{(1/\\gamma)^{2-o(1)}}$, \nassuming the Exponential Time Hypothesis. \n\n</div>\n\n\n</li>\n\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper79\">\nOutlier-Robust High-Dimensional Sparse Estimation via Iterative Filtering\n<a onclick=\"reveal_abstract(79)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1911.08085\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, S. Karmalkar, D. Kane, E. Price, A. Stewart\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019) \n<div id=\"abstract79\" class=\"abstract\" style=\"display:none\">\nWe study high-dimensional sparse estimation tasks in a robust setting where a constant fraction\nof the dataset is adversarially corrupted. Specifically, we focus on the fundamental problems of robust\nsparse mean estimation and robust sparse PCA. \nWe give the first practically viable robust estimators for these problems. \nIn more detail, our algorithms are sample and computationally efficient \nand achieve near-optimal robustness guarantees. \nIn contrast to prior provable algorithms which relied on the ellipsoid method, \nour algorithms use spectral techniques to iteratively remove outliers from the dataset. \nOur experimental evaluation on synthetic data shows that our algorithms are scalable and \nsignificantly outperform a range of previous approaches, nearly matching the best error rate without corruptions.\n</div>\n\n</li>\n\n<br>\n\n\n<li class=\"paper robust_PAC\" id=\"paper78\">\nDistribution-Independent PAC Learning of Halfspaces with Massart Noise\n<a onclick=\"reveal_abstract(78)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1906.10075\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, T. Gouleakis, C. Tzamos\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019) \n<br> <strong>Outstanding Paper Award at NeurIPS 2019</strong>\n<br> See <a href=\"https://medium.com/@NeurIPSConf/neurips-2019-paper-awards-807e41d0c1e\">here</a> \nfor a description of our work by the program chairs \nand <a href=\"http://www.iliasdiakonikolas.org/massart-neurips19.pdf\">here</a> for the slides of my NeurIPS talk.\n\n\n\n<div id=\"abstract78\" class=\"abstract\" style=\"display:none\">\n\nWe study the problem of {\\em distribution-independent} PAC learning of halfspaces in the presence of Massart noise. \nSpecifically, we are given a set of labeled examples $(\\bx, y)$ drawn \nfrom a distribution $\\D$ on $\\R^{d+1}$ such that the marginal distribution \non the unlabeled points $\\bx$ is arbitrary and the labels $y$ are generated by an unknown halfspace \ncorrupted with Massart noise at noise rate $\\eta&lt;1/2$. The goal is to find \na hypothesis $h$ that minimizes the misclassification error $\\pr_{(\\bx, y) \\sim \\D} \\left[ h(\\bx) \\neq y \\right]$. \n\n<p>\n\nWe give a $\\poly(d, 1/\\eps)$ time algorithm for this problem with misclassification error $\\eta+\\eps$. \nWe also provide evidence that improving on the error guarantee of our algorithm\nmight be computationally hard. Prior to our work, no efficient weak (distribution-independent) learner \nwas known in this model, even for the class of disjunctions. The existence of such an algorithm \nfor halfspaces (or even disjunctions) has been posed as an open question in various works, \nstarting with Sloan (1988), Cohen (1997), and was most recently highlighted in Avrim Blum's FOCS 2003 tutorial.\n</p></div>\n\n</li>\n\n<br>\n\n<li class=\"paper other_topics\" id=\"paper77\">\nEquipping Experts/Bandits with Long-term Memory\n<a onclick=\"reveal_abstract(77)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1905.12950\" class=\"abstract_link\">[arxiv]</a> \n<br> K. Zheng, H. Luo, I. Diakonikolas, L. Wang\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019) \n<div id=\"abstract77\" class=\"abstract\" style=\"display:none\">\n\nWe propose the first reduction-based approach to obtaining long-term memory guarantees for online learning \nin the sense of Bousquet and Warmuth, 2002,by reducing the problem to achieving typical switching regret.\nSpecifically, for the classical expert problem with $K$ actions and $T$ rounds, using our framework \nwe develop various algorithms with a regret bound of order $O(\\sqrt{T(S\\ln T + n \\ln K)})$ compared \nto any sequence of experts with $S-1$ switches among $n \\leq \\min\\{S, K\\}$ distinct experts.\nIn addition, by plugging specific adaptive algorithms into our framework, we also achieve the best of both \nstochastic and adversarial environments simultaneously.\nThis resolves an open problem of Warmuth and Koolen, 2014.\nFurthermore, we extend our results to the sparse multi-armed bandit setting and show both negative \nand positive results for long-term memory guarantees.\nAs a side result, our lower bound also implies that sparse losses do not help improve \nthe worst-case regret for contextual bandits, a sharp contrast with the non-contextual case.\n</div>\n\n</li>\n\n<br>\n\n<li class=\"paper nonparametric_statistics\" id=\"paper76\">\nA Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families\n<a onclick=\"reveal_abstract(76)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1907.08306\" class=\"abstract_link\">[arxiv]</a> \n<br> B. Axelrod, I. Diakonikolas, A. Sidiropoulos, A. Stewart, G. Valiant\n<br> Advances in Neural Information Processing Systems (NeurIPS 2019) \n<div id=\"abstract76\" class=\"abstract\" style=\"display:none\">\nWe study the problem of computing the maximum likelihood estimator (MLE) of multivariate log-concave densities.\nOur main result is the first computationally efficient algorithm for this problem. In more detail, \nwe give an algorithm that, on input a set of $n$ points in $\\R^d$ and an accuracy parameter $\\epsilon&gt;0$, \nit runs in time $\\poly(n, d, 1/\\epsilon)$, and outputs a log-concave density that with high probability \nmaximizes the log-likelihood up to an additive $\\epsilon$. Our approach relies on a natural convex optimization \nformulation of the underlying problem that can be efficiently solved by a projected stochastic subgradient method. \nThe main challenge lies in showing that a stochastic subgradient of our objective function can be efficiently approximated. \nTo achieve this, we rely on structural results on approximation of log-concave densities and \nleverage classical algorithmic tools on volume approximation of convex bodies and uniform sampling \nfrom convex sets.\n</div>\n\n</li>\n\n<br>\n\n<li class=\"paper robust_statistics\" id=\"paper75\">\nFaster Algorithms for High-Dimensional Robust Covariance Estimation\n<a onclick=\"reveal_abstract(75)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1906.04661\" class=\"abstract_link\">[arxiv]</a> \n<br> Y. Cheng, I. Diakonikolas, R. Ge, D. Woodruff\n<br> Proceedings of the 32nd Annual Conference on Learning Theory (COLT 2019)\n<div id=\"abstract75\" class=\"abstract\" style=\"display:none\">\n\nWe study the problem of estimating the covariance matrix of a high-dimensional distribution \nwhen a small constant fraction of the samples can be arbitrarily corrupted.\nRecent work gave the first polynomial time algorithms for this problem with near-optimal \nerror guarantees for several natural structured distributions. \nOur main contribution is to develop faster algorithms for this problem whose running time \nnearly matches that of computing the empirical covariance.\n\n<p>\n\nGiven $N = \\tilde{\\Omega}(d^2/\\epsilon^2)$ samples from a $d$-dimensional Gaussian distribution, \nan $\\epsilon$-fraction of which may be arbitrarily corrupted, our algorithm runs in time \n$\\tilde{O}(d^{3.26})/\\poly(\\epsilon)$ and approximates the unknown covariance matrix \nto optimal error up to a logarithmic factor. Previous robust algorithms with comparable error guarantees \nall have runtimes $\\tilde{\\Omega}(d^{2 \\omega})$ when $\\epsilon = \\Omega(1)$, where $\\omega$ is the \nexponent of matrix multiplication. We also provide evidence that improving the running time of our \nalgorithm may require new algorithmic techniques.\n</p></div>\n\n</li>\n\n<br>\n\n<li class=\"paper hypothesis_testing\" id=\"paper74\">\nCommunication and Memory Efficient Testing of Discrete Distributions\n<a onclick=\"reveal_abstract(74)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1906.04709\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, T. Gouleakis, D. Kane, S. Rao\n<br> Proceedings of the 32nd Annual Conference on Learning Theory (COLT 2019)\n<div id=\"abstract74\" class=\"abstract\" style=\"display:none\">\nWe study distribution testing with communication and memory constraints \nin the following computational models: (1) The {\\em one-pass streaming model} \nwhere the goal is to minimize the sample complexity of the protocol subject to a memory constraint,\nand (2) A {\\em distributed model} where the data samples reside at multiple machines and the\ngoal is to minimize the communication cost of the protocol. In both these models, we provide efficient\nalgorithms for uniformity/identity testing (goodness of fit) and closeness testing (two sample testing).\nMoreover, we show nearly-tight lower bounds on (1) the sample complexity\nof any one-pass streaming tester for uniformity, subject to the memory constraint, \nand (2) the communication cost of any uniformity testing protocol, \nin a restricted ``one-pass'' model of communication.\n</div>\n\n</li>\n\n<br>\n\n<li class=\"paper hypothesis_testing\" id=\"paper73\">\nTesting Identity of Multidimensional Histograms \n<a onclick=\"reveal_abstract(73)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1804.03636\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, J. Peebles\n<br> Proceedings of the 32nd Annual Conference on Learning Theory (COLT 2019)\n<div id=\"abstract73\" class=\"abstract\" style=\"display:none\">\nWe investigate the problem of identity testing for multidimensional histogram distributions.\nA distribution $p: D \\to \\R_+$, where $D \\subseteq \\R^d$, is called a $k$-histogram if there exists a partition of the\ndomain  into $k$ axis-aligned rectangles such that $p$ is constant within each such rectangle.\nHistograms are one of the most fundamental non-parametric families of distributions and\nhave been extensively studied in computer science and statistics.\nWe give the first identity tester for this problem with {\\em sub-learning} sample complexity\nin any fixed dimension and a nearly-matching sample complexity lower bound.\n\n<p>\n\nMore specifically, let $q$ be an unknown $d$-dimensional $k$-histogram and $p$ be an explicitly given $k$-histogram.\nWe want to correctly distinguish, with probability at least $2/3$, between the case that $p = q$ versus $\\|p-q\\|_1 \\geq \\eps$.\nWe design a computationally efficient algorithm for this hypothesis testing problem\nwith sample complexity $O((\\sqrt{k}/\\eps^2) \\log^{O(d)}(k/\\eps))$. Our algorithm is robust to model misspecification, i.e.,\nsucceeds even if $q$ is only promised to be {\\em close} to a $k$-histogram.\nMoreover, for $k = 2^{\\Omega(d)}$, we show a nearly-matching sample complexity lower bound of\n$\\Omega((\\sqrt{k}/\\eps^2) (\\log(k/\\eps)/d)^{\\Omega(d)})$ when $d\\geq 2$.\n\n</p><p>\n\nPrior to our work, the sample complexity of the $d=1$ case was well-understood,\nbut no algorithm with sub-learning sample complexity was known, even for $d=2$. Our new upper and lower bounds\nhave interesting conceptual implications regarding the relation between learning and testing in this setting.\n</p></div>\n\n\n</li>\n\n\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper72\">\nSever: A Robust Meta-Algorithm for Stochastic Optimization\n<a onclick=\"reveal_abstract(72)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1803.02815\" class=\"abstract_link\">[arxiv]</a> \n<a href=\"https://github.com/hoonose/sever\" class=\"abstract_link\">[code]</a>\n<br> I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, A. Stewart\n<br> Proceedings of the  36th International Conference on Machine Learning (ICML 2019)\n<br> <strong>Oral Presentation</strong> at the NeurIPS 2018 Workshop on Security in Machine Learning (SECML 2018)\n<div id=\"abstract72\" class=\"abstract\" style=\"display:none\">\n\nIn high dimensions, most machine learning methods are brittle to even a small \nfraction of structured outliers. To address this, we introduce a new \nmeta-algorithm that can take in a \\emph{base learner} such as least squares or stochastic \ngradient descent, and harden the learner to be resistant to outliers.\nOur method, Sever, possesses strong theoretical guarantees yet is also highly scalable---beyond \nrunning the base learner itself, it only requires computing the top singular vector of a certain\n$n \\times d$ matrix.\nWe apply Sever on a drug design dataset and a spam classification dataset, and \nfind that in both cases it has substantially greater robustness than several baselines.\nOn the spam dataset, with $1\\%$ corruptions, we achieved $7.4\\%$ test error, \ncompared to $13.4\\%-20.5\\%$ for the baselines, and $3\\%$ error on the uncorrupted dataset.\nSimilarly, on the drug design dataset, with $10\\%$ corruptions, we achieved $1.42$ mean-squared \ntest error, compared to $1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset. \n\n</div>\n\n</li>\n\n<br>\n\n\n\n<li class=\"paper robust_PAC\" id=\"paper71\">\nDegree-d Chow Parameters Robustly Determine Degree-d PTFs (and Algorithmic Applications)\n<a onclick=\"reveal_abstract(71)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://eccc.weizmann.ac.il/report/2018/189/download\" class=\"abstract_link\">[eccc]</a> \n<br> I. Diakonikolas, D. Kane\n<br> Proceedings of the 51st Annual ACM Symposium on Theory of Computing (STOC 2019)\n<div id=\"abstract71\" class=\"abstract\" style=\"display:none\">\n\nThe degree-$d$ Chow parameters of a Boolean function are its degree at most $d$ Fourier coefficients.\nIt is well-known that degree-$d$ Chow parameters uniquely characterize degree-$d$ polynomial threshold functions \n(PTFs) within the space of all bounded functions. In this paper, we prove a robust version of this theorem: \nFor $f$ any Boolean degree-$d$ PTF and $g$ any bounded function, if the degree-$d$ Chow parameters of\n$f$ are close to the degree-$d$ Chow parameters of $g$ in $\\ell_2$-norm, then $f$ is close to $g$ in $\\ell_1$-distance.\nNotably, our bound relating the two distances is independent of the dimension. That is,\nwe show that Boolean degree-$d$ PTFs are {\\em robustly identifiable} from their degree-$d$ Chow parameters.\nNo non-trivial bound was previously known for $d &gt;1$.\n\n<p>\n\nOur robust identifiability result gives the following algorithmic applications: \nFirst, we show that Boolean degree-$d$ PTFs can be efficiently approximately reconstructed\nfrom approximations to their degree-$d$ Chow parameters. This immediately implies\nthat degree-$d$ PTFs are efficiently learnable in the uniform distribution $d$-RFA model.\nAs a byproduct of our approach, we also obtain the first low integer-weight approximations of degree-$d$ PTFs, for $d&gt;1$.\nAs our second application, our robust identifiability result gives the first efficient \nalgorithm, with dimension-independent error guarantees, \nfor malicious learning of Boolean degree-$d$ PTFs under the uniform distribution.\n\n</p><p>\n\nThe proof of our robust identifiability result involves several new technical ingredients, including\nthe following structural result for degree-$d$ multivariate polynomials with very poor anti-concentration:\nIf $p$ is a degree-$d$ polynomial where $p(x)$ is {\\em very} close to $0$ on a {\\em large} number of points in $\\bn$,\nthen there exists a degree-$d$ hypersurface that exactly passes though {\\em almost all} of these points. \nWe leverage this structural result to show that if the degree-$d$ Chow distance between $f$ and $g$ is small, \nthen we can find many degree-$d$ polynomials that vanish on their disagreement region, \nand in particular enough that forces the $\\ell_1$-distance between $f$ and $g$ to also be small. \nTo implement this proof strategy, we require additional technical ideas. \nIn particular, in the $d=2$ case we show that for any large vector space of degree-$2$ polynomials \nwith a large number of common zeroes, there exists a linear function that vanishes on almost all of these zeroes. \nThe degree-$d$ degree generalization of this statement is significantly more complex, and can be viewed as an effective version \nof Hilbert's Basis Theorem for our setting.\n\n</p></div>\n\n\n</li>\n\n\n<br>\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper70\">\nLearning Wasserstein distance in spaces of low intrinsic dimension\n<br> T. Carpenter, I. Diakonikolas, A. Sidiropoulos\n<br> Manuscript\n\n</li>\n\n<br>\n\n<li class=\"paper nonparametric_statistics\" id=\"paper69\">\nEfficient Robust Proper Learning of Log-concave Distributions\n<a onclick=\"reveal_abstract(69)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://arxiv.org/abs/1606.03077\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Manuscript\n<div id=\"abstract69\" class=\"abstract\" style=\"display:none\">\n\nWe study the {\\it robust proper learning} of univariate log-concave distributions\n(over continuous and discrete domains). Given a set of samples drawn \nfrom an unknown target distribution, we want to compute a log-concave\nhypothesis distribution that is as close as possible to the target, in total variation distance.\nIn this work, we give the first computationally efficient algorithm\nfor this learning problem. Our algorithm achieves the information-theoretically optimal\nsample size (up to a constant factor), runs in polynomial time,\nand is robust to model misspecification with nearly-optimal\nerror guarantees.\n\n<p>\n\nSpecifically, we give an algorithm that,\non input $n=O(1/\\epsilon^{5/2})$ samples from an unknown distribution $f$,\nruns in time $\\widetilde{O}(n^{8/5})$,\nand outputs a log-concave hypothesis $h$ that (with high probability) satisfies\n$d_{\\mathrm{TV}}(h, f) = O(\\mathrm{opt})+\\epsilon$, where $\\mathrm{opt}$ \nis the minimum total variation distance between $f$\nand the class of log-concave distributions.\nOur approach to the robust proper learning problem is quite flexible and may be applicable\nto many other univariate distribution families.\n</p></div>\n\n</li>\n\n<br>\n\n\n\n<li class=\"paper game_theory\" id=\"paper68\">\nOn the Complexity of the Inverse Semivalue Problem for Weighted Voting Games\n<a onclick=\"reveal_abstract(68)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1812.11712\">[arxiv]</a>\n<br> I. Diakonikolas, C. Pavlou\n<br> Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI 2019)\n\n<div id=\"abstract68\" class=\"abstract\" style=\"display:none\">\nWeighted voting games are a family of cooperative games, \ntypically used to model voting situations where a number of agents (players) vote against or for a proposal.  \nIn such games, a proposal is accepted if an appropriately weighted sum of the votes \nexceeds a prespecified threshold. As the influence of a player over the voting outcome \nis not in general proportional to her assigned weight, \nvarious power indices have been proposed to measure each player's influence. \nThe inverse power index problem is the problem of designing a weighted \nvoting game that achieves a set of target influences according to a predefined power index. \nIn this work, we study the computational complexity of the inverse problem when \nthe power index belongs to the class of semivalues. We prove that the inverse problem \nis computationally intractable for a broad family of semivalues, including all regular semivalues. \nAs a special case of our general result, we establish computational hardness \nof the inverse problem for the Banzhaf indices and the Shapley values, \narguably the most popular power indices. \n</div>\n</li>\n\n\n\n\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper67\">\nCollision-based Testers are Optimal for Uniformity and Closeness\n<a onclick=\"reveal_abstract(67)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://eccc.hpi-web.de/report/2016/178/download\" class=\"abstract_link\">[eccc]</a> \n<br>  I. Diakonikolas, T. Gouleakis, J. Peebles, E. Price\n<br> Chicago Journal of Theoretical Computer Science, 2019\n<br> Also see Oded Goldreich's exposition of our proof <em><font color=\"gray\">\n      <a href=\"https://eccc.weizmann.ac.il/report/2016/178/\">here</a></font></em>\n<div id=\"abstract67\" class=\"abstract\" style=\"display:none\">\nWe study the fundamental problems of (i) uniformity testing of a discrete distribution, \nand (ii) closeness testing between two discrete distributions with bounded $\\ell_2$-norm.  \nThese problems have been extensively studied in distribution testing\nand sample-optimal estimators are known for them~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n\n<p>\n\nIn this work, we show that the original collision-based testers proposed for these problems\n~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant factors. \nPrevious analyses showed sample complexity upper bounds for these testers that are optimal\nas a function of the domain size $n$, but suboptimal by polynomial factors \nin the error parameter $\\epsilon$. Our main contribution is a new tight analysis \nestablishing that these collision-based testers are information-theoretically optimal, \nup to constant factors, both in the dependence on $n$ and in the dependence on $\\epsilon$.\n</p></div>\n</li>\n\n\n\n<br>\n\n<li class=\"paper robust_statistics\" id=\"paper66\">\nHigh-Dimensional Robust Mean Estimation in Nearly-Linear Time\n<a onclick=\"reveal_abstract(66)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1811.09380\" class=\"abstract_link\">[arxiv]</a> \n<br> Y. Cheng, I. Diakonikolas, R. Ge\n<br> Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2019)\n<div id=\"abstract66\" class=\"abstract\" style=\"display:none\">\n\nWe study the fundamental problem of high-dimensional mean estimation in a robust model where a constant fraction of the samples are adversarially corrupted.\nRecent work gave the first polynomial time algorithms for this problem with dimension-independent error guarantees for several families of structured distributions. \n\n<p>\n\n\nIn this work, we give the first nearly-linear time algorithms for high-dimensional robust mean estimation.\nSpecifically, we focus on distributions with (i) known covariance and sub-gaussian tails, and (ii) unknown bounded covariance.\nGiven $N$ samples on $\\mathbb{R}^d$, an $\\epsilon$-fraction of which may be arbitrarily corrupted, our algorithms run in time \n$\\tilde{O}(Nd) / \\mathrm{poly}(\\epsilon)$ and approximate the true mean within the information-theoretically optimal error, up to constant factors.\nPrevious robust algorithms with comparable error guarantees have running times $\\tilde{\\Omega}(N d^2)$, for $\\epsilon = \\Omega(1)$.\n\n</p><p>\n\n\nOur algorithms rely on a natural family of SDPs parameterized by our current guess $\\nu$ for the unknown mean $\\mu^\\star$.\nWe give a win-win analysis establishing the following: either a near-optimal solution to the primal SDP yields a good candidate \nfor $\\mu^\\star$ -- independent of our current guess $\\nu$ -- or the dual SDP yields a new guess $\\nu'$ whose distance from $\\mu^\\star$ is smaller by a constant factor.\nWe exploit the special structure of the corresponding SDPs to show that they are approximately solvable in nearly-linear time.\nOur approach is quite general, and we believe it can also be applied to obtain nearly-linear time algorithms for other high-dimensional robust learning problems.\n</p></div>\n\n\n\n</li>\n\n<br>\n\n<li class=\"paper robust_statistics\" id=\"paper65\">\nEfficient Algorithms and Lower Bounds for Robust Linear Regression\n<a onclick=\"reveal_abstract(65)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1806.00040\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, W. Kong, A. Stewart\n<br> Proceedings of the 30th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2019)\n<div id=\"abstract65\" class=\"abstract\" style=\"display:none\">\n\nWe study the prototypical problem of high-dimensional linear regression in a robust model where an $\\epsilon$-fraction\nof the samples can be adversarially corrupted. We focus on the fundamental setting where the covariates\nof the uncorrupted samples are drawn from a Gaussian distribution $\\mathcal{N}(0, \\Sigma)$ on $\\R^d$. \nWe give nearly tight upper bounds and computational lower bounds for this problem.\nSpecifically, our main contributions are as follows:\n\n\n<p>\n\nFor the case that the covariance matrix is known to be the identity, \nwe give a sample near-optimal and computationally efficient algorithm that draws $\\tilde{O}(d/\\eps^2)$ \nlabeled examples and outputs a candidate hypothesis vector $\\widehat{\\beta}$ that approximates the unknown regression vector \n$\\beta$ within $\\ell_2$-norm $O(\\eps \\log(1/\\eps) \\sigma)$, where $\\sigma$ is the standard deviation\nof the random observation noise. An error of $\\Omega (\\eps \\sigma)$ is information-theoretically\nnecessary, even with infinite sample size. Hence, the error guarantee of our algorithm is\noptimal, up to a logarithmic factor in $1/\\eps$. \nPrior work gave an algorithm for this problem with sample complexity $\\tilde{\\Omega}(d^2/\\eps^2)$ \nwhose error guarantee scales with the $\\ell_2$-norm of $\\beta$.\n\n</p><p>\n\n\nFor the case of unknown covariance $\\Sigma$,\nwe show that we can efficiently achieve the same error guarantee of $O(\\eps \\log(1/\\eps) \\sigma)$, \nas in the known covariance case, using an additional $\\tilde{O}(d^2/\\eps^2)$ unlabeled examples. \nOn the other hand, an error of $O(\\eps \\sigma)$ can be information-theoretically attained with $O(d/\\eps^2)$ samples.\nWe prove a Statistical Query (SQ) lower bound providing evidence that this quadratic \ntradeoff in the sample size is inherent. More specifically, we show that any polynomial time \nSQ learning algorithm for robust linear regression (in Huber's contamination model)\nwith estimation complexity $O(d^{2-c})$,  where $c&gt;0$ is an arbitrarily small constant, \nmust incur an error of $\\Omega(\\sqrt{\\eps} \\sigma)$.\n\n\n</p></div>\n\n</li>\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper64\">\nFourier-based Testing for Families of Distributions\n<a onclick=\"reveal_abstract(64)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://eccc.weizmann.ac.il/report/2017/075/\" class=\"abstract_link\">[eccc]</a> \n<br> C. Canonne, I. Diakonikolas, A. Stewart\n<br> Advances in Neural Information Processing Systems (NeurIPS 2018) \n\n<div id=\"abstract64\" class=\"abstract\" style=\"display:none\">\nWe study the general problem of testing whether an unknown discrete distribution belongs to a given family of distributions.\nMore specifically, given a class of distributions $\\mathcal{P}$ and sample access to an unknown distribution $p$,\nwe want to distinguish (with high probability) between the case that $p \\in \\mathcal{P}$ and the case\nthat $p$ is $\\epsilon$-far, in total variation distance, from every distribution in $\\mathcal{P}$.\nThis is the prototypical hypothesis testing problem that has received significant attention in statistics and, \nmore recently, in theoretical computer science.\n\n<p>\n\nThe sample complexity of this general problem depends on the underlying family  $\\mathcal{P}$.\nWe are interested in designing sample-optimal and computationally efficient algorithms for this task. \nThe main contribution of this work is a new and simple testing technique that is applicable to distribution families \nwhose \\emph{Fourier spectrum} approximately satisfies a certain \\emph{sparsity} property. As the main applications\nof our Fourier-based testing technique, we obtain the first non-trivial testers for two fundamental families of discrete distributions:\nSums of Independent Integer Random Variables (SIIRVs) and Poisson Multinomial Distributions (PMDs).\nOur testers for these families are nearly sample-optimal and computationally efficient. We also obtain \na tester with improved sample complexity for discrete log-concave distributions.\nTo the best of our knowledge, ours is the first use of the Fourier transform in the context of distribution testing.\n</p></div>\n\n</li>\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper63\">\nSharp Bounds for Generalized Uniformity Testing\n<a onclick=\"reveal_abstract(63)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1709.02087\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Advances in Neural Information Processing Systems (NeurIPS 2018)\n<br> Selected for <strong>Spotlight Presentation</strong> at NeurIPS 2018\n\n<br> See the comments in <em><font color=\"gray\">Oded Goldreich's Choices \n      <a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/229.html\">#229</a></font></em>\n<div id=\"abstract663\" class=\"abstract\" style=\"display:none\">\nWe study the problem of {\\em generalized uniformity testing}~\\cite{BC17} of a discrete probability distribution:\nGiven samples from a probability distribution $p$ over an {\\em unknown} discrete domain $\\mathbf{\\Omega}$,\nwe want to distinguish, with probability at least $2/3$, between the case that $p$\nis uniform on some {\\em subset} of $\\mathbf{\\Omega}$ versus $\\epsilon$-far,\nin total variation distance, from any such uniform distribution.\n\nWe establish tight bounds on the sample complexity of generalized uniformity testing.\nIn more detail, we present a computationally efficient tester\nwhose sample complexity is optimal, up to constant factors,\nand a matching information-theoretic lower bound.\nSpecifically, we show that the sample complexity of generalized uniformity testing is\n$\\Theta\\left(1/(\\epsilon^{4/3}\\|p\\|_3) + 1/(\\epsilon^{2} \\|p\\|_2) \\right)$.\n</div>\n\n\n\n</li>\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper62\">\nRobust Learning of Fixed-Structure Bayesian Networks\n<a onclick=\"reveal_abstract(62)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://arxiv.org/abs/1606.07384\" class=\"abstract_link\">[arxiv]</a> \n<br> Y. Cheng, I. Diakonikolas, D. Kane, A. Stewart\n<br> Advances in Neural Information Processing Systems (NeurIPS 2018)\n\n<div id=\"abstract62\" class=\"abstract\" style=\"display:none\">\nWe investigate the problem of learning Bayesian networks in an agnostic model\nwhere an $\\epsilon$-fraction of the samples are adversarially corrupted.\nOur agnostic learning model is similar to -- in fact, stronger than -- Huber's\ncontamination model in robust statistics. In this work, we study the fully observable\nBernoulli case where the structure of the network is given.\nEven in this basic setting, previous learning algorithms\neither run in exponential time or lose dimension-dependent factors in their\nerror guarantees.\nWe provide the first computationally efficient agnostic learning algorithm for this problem\nwith dimension-independent error guarantees. Our algorithm has polynomial sample complexity,\nruns in polynomial time, and achieves error that scales nearly-linearly with the fraction\nof adversarially corrupted samples.\n</div>\n</li>\n\n<br>\n\n<li class=\"paper hypothesis_testing\" id=\"paper61\">\nDifferentially Private Identity and Closeness Testing of Discrete Distributions\n<a onclick=\"reveal_abstract(61)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1707.05497\" class=\"abstract_link\">[arxiv]</a> \n<br> M. Aliakbarpour, I. Diakonikolas, R. Rubinfeld\n<br> Proceedings of the  35th International Conference on Machine Learning (ICML 2018)\n<div id=\"abstract61\" class=\"abstract\" style=\"display:none\">\nWe investigate the problems of identity and closeness testing over a discrete population\nfrom random samples. Our goal is to develop efficient testers while guaranteeing\nDifferential Privacy to the individuals of the population. We describe an approach that \nyields sample-efficient differentially private testers for these problems.\nOur theoretical results show that there exist private identity and closeness testers\nthat are nearly as sample-efficient as their non-private counterparts. We perform\nan experimental evaluation of our algorithms on synthetic data. Our experiments\nillustrate that our private testers achieve small type I and type II errors with sample size\n{\\em sublinear} in the domain size of the underlying distributions.\n</div>\n\n</li>\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper60\">\nNear-Optimal Sample Complexity Bounds for Maximum Likelihood Estimation of Multivariate Log-concave Densities\n<a onclick=\"reveal_abstract(60)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1802.10575\" class=\"abstract_link\">[arxiv]</a> \n<br> T. Carpenter, I. Diakonikolas, A. Sidiropoulos, A. Stewart\n<br> Proceedings of the 31st Annual Conference on Learning Theory (COLT 2018) </li>\n<div id=\"abstract60\" class=\"abstract\" style=\"display:none\">\nWe study the problem of learning multivariate log-concave densities\nwith respect to a global loss function. We obtain the first upper bound on the sample complexity \nof the maximum likelihood estimator (MLE) for a log-concave density on $\\mathbb{R}^d$, for all $d \\geq 4$.\nPrior to this work, no finite sample upper bound was known for this estimator in more than $3$ dimensions.\n\n<p>\n\nIn more detail, we prove that for any $d \\geq 1$ and $\\epsilon&gt;0$, given \n$\\tilde{O}_d((1/\\epsilon)^{(d+3)/2})$ samples drawn from an unknown log-concave density $f_0$ on $\\mathbb{R}^d$,\nthe MLE outputs a hypothesis $h$ that with high probability is $\\epsilon$-close\nto $f_0$, in squared Hellinger loss. A sample complexity lower bound of $\\Omega_d((1/\\epsiilon)^{(d+1)/2})$\nwas previously known for any learning algorithm that achieves this guarantee. \nWe thus establish that the sample complexity of the log-concave MLE is near-optimal, \nup to an $\\tilde{O}(1/\\epsilon)$ factor.\n\n</p></div>\n\n\n\n\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper59\">\nFast and Sample Near-Optimal Algorithms for Learning Multidimensional Histograms\n<a onclick=\"reveal_abstract(59)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1802.08513\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, J. Li, L. Schmidt\n<br> Proceedings of the 31st Annual Conference on Learning Theory (COLT 2018) </li>\n<div id=\"abstract59\" class=\"abstract\" style=\"display:none\">\nWe study the problem of robustly learning multi-dimensional histograms. \nA $d$-dimensional function $h: D \\to \\R$ is called a $k$-histogram if there exists a partition of the \ndomain $D \\subseteq \\R^d$ into $k$ axis-aligned rectangles such that $h$ is constant within each such rectangle.\nLet $f: D \\to \\R$ be a $d$-dimensional probability density function \nand suppose that $f$ is $\\mathrm{OPT}$-close, in $L_1$-distance, \nto an unknown $k$-histogram (with unknown partition). Our goal is to output a hypothesis\nthat is $O(\\mathrm{OPT}) + \\epsilon$ close to $f$, in $L_1$-distance. We give an algorithm for this learning \nproblem that uses  $n = \\tilde{O}_d(k/\\eps^2)$ samples and runs in time $\\tilde{O}_d(n)$.\nFor any fixed dimension, our algorithm has optimal sample complexity, up to logarithmic factors,\nand runs in near-linear time. Prior to our work, the time complexity of the $d=1$ case was well-understood, \nbut significant gaps in our understanding remained even for $d=2$.\n</div>\n\n\n\n\n<br>\n\n<li class=\"paper hypothesis_testing\" id=\"paper58\">\nSample-Optimal Identity Testing with High Probability\n<a onclick=\"reveal_abstract(58)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1708.02728\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, T. Gouleakis, J. Peebles, E. Price\n<br> Proceedings of the 45th Intl. Colloquium on Automata, Languages and Programming (ICALP 2018)\n<br> See the comments in <em><font color=\"gray\">Oded Goldreich's Choices \n      <a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/229.html\">#229</a></font></em>\n<div id=\"abstract58\" class=\"abstract\" style=\"display:none\">\nWe study the problem of testing identity against a given\ndistribution (a.k.a. goodness-of-fit) with a focus on the high\nconfidence regime. More precisely, given samples from\nan unknown distribution $p$ over $n$ elements, an explicitly given\ndistribution $q$, and parameters $0&lt; \\epsilon, \\delta &lt; 1$, we wish\nto distinguish, {\\em with probability at least $1-\\delta$}, whether\nthe distributions are identical versus $\\epsilon$-far in total variation (or statistical) distance.\nExisting work has focused on\nthe constant confidence regime, i.e., the case that\n$\\delta = \\Omega(1)$, for which the sample complexity of\nidentity testing is known to be $\\Theta(\\sqrt{n}/\\epsilon^2)$.\n \n<p>\n\nTypical applications of distribution property testing\nrequire small values of the confidence parameter $\\delta$ (which correspond to small\n``$p$-values'' in the statistical hypothesis testing terminology). Prior work achieved arbitrarily small values \nof $\\delta$ via black-box amplification, which multiplies the required number of samples by\n$\\Theta(\\log(1/\\delta))$.  We show that this upper bound is suboptimal for any\n$\\delta = o(1)$, and give a new identity tester that achieves the\noptimal sample complexity.  Our new upper and lower bounds show that\nthe optimal sample complexity of identity testing is\n\\[\n  \\Theta\\left( \\frac{1}{\\epsilon^2}\\left(\\sqrt{n \\log(1/\\delta)} + \\log(1/\\delta) \\right)\\right)\n\\]\nfor any $n, \\epsilon$, and $\\delta$.  For the special case of uniformity\ntesting, where the given distribution is the uniform distribution $U_n$ over the domain, \nour new tester is surprisingly simple: to test whether $p = U_n$ versus $\\mathrm{d}_{TV}(p, U_n) \\geq \\epsilon$, \nwe simply threshold $\\mathrm{d}_{TV}(\\hat{p}, U_n)$, where $\\hat{p}$ is the empirical probability distribution.\nWe believe that our novel analysis techniques may be useful for\nother distribution testing problems as well.\n\n\n</p></div>\n\n</li>\n\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper57\">\nTesting Conditional Independence of Discrete Distributions\n<a onclick=\"reveal_abstract(57)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://arxiv.org/abs/1711.11560\" class=\"abstract_link\">[arxiv]</a> \n<br>  C. Canonne, I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 50th Annual ACM Symposium on Theory of Computing (STOC 2018)</li>\n<div id=\"abstract57\" class=\"abstract\" style=\"display:none\">\nWe study the problem of testing \\emph{conditional independence} for discrete distributions.\nSpecifically, given samples from a discrete random variable $(X, Y, Z)$ on domain $[\\ell_1]\\times[\\ell_2] \\times [n]$, \nwe want to distinguish, with probability at least $2/3$, between the case that $X$ and $Y$ are conditionally independent \ngiven $Z$ from the case that $(X, Y, Z)$ is $\\eps$-far, in $\\lp[1]$-distance, from every distribution that has this property.\nConditional independence is a concept of central importance in probability and statistics with a range of applications\nin various scientific domains. As such, the statistical task of testing conditional independence has been extensively studied\nin various forms within the statistics and econometrics communities for nearly a century. \nPerhaps surprisingly, this problem has not been previously considered in the framework of distribution property testing \nand in particular no tester with sublinear sample complexity is known, even for the important special case that the domains of $X$ and $Y$ are binary.\n\n<p>\n\nThe main algorithmic result of this work is the first conditional independence tester with {\\em sublinear} sample complexity for \ndiscrete distributions over $[\\ell_1]\\times[\\ell_2] \\times [n]$. \nTo complement our upper bounds, we prove information-theoretic lower bounds establishing \nthat the sample complexity of our algorithm is optimal, up to constant factors, for a number of settings.\nSpecifically, for the prototypical setting when $\\ell_1, \\ell_2 = O(1)$, we show that the sample complexity of testing \nconditional independence (upper bound and matching lower bound) is \n \\[\n      \\bigTheta{\\max\\left(n^{1/2}/\\eps^2,\\min\\mleft(n^{7/8}/\\eps,n^{6/7}/\\eps^{8/7}\\mright)\\right)}\\,.\n  \\]\n\n</p><p>\n\nTo obtain our tester, we employ a variety of tools, including \n(1) a suitable weighted adaptation of the flattening technique~\\cite{DK:16},\nand (2) the design and analysis of an optimal (unbiased) estimator \nfor the following statistical problem of independent interest: \nGiven a degree-$d$ polynomial $Q\\colon\\mathbb{R}^n \\to \\R$ \nand sample access to a distribution $p$ over $[n]$, \nestimate $Q(p_1, \\ldots, p_n)$ up to small additive error. \nObtaining tight variance analyses for specific estimators of this form\nhas been a major technical hurdle in distribution testing (see, e.g.,~\\cite{CDVV14}). \nAs an important contribution of this work, we develop a general theory \nproviding tight variance bounds for \\emph{all} such estimators. Our lower bounds, established\nusing the mutual information method, rely on novel constructions of hard instances \nthat may be useful in other settings.\n</p></div>\n\n\n\n\n<br>\n\n\n\n<li class=\"paper robust_statistics\" id=\"paper56\">\nList-Decodable Robust Mean Estimation and Learning Mixtures of Spherical Gaussians\n<a onclick=\"reveal_abstract(56)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1711.07211\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 50th Annual ACM Symposium on Theory of Computing (STOC 2018)</li>\n<div id=\"abstract56\" class=\"abstract\" style=\"display:none\">\nWe study the problem of {\\emph list-decodable (robust) Gaussian mean estimation} and the related problem\nof {\\emph learning mixtures of separated spherical Gaussians}. In the former problem, we are given a set $T$ \nof points in $\\mathbb{R}^n$ with the promise that an $\\alpha$-fraction of points in $T$, where $0&lt; \\alpha &lt; 1/2$, \nare drawn from an unknown mean identity covariance Gaussian $G$, \nand no assumptions are made about the remaining points.\nThe goal is to output a small list of candidate vectors with the guarantee that at least one of \nthe candidates is close to the mean of $G$. In the latter problem, we are given samples from a $k$-mixture\nof spherical Gaussians on $\\mathbb{R}^n$ and the goal is to estimate the unknown model parameters up to small accuracy.\nWe develop a set of techniques that yield new efficient algorithms with significantly improved\nguarantees for these problems. Specifically, our main contributions are as follows:\n\n<p>\n\nList-Decodable Mean Estimation. Fix any $d \\in \\mathbb{Z}_+$ and $0&lt; \\alpha &lt;1/2$.\n We design an algorithm with sample complexity $O_d (\\mathrm{poly}(n^d/\\alpha))$ and runtime \n$O_d (\\mathrm{poly}(n/\\alpha)^{d})$\nthat outputs a list of $O(1/\\alpha)$ many candidate vectors such that with high probability\none of the candidates is within $\\ell_2$-distance $O_d(\\alpha^{-1/(2d)})$ from the mean of $G$.\nThe only previous algorithm for this problem~\\cite{CSV17}\nachieved error $\\tilde O(\\alpha^{-1/2})$ under second moment conditions.\nFor $d = O(1/\\eps)$, where $\\eps&gt;0$ is a constant, \nour algorithm runs in polynomial time and achieves error $O(\\alpha^{\\eps})$.\nFor $d = \\Theta(\\log(1/\\alpha))$, our algorithm runs in time $(n/\\alpha)^{O(\\log(1/\\alpha))}$\nand achieves error $O(\\log^{3/2}(1/\\alpha))$, almost matching the information-theoretically\noptimal bound of $\\Theta(\\log^{1/2}(1/\\alpha))$ that we establish.\nWe also give a Statistical Query (SQ) lower bound \nsuggesting that the complexity of our algorithm is qualitatively close to best possible. \n\n</p><p>\n\nLearning Mixtures of Spherical Gaussians. We give a learning algorithm\nfor mixtures of spherical Gaussians,\nwith unknown spherical covariances, that succeeds under significantly weaker\nseparation assumptions compared to prior work. For the prototypical case \nof a uniform $k$-mixture of identity covariance Gaussians we obtain the following:\nFor any $\\eps&gt;0$, if the pairwise separation between the means is at least \n$\\Omega(k^{\\epsilon}+\\sqrt{\\log(1/\\delta)})$, our algorithm learns the unknown parameters \nwithin accuracy $\\delta$ with sample complexity and running time $\\poly (n, 1/\\delta, (k/\\epsilon)^{1/\\epsilon})$. \nMoreover, our algorithm is robust to a small dimension-independent\nfraction of corrupted data. The previously best known polynomial time algorithm~\\cite{VempalaWang:02} \nrequired separation at least $k^{1/4} \\polylog(k/\\delta)$. \nFinally, our algorithm works under separation of $\\new{\\tilde O(\\log^{3/2}(k)+\\sqrt{\\log(1/\\delta)})}$\nwith  sample complexity and running time $\\poly(n, 1/\\delta, k^{\\log k})$. \nThis bound is close to the information-theoretically minimum separation of $\\Omega(\\sqrt{\\log k})$~\\cite{RV17}.\n\n</p><p>\n\nOur main technical contribution is a new technique, using degree-$d$ multivariate polynomials, \nto remove outliers from high-dimensional datasets where the majority of the points are corrupted.\n</p></div>\n\n\n\n\n\n<br>\n\n\n<li class=\"paper robust_PAC\" id=\"paper55\">\nLearning Geometric Concepts with Nasty Noise\n<a onclick=\"reveal_abstract(55)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1707.01242\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 50th Annual ACM Symposium on Theory of Computing (STOC 2018)</li>\n<div id=\"abstract55\" class=\"abstract\" style=\"display:none\">\nWe study the efficient learnability of geometric concept classes --  \nspecifically, low-degree polynomial threshold functions (PTFs)\nand intersections of halfspaces -- when a fraction of the \ntraining data is adversarially corrupted. We give the first polynomial-time \nPAC learning algorithms for these concept classes with {\\em dimension-independent} error guarantees\nin the presence of {\\em nasty noise} under the Gaussian distribution. In the nasty noise model, \nan omniscient adversary can arbitrarily corrupt a small fraction of both the unlabeled data points and their labels.\nThis model generalizes well-studied noise models,\nincluding the malicious noise model and the agnostic (adversarial label noise) model. \nPrior to our work, the only concept class for which efficient malicious learning\nalgorithms were known was the class of {\\em origin-centered} halfspaces.\n\n<p>\n\nSpecifically, our robust learning algorithm for low-degree PTFs \nsucceeds under a number of tame distributions -- including the Gaussian distribution \nand, more generally, any log-concave distribution with (approximately) known low-degree moments. \nFor LTFs under the Gaussian distribution, we give \na polynomial-time algorithm that achieves error $O(\\epsilon)$, where $\\epsilon$ is the noise rate. \nAt the core of our PAC learning results is an efficient algorithm \nto approximate the {\\em low-degree Chow-parameters}\nof any bounded function in the presence of nasty noise.\nTo achieve this, we employ an iterative spectral method for outlier detection and removal, \ninspired by recent work in robust unsupervised learning. \nOur aforementioned algorithm succeeds for a range of distributions satisfying\nmild concentration bounds and moment assumptions.\nThe correctness of our robust learning algorithm for intersections of halfspaces \nmakes essential use of a novel robust inverse independence lemma \nthat may be of broader interest.\n</p></div>\n\n\n\n\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper54\">\nRobustly Learning a Gaussian: Getting Optimal Error, Efficiently\n<a onclick=\"reveal_abstract(54)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1704.03866\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, G. Kamath, D. Kane, J. Li\u0003, A. Moitra, A. Stewart\n<br> Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2018)\n<div id=\"abstract54\" class=\"abstract\" style=\"display:none\">\nWe study the fundamental problem of learning the parameters of a high-dimensional Gaussian \nin the presence of noise - where an $\\epsilon$-fraction of our samples were chosen by an adversary. \nWe give robust estimators that achieve estimation error $O(\\epsilon)$ in the total variation distance, \nwhich is optimal up to a universal constant that is independent of the dimension. \n\n<p>\n\nIn the case where just the mean is unknown, our robustness guarantee is optimal up to a factor of $\\sqrt{2}$ \nand the running time is polynomial in $d$ and $1/\\epsilon$. When both the mean and covariance are unknown, \nthe running time is polynomial in $d$ and quasipolynomial in $1/\\epsilon$. Moreover all of our algorithms \nrequire only a polynomial number of samples. Our work shows that the same sorts of error guarantees \nthat were established over fifty years ago in the one-dimensional setting can also be achieved \nby efficient algorithms in high-dimensional settings. \n\n</p></div>\n\n</li>\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper53\">\nCommunication-Efficient Distributed Learning of Discrete Distributions\n<a onclick=\"reveal_abstract(53)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://papers.nips.cc/book/advances-in-neural-information-processing-systems-30-2017\" class=\"abstract_link\">[proceedings]</a> \n<br> I. Diakonikolas, E. Grigorescu, J. Li, A. Natarajan, K. Onak, L. Schmidt\n<br> Advances in Neural Information Processing Systems (NIPS 2017)\n<br> Selected for <strong>Oral Presentation</strong> at NIPS 2017\n<div id=\"abstract53\" class=\"abstract\" style=\"display:none\">\n\nWe initiate a systematic investigation of density estimation \nwhen the data is {\\em distributed} across multiple servers. \nThe servers must communicate with a referee and the goal is to estimate \nthe underlying distribution with as few bits of communication as possible. \nWe focus on non-parametric density estimation of discrete distributions \nwith respect to the $\\ell_1$ and $\\ell_2$ norms. We provide the first non-trivial \nupper and lower bounds on the communication complexity of this basic estimation \ntask in various settings of interest. \n\n<p>\n\nWhen the unknown discrete distribution is {\\em unstructured} and each server \nhas only one sample, we show that any {\\em blackboard} protocol \n(i.e., any protocol in which servers interact arbitrarily using public messages) \nthat  learns the distribution must essentially communicate the entire sample. \nFor the case of {\\em structured} distributions, such as $k$-histograms and monotone distributions, \nwe design distributed learning algorithms that achieve significantly better communication \nguarantees than the naive ones, and obtain tight upper and lower bounds in several regimes. \nOur distributed learning algorithms run in near-linear time and are robust to model misspecification.\n\n</p><p>\n\nOur results provide insights on the interplay between structure and communication efficiency for a range\nof fundamental distribution estimation tasks.\n</p></div>\n\n\n</li>\n\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper52\">\nStatistical Query Lower Bounds for Robust Estimation of High-dimensional Gaussians and Gaussian Mixtures\n<a onclick=\"reveal_abstract(52)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://eccc.weizmann.ac.il/report/2016/177/revision/1/download\" class=\"abstract_link\">[eccc]</a> \n<a href=\"https://arxiv.org/abs/1611.03473\" class=\"abstract_link\">[arxiv]</a> \n<br>  I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2017)\n<br> See <a href=\"https://www.youtube.com/watch?time_continue=4&amp;v=IIp4CDab2N4\">here</a>\nand <a href=\"https://mediaspace.gatech.edu/media/Statistical+Query+Lower+Bounds+for+High-Dimensional+Unsupervised+Learning+-+Ilias+Diakonikolas/1_bqzha5b1\">here</a>\nfor videos of related talks.\n\n<div id=\"abstract52\" class=\"abstract\" style=\"display:none\">\nWe prove the first {\\em Statistical Query lower bounds} for\ntwo fundamental high-dimensional learning problems involving \nGaussian distributions: (1) learning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning\nof a single unknown mean Gaussian. In particular, we show a {\\em super-polynomial gap} between the (information-theoretic)\nsample complexity and the complexity of {\\em any} Statistical Query algorithm for these problems. \nStatistical Query (SQ) algorithms are a class of algorithms\nthat  are only allowed to query expectations of functions of the distribution rather than directly access samples.\nThis class of algorithms is quite broad: with the sole exception of Gaussian elimination over finite fields,\nall known algorithmic approaches in machine learning can be implemented in this model.\n\n<p>\n\nOur SQ lower bound for Problem (1)\nis qualitatively matched by known learning algorithms for GMMs (all of which can be implemented as SQ algorithms).\nAt a conceptual level, this result implies that -- as far as SQ algorithms are concerned -- the computational complexity \nof learning GMMs is inherently exponential \n{\\it in the dimension of the latent space} -- even though there \nis no such information-theoretic barrier. Our lower bound for Problem (2) implies that the accuracy of the robust learning algorithm \nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all polynomial-time SQ algorithms.\nOn the positive side, we give a new SQ learning algorithm for this problem \nwith optimal accuracy whose running time nearly matches our lower bound.\nBoth our SQ lower bounds are attained via a unified moment-matching technique that may be useful in other contexts.\nOur SQ learning algorithm for Problem (2) relies on a filtering technique that removes outliers based on higher-order tensors.\n\n</p><p>\n\nOur lower bound technique also has implications for related inference problems,\nspecifically for the problem of robust {\\it testing} of an unknown mean Gaussian. \nHere we show an information-theoretic lower bound \nwhich separates the sample complexity of the robust testing problem from its non-robust variant.\nThis result is surprising because such a separation does not exist\nfor the corresponding learning problem.\n</p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper robust_statistics\" id=\"paper51\">\nBeing Robust (in High Dimensions) Can be Practical\n<a onclick=\"reveal_abstract(51)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"https://arxiv.org/abs/1703.00893\" class=\"abstract_link\">[arxiv]</a> \n<a href=\"https://github.com/hoonose/robust-filter\" class=\"abstract_link\">[code]</a>\n<br> I. Diakonikolas, G. Kamath, D. Kane, J. Li, A. Moitra, A. Stewart\n<br> Proceedings of the  34th International Conference on Machine Learning (ICML 2017)\n<div id=\"abstract51\" class=\"abstract\" style=\"display:none\">\nRobust estimation is much more challenging in high dimensions than it is in one dimension: \nMost techniques either lead to intractable optimization problems or estimators \nthat can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, \nin appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time \nalgorithms that can tolerate a constant fraction of corruptions, independent of the dimension. \nHowever, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. \nIn this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, \nas well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. \nFinally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make \nhigh-dimensional robust estimation a realistic possibility.  </div>\n</li>\n\n<br>\n\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper50\">\nTesting Bayesian Networks\n<a onclick=\"reveal_abstract(50)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://arxiv.org/abs/1612.03156\" class=\"abstract_link\">[arxiv]</a> \n<br>  C. Canonne, I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 30th Annual Conference on Learning Theory (COLT 2017) \n<br> Journal version in IEEE Transactions on Information Theory, to appear.\n<div id=\"abstract50\" class=\"abstract\" style=\"display:none\">\nThis work initiates a systematic investigation of testing {\\em high-dimensional} structured \ndistributions by focusing on testing {\\em Bayesian networks} -- \nthe prototypical family of directed graphical models. A Bayesian network \nis defined by a directed acyclic graph, where we associate a random variable with each node. \nThe value at any particular node is conditionally independent of all the other non-descendant nodes once its parents are fixed. \nSpecifically, we study the properties of identity testing and closeness testing of Bayesian networks. Our main contribution is \nthe first non-trivial efficient testing algorithms for these problems and corresponding information-theoretic lower bounds. \nFor a wide range of parameter settings, our testing algorithms have sample complexity {\\em sublinear} in the dimension\nand are sample-optimal, up to constant factors.\n</div>\n\n</li>\n\n\n<br>\n\n\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper49\">\nLearning Multivariate Log-concave Distributions\n<a onclick=\"reveal_abstract(49)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://arxiv.org/abs/1605.08188\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 30th Annual Conference on Learning Theory (COLT 2017) </li>\n<div id=\"abstract49\" class=\"abstract\" style=\"display:none\">\nWe study the problem of estimating multivariate log-concave probability density functions.\nWe prove the first sample complexity upper bound for learning log-concave densities \non $\\mathbb{R}^d$, for all $d \\geq 1$. Prior to our work, no upper bound on the \nsample complexity of this learning problem was known for the case of $d&gt;3$.\n\n<p>\n\nIn more detail, we give an estimator that, for any $d \\ge 1$ and $\\epsilon&gt;0$,\ndraws $\\tilde{O}_d \\left( (1/\\epsilon)^{(d+5)/2} \\right)$ samples from an unknown \ntarget log-concave density on $\\mathbb{R}^d$, and outputs a hypothesis that \n(with high probability) is $\\epsilon$-close to the target, in total variation distance. \nOur upper bound on the sample complexity comes close to the known lower bound of\n$\\Omega_d \\left( (1/\\epsilon)^{(d+1)/2} \\right)$ for this problem.\n</p></div>\n\n\n\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper48\">\nNear-optimal Closeness Testing of Discrete Histogram Distributions\n<a onclick=\"reveal_abstract(48)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"https://arxiv.org/abs/1703.01913\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, V. Nikishkin\n<br> Proceedings of the 44th Intl. Colloquium on Automata, Languages and Programming (ICALP 2017)\n<div id=\"abstract48\" class=\"abstract\" style=\"display:none\">\nWe investigate the problem of testing the equivalence between two discrete histograms.\nA {\\em $k$-histogram} over $[n]$ is a probability distribution that is piecewise constant over some set of $k$ intervals over $[n]$.\nHistograms have been extensively studied in computer science and statistics.\nGiven a set of samples from two $k$-histogram distributions $p, q$ over $[n]$,\nwe want to distinguish (with high probability) between the cases that $p = q$ and $\\|p-q\\|_1 \\geq \\epsilon$.\nThe main contribution of this paper is a new algorithm for this testing problem\nand a nearly matching information-theoretic lower bound. \nSpecifically, the sample complexity of our algorithm matches our lower bound up to a logarithmic factor, improving\non previous work by polynomial factors in the relevant parameters.\nOur algorithmic approach applies in a more general framework and yields improved sample upper bounds \nfor testing closeness of other structured distributions as well.\n</div>\n\n</li>\n\n\n<br>\n\n<li class=\"paper game_theory\" id=\"paper47\">\nPlaying Anonymous Games using Simple Strategies\n<a onclick=\"reveal_abstract(47)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://arxiv.org/abs/1608.07336\" class=\"abstract_link\">[arxiv]</a> \n<br> Y. Cheng, I. Diakonikolas, A. Stewart\n<br> Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2017)\n<div id=\"abstract47\" class=\"abstract\" style=\"display:none\">\nWe investigate the complexity of computing approximate Nash equilibria in anonymous games.\nOur main algorithmic result is the following: For any $n$-player anonymous game with a bounded \nnumber of strategies and any constant $\\delta&gt;0$, an $O(1/n^{1-\\delta})$-approximate Nash \nequilibrium can be computed in polynomial time.\nComplementing this positive result, we show that if there exists any constant $\\delta&gt;0$ \nsuch that an $O(1/n^{1+\\delta})$-approximate equilibrium can be computed in polynomial time, \nthen there is a fully polynomial-time approximation scheme for this problem.\n\n<p>\n\nWe also present a faster algorithm that, for any $n$-player $k$-strategy anonymous game, \nruns in time $\\tilde O((n+k) k n^k)$ and computes an $\\tilde O(n^{-1/3} k^{11/3})$-approximate equilibrium.\nThis algorithm follows from the existence of simple approximate equilibria of anonymous games, \nwhere each player plays one strategy with probability $1-\\delta$, for some small $\\delta$, \nand plays uniformly at random with probability $\\delta$.\n\n</p><p>\n\nOur approach exploits the connection between Nash equilibria in anonymous games and Poisson multinomial distributions (PMDs).\nSpecifically, we prove a new probabilistic lemma establishing the following: \nTwo PMDs,  with large variance in each direction, whose first few moments \nare approximately matching are close in total variation distance.\nOur structural result strengthens previous work by providing a smooth tradeoff \nbetween the variance bound and the number of matching moments.\n\n</p></div>\n</li>\n\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper46\">\nSample Optimal Density Estimation in Nearly-Linear Time\n<a onclick=\"reveal_abstract(46)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/fast-piecewise-poly.pdf\" class=\"abstract_link\">[pdf]</a>\n<a href=\"https://github.com/ludwigschmidt/ppoly_density\" class=\"abstract_link\">[code]</a>\n<br> J. Acharya, I. Diakonikolas, J. Li\u0003, L. Schmidt\u0003\n<br> Proceedings of the 28th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2017)\n</li>\n<div id=\"abstract46\" class=\"abstract\" style=\"display:none\">\nWe design a new, fast algorithm for agnostically learning univariate probability distributions\nwhose densities are well approximated by piecewise polynomial functions. \nLet $f$ be the density function of an arbitrary univariate distribution, \nand suppose that $f$ is $\\mathrm{OPT}$ close in $L_1$ distance to an unknown piecewise polynomial function\nwith $t$ interval pieces and degree $d$. Our algorithm \ndraws $m = O(t(d+1)/\\epsilon^2)$ samples from $f$,  runs in time  $\\widetilde{O} (m \\cdot \\mathrm{poly} (d))$ and with probability at least \n$9/10$ outputs an $O(t)$-piecewise degree-$m$ hypothesis $h$ that is \n$4 \\mathrm{OPT} +\\epsilon$ close to $f$.\n\n<p>\n\nOur general algorithm yields (near-)sample-optimal and <i>near-linear time</i> estimators for  a wide range of structured distribution families\nover both continuous and discrete domains in a unified way. For most of our applications, these are the <i>first</i> sample-optimal and near-linear time\nestimators in the literature. As a consequence, our work resolves the sample and computational complexities of a broad class of inference\ntasks via a single \"meta-algorithm\". Moreover, we experimentally demonstrate that our algorithm performs very well in practice.\n\n</p></div>\n\n<br>\n\n\n\n<li class=\"paper robust_statistics\" id=\"paper45\">\nRobust Estimators in High Dimensions without the Computational Intractability\n<a onclick=\"reveal_abstract(45)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://arxiv.org/abs/1604.06443\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, G. Kamath, D. Kane, J. Li\u0003, A. Moitra, A. Stewart\n<br> Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2016)\n<br>  <strong> SIAM Journal on Computing Special Issue for FOCS 2016</strong>\n<br>  <strong> Invited to Communications of the ACM, Research Highlights</strong>\n<br> See <a href=\"http://www.iliasdiakonikolas.org/simons-tutorial-robust.html\">here</a> \nfor my Simons Institute tutorial on the topic and <a href=\"http://www.youtube.com/watch?v=EGaIgl0HxXc&amp;t=158s\">here</a> for a previous related talk.\n</li>\n\n\n\n<div id=\"abstract45\" class=\"abstract\" style=\"display:none\">\n\nWe study high-dimensional distribution learning in an agnostic setting \nwhere an adversary is allowed to arbitrarily corrupt an $\\epsilon$ fraction of the samples. \nSuch questions have a rich history spanning statistics, machine learning and theoretical computer science. \nEven in the most basic settings, \nthe only known approaches are either computationally inefficient \nor lose dimension dependent factors in their error guarantees. \nThis raises the following question:\nIs high-dimensional agnostic distribution learning even possible, algorithmically? \n\n<p>\n\nIn this work, we obtain the first computationally efficient algorithms with dimension-independent error guarantees \nfor agnostically learning several fundamental classes of high-dimensional distributions: \n(1) a single Gaussian, (2) a product distribution on the hypercube, \n(3) mixtures of two product distributions (under a natural balancedness condition),\nand (4) mixtures of spherical Gaussians. \nOur algorithms achieve error that is independent of the dimension, \nand in many cases scales nearly-linearly \nwith the fraction of adversarially corrupted samples. \nMoreover, we develop a general recipe for detecting and correcting corruptions in high dimensions, \nthat may be applicable to many other problems.\n\n</p></div>\n\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper44\">\nA New Approach for Testing Properties of Discrete Distributions\n<a onclick=\"reveal_abstract(44)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/new-testing.pdf\" class=\"abstract_link\">[pdf]</a> \n  <a href=\"http://arxiv.org/abs/1601.05557\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane\n<br> Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2016)\n <br> Also see the comments in <em><font color=\"gray\">Oded Goldreich's Choices \n      <a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/188.html\">#188</a></font></em> and \n      <em><a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/195.html\">#195</a></em>, \n      and Oded's very nice exposition of our framework <a href=\"http://www.wisdom.weizmann.ac.il/~oded/pt-ln.html\">here</a>\n</li>\n\n\n<div id=\"abstract44\" class=\"abstract\" style=\"display:none\">\nWe study problems in distribution property testing:\nGiven sample access to one or more unknown discrete distributions,\nwe want to determine whether they have some global property or are $\\epsilon$-far\nfrom having the property in $\\ell_1$ distance.\nIn this paper, we provide a simple and general approach to obtain upper bounds in this setting,\nby reducing $\\ell_1$-testing to $\\ell_2$-testing.\nOur reduction yields optimal $\\ell_1$-testers, by using a standard $\\ell_2$-tester as a black-box.\n\n<p>\n\nUsing our framework, we obtain sample--optimal and computationally efficient estimators for\na wide variety of $\\ell_1$ distribution testing problems, including the following: identity testing to a fixed distribution,\ncloseness testing between two unknown distributions (with equal/unequal sample sizes),\nindependence testing (in any number of dimensions), closeness testing for collections of distributions, and testing\n$k$-histograms. For most of these problems, we give the first optimal testers in the literature.\nMoreover, our estimators are significantly simpler to state and analyze compared to previous approaches.\n\n</p><p>\n\nAs our second main contribution, we provide a direct general approach for proving distribution testing lower bounds,\nby bounding the mutual information. Our lower bound approach is not restricted to symmetric properties,\nand we use it to prove tight lower bounds for the aforementioned problems.\n\n\n</p></div>\n\n\n\n\n\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper43\">\nFast Algorithms for Segmented Regression\n<a onclick=\"reveal_abstract(43)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/piecewise-regression.pdf\" class=\"abstract_link\">[pdf]</a>\n<a href=\"https://github.com/ludwigschmidt/fast-segmented-regression\" class=\"abstract_link\">[code]</a>\n<br> J. Acharya, I. Diakonikolas, J. Li, L. Schmidt\n<br> Proceedings of the  33rd International Conference on Machine Learning (ICML 2016)\n</li>\n\n<div id=\"abstract43\" class=\"abstract\" style=\"display:none\">\n\nWe study the  fixed design segmented regression problem: \nGiven noisy samples from a piecewise linear function $f$, \nwe want to recover $f$ up to a desired accuracy in mean-squared error.\n\n<p>\n\nPrevious rigorous approaches for this problem rely on dynamic programming (DP)\nand, while sample efficient, have running time quadratic in the sample size. \nAs our main contribution, we provide new \nsample near-linear time algorithms for the problem that -- \nwhile not being minimax optimal -- \nachieve a significantly better sample-time tradeoff \non large datasets compared to the DP approach.\nOur experimental evaluation shows that, compared with the DP approach, \nour algorithms provide a convergence rate that is only off by a factor of $2$ to $3$, \nwhile achieving speedups of two orders of magnitude.\n</p></div>\n\n\n\n<br>\n\n<li class=\"paper nonparametric_statistics\" id=\"paper42\">\nProperly Learning Poisson Binomial Distributions in Almost Polynomial Time\n<a onclick=\"reveal_abstract(42)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/pbd-proper.pdf\" class=\"abstract_link\">[pdf]</a> \n  <a href=\"http://arxiv.org/abs/1511.04066\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 29th Annual Conference on Learning Theory (COLT 2016) </li>\n<div id=\"abstract42\" class=\"abstract\" style=\"display:none\">\nWe give an algorithm for properly learning Poisson binomial distributions.\nA Poisson binomial distribution (PBD) of order $n$\nis the discrete probability distribution of the sum of $n$ mutually independent Bernoulli random variables.\nGiven $\\widetilde{O}(1/\\epsilon^2)$ samples from an unknown PBD $\\mathbf{P}$, our algorithm runs in time\n$(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$, and outputs a hypothesis PBD that is $\\epsilon$-close to $\\mathbf{P}$ in total variation distance.\nThe sample complexity of our algorithm is known to be nearly-optimal, up to logarithmic factors, as established\nin previous work~\\cite{DDS12stoc}. However, the previously best known running time for properly\nlearning PBDs~\\cite{DDS12stoc, DKS15} was $(1/\\epsilon)^{O(\\log(1/\\epsilon))}$, and was essentially obtained by\nenumeration over an appropriate  $\\epsilon$-cover. We remark that the running time of this cover-based approach cannot be\nimproved, as any $\\epsilon$-cover for the space of PBDs has size  $(1/\\epsilon)^{\\Omega(\\log(1/\\epsilon))}$~\\cite{DKS15}.\n\n<p>\n\n\nAs one of our main contributions, we provide a novel structural characterization of PBDs,\nshowing that any PBD $\\mathbf{P}$ is $\\epsilon$-close to another PBD $\\mathbf{Q}$ with $O(\\log(1/\\epsilon))$ distinct parameters.\nMore precisely, we prove that, for all $\\epsilon &gt;0,$ there exists\nan explicit collection $\\cal{M}$ of $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$ vectors of multiplicities,\nsuch that for any PBD $\\mathbf{P}$ there exists a PBD $\\mathbf{Q}$ with $O(\\log(1/\\epsilon))$ \ndistinct parameters whose multiplicities are given by some element of ${\\cal M}$,\nsuch that $\\mathbf{Q}$ is $\\epsilon$-close to $\\mathbf{P}.$  Our proof combines tools from Fourier analysis and algebraic geometry.\n\n</p><p>\n\nOur approach to the proper learning problem is as follows:\nStarting with an accurate non-proper hypothesis, we fit a PBD to this hypothesis.\nMore specifically, we essentially start with the hypothesis computed by the\ncomputationally efficient non-proper learning algorithm in our recent work~\\cite{DKS15}.\nOur aforementioned structural characterization allows\nus to reduce the corresponding fitting problem\nto a collection of $(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$\nsystems of low-degree polynomial inequalities.\nWe show that each such system can be solved in time $(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$,\nwhich yields the overall running time of our algorithm.\n\n</p></div>\n\n\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper41\">\nOptimal Learning via the Fourier Transform for Sums of Independent Integer Random Variables\n<a onclick=\"reveal_abstract(41)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/siirv-optimal-v2.pdf\" class=\"abstract_link\">[pdf]</a>\n  <a href=\"http://arxiv.org/abs/1505.00662\" class=\"abstract_link\">[arxiv]</a> \n  <a href=\"http://www.iliasdiakonikolas.org/notes/siirvs.txt\" class=\"abstract_link\">[notes]</a> \n\n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 29th Annual Conference on Learning Theory (COLT 2016)\n<div id=\"abstract41\" class=\"abstract\" style=\"display:none\">\n\nWe study the structure and learnability of sums of independent integer random variables (SIIRVs).\nFor $k \\in \\mathbb{Z}_{+}$, a {\\em $k$-SIIRV of order $n \\in \\mathbb{Z}_{+}$} is the probability distribution of the sum of $n$\nmutually independent random variables each supported on $\\{0, 1, \\dots, k-1\\}$.\nWe denote by ${\\cal S}_{n,k}$ the set of all $k$-SIIRVs of order $n$.\n\n<p>\n\nHow many samples are required to learn an arbitrary distribution in ${\\cal S}_{n,k}$?\nIn this paper, we tightly characterize the sample and computational complexity of this problem.\nMore precisely, we design a computationally efficient algorithm that uses $\\widetilde{O}(k/\\epsilon^2)$ samples, \nand learns an arbitrary $k$-SIIRV within error $\\epsilon,$ in total variation distance. Moreover, we show that\nthe {\\em optimal} sample complexity of this learning problem is \n$\\Theta((k/\\epsilon^2)\\sqrt{\\log(1/\\epsilon)}),$ i.e., we prove an upper bound and a matching \ninformation-theoretic lower bound.\nOur algorithm proceeds by learning the Fourier transform of the target $k$-SIIRV in its effective support. \nIts correctness relies on the {\\em approximate sparsity} of the Fourier transform of $k$-SIIRVs -- \na structural property that we establish, roughly stating that the Fourier transform of $k$-SIIRVs\nhas small magnitude outside a small set.\n\n</p><p>\n\nAlong the way we prove several new structural results about $k$-SIIRVs.\nAs one of our main structural contributions, we give an efficient algorithm to construct a \nsparse {\\em proper} $\\epsilon$-cover for ${\\cal S}_{n,k},$ in total variation distance.\nWe also obtain a novel geometric characterization of the space of $k$-SIIRVs. Our\ncharacterization allows us to prove a tight lower bound on the size of $\\epsilon$-covers for ${\\cal S}_{n,k}$\n-- establishing that our cover upper bound is optimal -- and is the key ingredient in our tight sample complexity lower bound.\n\n</p><p>\n\nOur approach of exploiting the sparsity of the Fourier transform in \ndistribution learning is general, and has recently found additional applications. \nIn a subsequent work~\\cite{DKS15c}, we use a generalization of this idea (in higher dimensions)\nto obtain the first efficient learning algorithm for Poisson multinomial distributions.\nIn~\\cite{DKS15b}, we build on this approach to obtain the fastest known proper learning algorithm \nfor Poisson binomial distributions ($2$-SIIRVs).\n\n</p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper40\">\nThe Fourier Transform of Poisson Multinomial Distributions and its Algorithmic Applications\n<a onclick=\"reveal_abstract(40)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/fourier-pmd.pdf\" class=\"abstract_link\">[pdf]</a> \n  <a href=\"http://arxiv.org/abs/1511.03592\" class=\"abstract_link\">[arxiv]</a> \n<br> I. Diakonikolas, D. Kane, A. Stewart\n<br> Proceedings of the 48th Annual ACM Symposium on Theory of Computing (STOC 2016)\n<div id=\"abstract40\" class=\"abstract\" style=\"display:none\">\nWe study Poisson Multinomial Distributions -- a fundamental family of discrete distributions \nthat generalize the binomial and multinomial distributions, and are commonly encountered\nin computer science. \nFormally, an $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable\nof the form $X = \\sum_{i=1}^n X_i$, where the $X_i$'s are independent random vectors supported \non the set  $\\{e_1, e_2, \\ldots, e_k \\}$ of standard basis vectors in $\\mathbb{R}^k$.\nIn this paper, we obtain a refined structural understanding of PMDs \nby analyzing their Fourier transform. \nAs our core structural result, we prove that the Fourier transform of PMDs is \\emph{approximately sparse}, \ni.e., roughly speaking, its $L_1$-norm is small outside a small set. By building on this result, we obtain the following \napplications:\n\n<p>\n\n<strong>Learning Theory.</strong>\nWe design the first computationally efficient learning algorithm for PMDs\nwith respect to the total variation distance. Our algorithm learns an arbitrary $(n, k)$-PMD \nwithin variation distance $\\epsilon$ using a near-optimal sample size of $\\widetilde{O}_k(1/\\epsilon^2),$ \nand runs in time $\\widetilde{O}_k(1/\\epsilon^2) \\cdot \\log n.$ Previously, no algorithm with a $\\mathrm{poly}(1/\\epsilon)$\nruntime was known, even for $k=3.$\n\n</p><p>\n\n<strong>Game Theory.</strong> We give the first efficient polynomial-time approximation scheme (EPTAS) for computing Nash equilibria\nin anonymous games. For normalized anonymous games\nwith $n$ players and $k$ strategies, our algorithm computes a well-supported $\\epsilon$-Nash equilibrium in time \n$n^{O(k^3)} \\cdot (k/\\epsilon)^{O(k^3\\log(k/\\epsilon)/\\log\\log(k/\\epsilon))^{k-1}}.$ \nThe best previous algorithm for this problem~\\cite{DaskalakisP08, DaskalakisP2014} \nhad running time $n^{(f(k)/\\epsilon)^k},$ where $f(k) = \\Omega(k^{k^2})$, for any $k&gt;2.$\n\n</p><p>\n\n<strong>Statistics.</strong> We prove a multivariate central limit theorem (CLT) that relates \nan arbitrary PMD to a discretized multivariate Gaussian with the same mean and covariance, in total variation distance. \nOur new CLT strengthens the CLT of Valiant and Valiant~\\cite{VV10b, ValiantValiant:11} by completely removing the dependence on $n$ in the error bound.\n\n</p><p>\n\nAlong the way we prove several new structural results of independent interest about PMDs. These include: (i) a robust moment-matching lemma, \nroughly stating that two PMDs that approximately agree on their low-degree parameter moments are close in variation distance; \n(ii) near-optimal size proper $\\epsilon$-covers for PMDs in total variation distance (constructive upper bound and nearly-matching lower bound).\nIn addition to Fourier analysis, we employ a number of analytic tools, including the saddlepoint method from complex analysis,\nthat may find other applications.\n</p></div>\n</li>\n\n\n\n<br>\n\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper39\">\nTesting Shape Restrictions of Discrete Distributions\n<a onclick=\"reveal_abstract(39)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/testing-shape.pdf\" class=\"abstract_link\">[pdf]</a>\n<br> C. Canonne, I. Diakonikolas, T. Gouleakis, R. Rubinfeld\n<br> Proceedings of the 33rd International Symposium on Theoretical Aspects of Computer Science (STACS 2016)\n<br>  <strong> Invited to Special Issue for STACS 2016</strong>\n<div id=\"abstract39\" class=\"abstract\" style=\"display:none\">\nWe study the question of testing <i>structured</i> properties (classes) of discrete distributions. Specifically, given \nsample access to an arbitrary distribution $D$ over $[n]$ and a property $\\mathcal{P}$, the goal is to distinguish \nbetween $D\\in \\mathcal{P}$ and $D$ is $\\epsilon$-far in $\\ell_1$ distance from $\\mathcal{P}$.\n\n<p>\n\nWe develop a general algorithm for this question, which applies to a large range of  \"shape-constrained\" properties, \nincluding monotone, log-concave, $t$-modal, piecewise-polynomial, and Poisson Binomial distributions. Moreover, for all cases \nconsidered, our algorithm has near-optimal sample complexity with regard to the domain size and is computationally efficient.  \nFor most of these classes, we provide the first non-trivial tester in the literature. \nIn addition, we also describe a generic method to prove lower bounds for this problem, and use it to show our upper bounds are nearly tight. \nFinally, we extend some of our techniques to tolerant testing, deriving nearly-tight upper and lower bounds for the corresponding questions.\n</p></div>  \n\n\n</li>\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper38\">\nDifferentially Private Learning of Structured Discrete Distributions\n<a onclick=\"reveal_abstract(38)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/dp-learn-nips15.pdf\" class=\"abstract_link\">[pdf]</a>\n<a href=\"https://github.com/ludwigschmidt/private_density\" class=\"abstract_link\">[code]</a>\n<br> I. Diakonikolas, M. Hardt, L. Schmidt\n<br> Advances in Neural Information Processing Systems (NIPS 2015) </li>\n\n<div id=\"abstract38\" class=\"abstract\" style=\"display:none\">\nWe investigate the problem of learning an unknown probability distribution\nover a discrete population from random samples. Our goal is to design \nefficient algorithms that simultaneously achieve low error in total variation\nnorm while guaranteeing Differential Privacy to the individuals of the\npopulation.  \n\n<p>\n\nWe describe a general approach that yields near sample-optimal and computationally efficient differentially \nprivate estimators for a wide range of well-studied and natural distribution families. Our theoretical results\nshow that for a wide variety of structured distributions there exist private estimation algorithms that are nearly\nas efficient---both in terms of sample size and running time---as their non-private counterparts. We complement our theoretical\nguarantees with an experimental evaluation. Our experiments illustrate the speed and accuracy \nof our private estimators on both synthetic mixture models, as well as a large public data set.\n</p></div>\n\n\n\n<br>\n\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper37\">\nOptimal Algorithms and Lower Bounds for Testing Closeness of Structured Distributions\n<a onclick=\"reveal_abstract(37)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/closeness-structured.pdf\" class=\"abstract_link\">[pdf]</a>\n<br> I. Diakonikolas, D. Kane, V. Nikishkin\n<br> Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015)\n</li>\n<div id=\"abstract37\" class=\"abstract\" style=\"display:none\">\nWe give a general unified method that can be used for <i>$L_1$ closeness testing</i> of a wide range of univariate structured distribution families.\nMore specifically, we design a sample optimal and computationally efficient algorithm for testing \nthe identity of two unknown (potentially arbitrary) univariate distributions under the $\\mathcal{A}_k$-distance metric:\nGiven sample access to distributions with density functions $p, q: I \\to \\mathbb{R}$, we want to distinguish\nbetween the cases that $p=q$ and $\\|p-q\\|_{\\mathcal{A}_k} \\ge \\epsilon$ with probability at least $2/3$.\nWe show that for any $k \\ge 2, \\epsilon&gt;0$, the <i>optimal</i> sample complexity of the  $\\mathcal{A}_k$-closeness testing\nproblem is $\\Theta(\\max\\{ k^{4/5}/\\epsilon^{6/5}, k^{1/2}/\\epsilon^2 \\})$.\nThis is the first $o(k)$ sample algorithm for this problem, and yields \nnew, simple $L_1$ closeness testers, in most cases with optimal sample complexity, \nfor broad classes of structured distributions.\n</div>\n\n\n\n<br>\n\n\n\n<li class=\"paper game_theory\" id=\"paper36\">\nOn the Complexity of Optimal Lottery Pricing and Randomized Mechanisms\n<a onclick=\"reveal_abstract(36)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/lottery-pricing.pdf\" class=\"abstract_link\">[pdf]</a>\n<br> X. Chen, I. Diakonikolas, A. Orfanou, D. Paparas, X. Sun, M. Yannakakis\n<br> Proceedings of the 56th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2015)\n<div id=\"abstract36\" class=\"abstract\" style=\"display:none\">\nWe study the optimal lottery problem and the optimal mechanism design problem\nin the setting of a single unit-demand buyer with item values drawn from independent distributions.\nOptimal solutions to both problems are characterized by a linear program with exponentially many variables.\n\n<p>\n\nFor the menu size complexity of the optimal lottery problem, we present an explicit, simple instance\nwith distributions of support size $2$, and show that exponentially many lotteries \nare required to achieve the optimal revenue. We also show that, when distributions have support size $2$ \nand share the same high value, the simpler scheme of item pricing can achieve the same revenue as the optimal\nmenu of lotteries. The same holds for the case of two items with support size $2$ \n(but not necessarily the same high value).\n\n</p><p>\n  \nFor the computational complexity of the optimal mechanism design problem,\nwe show that unless the polynomial-time hierarchy collapses \n(more precisely, $\\mathrm{P}^{\\mathrm{NP}}=\\mathrm{P}^{\\mathrm{\\#P}}$), there is \nno universal efficient randomized algorithm to implement \nan optimal mechanism even when distributions have support size $3$.\n</p></div>  \n\n\n</li>\n\n<br>\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper35\">\nFast and Near-Optimal Algorithms for Approximating Distributions by Histograms\n<a onclick=\"reveal_abstract(35)\" class=\"abstract_link\">[abstract]</a>\n<a href=\"http://www.iliasdiakonikolas.org/papers/pods-hist.pdf\" class=\"abstract_link\">[pdf]</a>\n<br> J. Acharya, I. Diakonikolas, C. Hegde, J. Li\u0003, L. Schmidt\u0003\n<br> Proceedings of the 34th Annual ACM Symposium on Principles of Database Systems (PODS 2015)\n<div id=\"abstract35\" class=\"abstract\" style=\"display:none\">\nHistograms are among the most popular structures for the succinct summarization of data in a variety of database applications.\nIn this work, we provide fast and near-optimal algorithms for approximating arbitrary \none dimensional data distributions by histograms. \n\n<p>\n\nA $k$-histogram is a piecewise constant function with $k$ pieces.\nWe consider the following natural problem, previously studied by Indyk, Levi, and Rubinfeld in PODS 2012:\nGiven samples from a distribution $p$ over $\\{1, \\ldots, n \\}$, compute a $k$-histogram that minimizes \nthe $\\ell_2$-distance from $p$, up to an additive $\\epsilon$. \nWe design an algorithm for this problem that uses the information--theoretically minimal sample \nsize of $m = O(1/\\epsilon^2)$, runs in sample--linear time $O(m)$, \nand outputs an $O(k)$-- histogram whose $\\ell_2$-distance from $p$ is at most $O(\\mathrm{opt}_k) +\\epsilon$, \nwhere $\\mathrm{opt}_k$ is the minimum $\\ell_2$-distance between $p$ and any $k$-histogram.\nPerhaps surprisingly, the sample size and running time of our algorithm are independent of the universe size $n$. \n</p><p>\n\nWe generalize our approach to obtain fast algorithms for multi-scale histogram construction, \nas well as approximation by piecewise polynomial distributions. \nWe experimentally demonstrate one to two orders of magnitude improvement in terms of empirical \nrunning times over previous state-of-the-art algorithms.\n</p></div>\n  \n\n</li>\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper34\">\n    Testing Identity of Structured Distributions\n  <a onclick=\"reveal_abstract(34)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/identity-testing-structured.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> I. Diakonikolas, D. Kane, V. Nikishkin\n  <br> Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2015)\n  <div id=\"abstract34\" class=\"abstract\" style=\"display:none\">\n  We study the question of identity testing for structured distributions.\nMore precisely, given samples from a {\\em structured} distribution $q$ over $[n]$ and an explicit distribution $p$ over $[n]$,\nwe wish to distinguish whether $q=p$ versus $q$ is at least $\\epsilon$-far from $p$,\nin $L_1$  distance. In this work, we present a unified approach that yields new, simple testers, with sample complexity\nthat is information-theoretically optimal, for broad classes of structured distributions, including $t$-flat distributions,\n$t$-modal distributions, log-concave distributions, monotone hazard rate (MHR) distributions, and mixtures thereof.\n  </div>\n</li>\n\n\n<br>\n\n\n\n\n<li class=\"paper robust_PAC\" id=\"paper29\">\n  Learning from Satisfying Assignments\n  <a onclick=\"reveal_abstract(29)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/inverse-learning.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, R. Servedio\n  <br> Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2015)\n  <div id=\"abstract29\" class=\"abstract\" style=\"display:none\">\nThis paper studies the problem of learning ``low-complexity\"\nprobability distributions over the Boolean hypercube $\\{-1,1\\}^n$.\nAs in the standard PAC learning model, a learning problem in our\nframework is defined by a class ${\\cal C}$ of Boolean functions\nover $\\{-1,1\\}^n$, but\nin our model the learning algorithm is given uniform random satisfying\nassignments of an unknown $f \\in \\cal{C}$ and its goal is to output\na high-accuracy approximation of the uniform distribution\nover $f^{-1}(1).$ This distribution learning problem may be viewed as a\ndemanding variant of standard Boolean function learning, where the learning\nalgorithm only receives positive examples and -- more importantly --\nmust output a hypothesis function which has small \\emph{multiplicative}\nerror (i.e., small error relative to the size of $f^{-1}(1)$).\n\n<p>\n\n\nAs our main results, we show that the two most widely studied\nclasses of Boolean functions in computational learning theory --\nlinear threshold functions and DNF formulas -- have\nefficient distribution learning algorithms in our model.\nOur algorithm for linear threshold functions runs in\ntime poly$(n,1/\\epsilon)$ and our algorithm for\npolynomial-size DNF runs in time quasipoly$(n,1/\\epsilon)$.\nWe obtain both these results via a general approach that\ncombines a broad range of technical ingredients, including the complexity-theoretic study \nof approximate counting and uniform generation;  \nthe Statistical Query model from learning theory; and hypothesis testing techniques from statistics. \nA key conceptual and technical ingredient of\nthis approach is a new kind of algorithm which we devise called a ``densifier'' and which we\nbelieve may be useful in other contexts.\n\n</p><p>\n\n\nWe also establish limitations on efficient learnability in our model by showing\nthat the existence of certain types of cryptographic signature schemes\nimply that certain learning problems in our framework are computationally\nhard.  Via this connection we show that\nassuming the existence of sufficiently strong unique signature schemes,\n there are no sub-exponential time learning algorithms in our framework for\nintersections of two halfspaces, for\ndegree-2 polynomial threshold functions, or for monotone 2-CNF formulas.\nThus our positive results for distribution learning come close to the\nlimits of what can be achieved by efficient algorithms.\n\n\n  </p></div>\n</li>\n\n\n<br>\n\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper33\">\n  Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width Histograms\n<a onclick=\"reveal_abstract(33)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/histograms-nips.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> S. Chan, I. Diakonikolas, R. Servedio, X. Sun\n  <br> Advances in Neural Information Processing Systems (NIPS 2014)\n <div id=\"abstract33\" class=\"abstract\" style=\"display:none\">\nLet $p$ be an unknown and arbitrary probability distribution over $[0,1)$.  We consider the problem\nof density estimation, in which a learning algorithm is given i.i.d. draws from $p$ and must\n(with high probability) output a hypothesis distribution that is close to $p$.  The main contribution of this paper is\na highly efficient density estimation algorithm for learning using a variable-width histogram, i.e., a\nhypothesis distribution with a piecewise constant probability density function.\n\n<p>\n\nIn more detail,  for any $k$ and $\\epsilon$, we give an algorithm that makes $\\tilde{O}(k/\\epsilon^2)$ draws from $p$, runs in $\\tilde{O}(k/\\epsilon^2)$ time, and outputs a\nhypothesis distribution $h$ that is piecewise constant with $O(k \\log^2(1/\\epsilon))$ pieces.  With high probability the \nhypothesis $h$ satisfies $d_{\\mathrm{TV}}(p,h) \\leq C \\cdot \\mathrm{opt}_k(p) + \\epsilon$, where $d_{\\mathrm{TV}}$ denotes the total variation distance\n(statistical distance), $C$ is a universal constant,\nand $\\mathrm{opt}_k(p)$ is the smallest total variation distance between $p$ and any $k$-piecewise constant distribution.\nThe sample size and running time of our algorithm are optimal up to logarithmic factors.\nThe ``approximation factor'' $C$  in our result is inherent in the problem, as we prove that\nno algorithm with sample size bounded in terms of $k$ and $\\epsilon$ can achieve $C&lt;2$ regardless \nof what kind of hypothesis distribution it uses.\n  </p></div>\n</li>\n\n<br>\n\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper30\">\n  Efficient Density Estimation via Piecewise Polynomial Approximation\n  <a onclick=\"reveal_abstract(30)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/piecewise-poly-learning.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> S. Chan, I. Diakonikolas, R. Servedio, X. Sun\n  <br> Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC 2014)\n  <div id=\"abstract30\" class=\"abstract\" style=\"display:none\">\n   We give a highly efficient \"semi-agnostic\" algorithm for learning univariate probability distributions that are well approximated by piecewise polynomial density functions. Let $p$ be an arbitrary distribution over an interval $I$ which is $\\tau$-close (in total variation distance) to an unknown probability distribution $q$ that is defined by an unknown partition of $I$ into $t$ intervals and $t$ unknown degree-$d$ polynomials specifying $q$ over each of the intervals. We give an algorithm that draws $\\tilde{O}(t(d+1)/\\epsilon^2)$ samples from $p$, runs in time $\\mathrm{poly}(t,d,1/\\epsilon)$, and with high probability outputs a piecewise polynomial hypothesis distribution $h$ that is $(O(\\tau)+\\epsilon)$-close (in total variation distance) to $p$. This sample complexity is essentially optimal; we show that even for $\\tau=0$, any algorithm that learns an unknown $t$-piecewise degree-$d$ probability distribution over $I$ to accuracy $\\epsilon$ must use $\\Omega({\\frac {t(d+1)} {\\mathrm{poly}(1 + \\log(d+1))}} \\cdot {\\frac 1 {\\epsilon^2}})$ samples from the distribution, regardless of its running time. Our algorithm combines tools from approximation theory, uniform convergence, linear programming, and dynamic programming. \n\n<br> <br>\nWe apply this general algorithm to obtain a wide range of results for many natural problems in density estimation over both continuous and discrete domains. These include state-of-the-art results for learning mixtures of log-concave distributions; mixtures of $t$-modal distributions; mixtures of Monotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions; mixtures of Gaussians; and mixtures of $k$-monotone densities. Our general technique yields computationally efficient algorithms for all these problems, in many cases with provably optimal sample complexities (up to logarithmic factors) in all parameters.\n  </div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper applied_probability\" id=\"paper32\">\n  Deterministic Approximate Counting for Juntas of Degree-$2$ Polynomial Threshold Functions\n  <a onclick=\"reveal_abstract(32)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/deg2juntas-count.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, R. Servedio\n  <br> Proceedings of the 29th Annual IEEE Conference on Computational Complexity (CCC 2014) \n  <div id=\"abstract32\" class=\"abstract\" style=\"display:none\">\nLet $g: \\{-1,1\\}^k \\rightarrow \\{-1,1\\}$ be any Boolean function\nand $q_1,\\dots,q_k$ be any degree-$2$ polynomials over\n$\\{-1,1\\}^n.$ We give a <i>deterministic</i> algorithm which,\ngiven as input explicit descriptions of\n$g,q_1,\\dots,q_k$ and an accuracy parameter $\\epsilon&gt;0$,\napproximates\n\\[\n\\mathbf{Pr}_{x \\sim \\{-1,1\\}^n}[g(\\mathrm{sign}(q_1(x)),\\dots,\\mathrm{sign}(q_k(x)))=1]\n\\]\nto within an additive $\\pm \\epsilon$.  For any constant $\\epsilon &gt; 0$\nand $k \\geq 1$ the running time of our algorithm is a fixed\npolynomial in $n$ (in fact this is true even for some not-too-small\n$\\epsilon = o_n(1)$ and not-too-large $k = \\omega_n(1)$).\nThis is the first fixed polynomial-time algorithm\nthat can deterministically approximately count\nsatisfying assignments of a natural\nclass of depth-$3$ Boolean circuits.\n\n<p>\n\nOur algorithm extends a recent result \\cite{DDS13:deg2count}\nwhich gave a deterministic\napproximate counting algorithm for a single degree-$2$ polynomial\nthreshold function $\\mathrm{sign}(q(x)),$ corresponding to the $k=1$ case of our\nresult.  Note that even in the $k=1$ case it is NP-hard to determine\nwhether $\\mathbf{Pr}_{x \\sim \\{-1,1\\}^n}[\\mathrm{sign}(q(x))=1]$ is nonzero,\nso any sort of multiplicative approximation is almost certainly\nimpossible even for efficient randomized algorithms.\n\n</p><p>\n\nOur algorithm and analysis requires several novel technical ingredients\nthat go significantly beyond the tools required to handle the $k=1$ case\nin \\cite{DDS13:deg2count}.  One of these\nis a new multidimensional central limit theorem \nfor degree-$2$ polynomials in Gaussian random variables which builds\non recent Malliavin-calculus-based results from probability theory.  We\nuse this CLT as the basis of a new decomposition technique for $k$-tuples\nof degree-$2$ Gaussian polynomials and thus obtain an efficient\ndeterministic approximate counting \nalgorithm for the Gaussian distribution, i.e., an algorithm for estimating\n\\[\n\\mathbf{Pr}_{x \\sim \\mathcal{N}(0,1)^n}[g(\\mathrm{sign}(q_1(x)),\\dots,\\mathrm{sign}(q_k(x)))=1].\n\\]\nFinally, a third new ingredient\nis a ``regularity lemma'' for <i>$k$-tuples</i> of degree-$d$ \npolynomial threshold functions.  This generalizes both the regularity lemmas\nof \\cite{DSTW:10,HKM:09} \n(which apply to a single degree-$d$ polynomial threshold\nfunction) and the regularity lemma of Gopalan et al \\cite{GOWZ10} \n(which applies to\na $k$-tuples of <i>linear</i> threshold functions, i.e., the case $d=1$).\nOur new regularity lemma lets us extend our deterministic approximate\ncounting results from the Gaussian to the Boolean domain.\n\n  </p></div>\n</li>\n\n\n\n<br>\n\n\n<li class=\"paper game_theory\" id=\"paper25\">\n  The Complexity of Optimal Multidimensional Pricing\n  <a onclick=\"reveal_abstract(25)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/item-pricing.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> X. Chen, I. Diakonikolas, D. Paparas, X. Sun, M. Yannakakis\n  <br> Games and Economic Behavior, accepted with minor revisions\n  <br> Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2014)\n  <div id=\"abstract25\" class=\"abstract\" style=\"display:none\">\nWe resolve the complexity of revenue-optimal deterministic auctions in \n  the unit-demand single-buyer Bayesian setting, i.e., the optimal item \n  pricing problem, when the buyer's values for the items are independent. \nWe show that the problem of computing a revenue-optimal pricing can be solved\n  in polynomial time for distributions of support size $2$ \n  and its decision version is NP-complete for distributions of support size $3$. \nWe also show that the problem remains NP-complete for the case of identical distributions.\n  </div>\n</li>\n\n\n<br>\n\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper24\">\n  Optimal Algorithms for Testing Closeness of Discrete Distributions\n  <a onclick=\"reveal_abstract(24)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/closeness-soda.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> S. Chan, I. Diakonikolas, G. Valiant, P. Valiant\n  <br> Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2014)\n  <br> <em><font color=\"gray\">Blog post about this work: <a href=\"http://ptreview.sublinear.info/?p=78\">Property Testing Review</a></font> </em> \n  <div id=\"abstract24\" class=\"abstract\" style=\"display:none\">\nWe study the question of closeness testing for two discrete distributions.\nMore precisely, given samples from two distributions $p$ and $q$ over an $n$-element set,\nwe wish to distinguish whether $p=q$ versus $p$ is at least $\\epsilon$-far from $q$,\nin either $\\ell_1$ or $\\ell_2$ distance.  Batu et al~\\cite{BFR+:00, Batu13} gave the first sub-linear time algorithms for these problems, \nwhich matched the lower bounds of~\\cite{PV11sicomp} up to a logarithmic factor in $n$, and a polynomial factor of $\\epsilon.$\n\n<br> <br>\n\nIn this work, we present simple testers for both the $\\ell_1$ and $\\ell_2$ settings, with sample complexity\nthat is information-theoretically optimal, to constant factors, both in the dependence on $n$, and the dependence on $\\epsilon$; \nfor the $\\ell_1$ testing problem we establish that the sample complexity is $\\Theta(\\max\\{n^{2/3}/\\epsilon^{4/3}, n^{1/2}/\\epsilon^2 \\}).$\n  </div>\n</li>\n\n\n\n<br>\n\n\n\n<li class=\"paper applied_probability\" id=\"paper23\">\n  A Polynomial-time Approximation Scheme for Fault-tolerant Distributed Storage\n  <a onclick=\"reveal_abstract(23)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/distr-soda.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> C. Daskalakis, A. De, I. Diakonikolas, A. Moitra, R. Servedio\n  <br> Proceedings of the 25th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2014)\n  <br> <em><font color=\"gray\">Featured in <a href=\"http://www.abstract-talk.org/wp/\">Abstract Talk</a>. Podcast is <a href=\"http://www.abstract-talk.org/wp/?p=794\">here</a></font> </em> \n  <div id=\"abstract23\" class=\"abstract\" style=\"display:none\">\nWe consider a problem which has received considerable attention in\nsystems literature because of its applications to routing in delay tolerant networks and replica placement\nin distributed storage systems.\nIn abstract terms the problem can be stated as follows: Given a random variable $X$\ngenerated by a known product distribution over $\\{0,1\\}^n$ and a target\nvalue $0 \\leq \\theta \\leq 1$, output a non-negative vector $w$, with\n$\\|w\\|_1 \\le 1$, which maximizes the probability of the event $w \\cdot X\n\\ge \\theta$.  This is a challenging non-convex optimization problem for\nwhich even computing the value $\\Pr[w \\cdot X \\ge \\theta]$ of a proposed\nsolution vector $w$ is #P-hard.\n\n<p>\n\nWe provide an additive EPTAS for this problem\nwhich, for constant-bounded product distributions,\nruns in $ \\mathrm{poly}(n) \\cdot 2^{\\mathrm{poly}(1/\\epsilon)}$ time and outputs an\n$\\epsilon$-approximately optimal solution vector $w$ for this problem. Our\napproach is inspired by, and extends,\nrecent structural results from the complexity-theoretic\nstudy of linear threshold functions. Furthermore, in spite of the objective function being non-smooth, \nwe give a <i>unicriterion</i> PTAS while previous work for such objective functions has typically \nled to a <i>bicriterion</i> PTAS. We believe our techniques may be applicable to get unicriterion PTAS for other non-smooth objective functions. \n  </p></div>\n</li>\n\n<br>\n\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper22\">\n  Learning Sums of Independent Integer Random Variables\n  <a onclick=\"reveal_abstract(22)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/siirvs.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> C. Daskalakis, I. Diakonikolas, R. O'Donnell, R. Servedio, L-Y. Tan\n  <br> Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2013)\n  <br> <em><font color=\"gray\">Blog post about this work: <a href=\"http://mittheory.wordpress.com/2013/11/15/focs-2013-recaps-part-2/\">MIT theory student blog</a></font> </em> \n  \n  <div id=\"abstract22\" class=\"abstract\" style=\"display:none\">\nLet $\\mathbf{S} = \\mathbf{X}_1 + \\cdots + \\mathbf{X}_n$ be a sum of $n$ independent\ninteger random variables $\\mathbf{X}_i$, where each $\\mathbf{X}_i$ is supported on\n$\\{0,1,\\dots,k-1\\}$ but otherwise may have an arbitrary distribution\n(in particular the $\\mathbf{X}_i$'s need not be identically distributed).\nHow many samples are required to learn the distribution $\\mathbf{S}$ to high\naccuracy?  In this paper we show\nthat the answer is <i>completely independent of $n$</i>, and moreover we give a\ncomputationally efficient algorithm which achieves this low sample\ncomplexity.  More precisely, our algorithm learns any such $\\mathbf{S}$ to $\\epsilon$-accuracy (with respect\nto the total variation distance between distributions)\nusing $\\mathrm{poly}(k,1/\\epsilon)$ samples, independent of $n$.  Its running time is\n$\\mathrm{poly}(k,1/\\epsilon)$ in the standard word RAM model. Thus we give\na broad generalization of the main result of~\\cite{DDS12stoc}\nwhich gave a similar learning result for the special case $k=2$ (when\nthe distribution~$\\mathbf{S}$ is a Poisson Binomial Distribution).\n\n<p>\n\nPrior to this work, no nontrivial results were\nknown for learning these distributions even in the case $k=3$.\nA key difficulty is that, in contrast to the case of $k = 2$,\nsums of independent $\\{0,1,2\\}$-valued random variables may\nbehave very differently from\n(discretized) normal distributions, and in fact may be\nrather complicated --- they are not log-concave, they can\nbe $\\Theta(n)$-modal, there is no relationship between\nKolmogorov distance and total variation distance for the class, etc.\nNevertheless, the heart of our learning result is a new limit theorem\nwhich characterizes what the sum of an arbitrary number of arbitrary\nindependent $\\{0,1,\\dots, k-1\\}$-valued random variables may look like.\nPrevious limit theorems in this setting made strong assumptions on the\n\"shift invariance\" of the random variables $\\mathbf{X}_i$ in order to\nforce a discretized normal limit. We believe that our new\nlimit theorem, as the first result for truly arbitrary sums of\nindependent $\\{0,1,\\dots,k-1\\}$-valued random variables,\n is of independent interest.\n   </p></div>\n</li>\n\n<br>\n\n\n\n<li class=\"paper applied_probability\" id=\"paper31\">\n  Deterministic Approximate Counting for Degree-$2$ Polynomial Threshold Functions\n  <a onclick=\"reveal_abstract(31)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/deg2-count.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, R. Servedio\n  <br> Manuscript, 2013. Available as ECCC technical report <a href=\"http://eccc.hpi-web.de/report/2013/172/\" class=\"abstract_link\">[link]</a>\n  <div id=\"abstract31\" class=\"abstract\" style=\"display:none\">\nWe give a <i>deterministic</i> algorithm for\napproximately computing the fraction of Boolean assignments\nthat satisfy a degree-$2$ polynomial threshold function.\nGiven a degree-$2$ input polynomial $p(x_1,\\dots,x_n)$\nand a parameter $\\epsilon &gt; 0$, the algorithm approximates\n$\\Pr_{x \\sim \\{-1,1\\}^n}[p(x) \\geq 0]$\nto within an additive $\\pm \\epsilon$ in time $\\mathrm{poly}(n,2^{\\mathrm{poly}(1/\\epsilon)})$.\nNote that it is NP-hard to determine whether the above probability\nis nonzero, so any sort of multiplicative approximation is almost certainly\nimpossible even for efficient randomized algorithms.\nThis is the first deterministic algorithm for this counting problem\nin which the running time is polynomial in $n$ for $\\epsilon= o(1)$.\nFor \"regular\" polynomials $p$ (those in which no individual variable's\ninfluence is large compared to the sum of all $n$\nvariable influences)\nour algorithm runs in $\\mathrm{poly}(n,1/\\epsilon)$ time.\nThe algorithm also runs in $\\mathrm{poly}(n,1/\\epsilon)$ time to approximate\n$\\Pr_{x \\sim \\mathcal{N}(0,1)^n}[p(x) \\geq 0]$ to within an additive $\\pm \\epsilon$,\nfor any degree-2 polynomial $p$.\n\n<p>\nAs an application of our counting result, we give a deterministic\nmultiplicative $(1 \\pm \\epsilon)$-approximation algorithm\nto approximate the $k$-th absolute moment $\\mathbf{E}_{x \\sim \\{-1,1\\}^n}[|p(x)^k|]$\nof a degree-$2$ polynomial.  The algorithm runs in fixed\npolynomial time for any constants $k$ and $\\epsilon.$\n  </p></div>\n</li>\n\n\n<br>\n\n\n\n<li class=\"paper applied_probability\" id=\"paper21\">\n  A robust Khintchine Inequality and computing optimal constants in Fourier analysis and high-dimensional geometry\n  <a onclick=\"reveal_abstract(21)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/bks-kk.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, R. Servedio\n  <br> SIAM Journal on Discrete Mathematics, 30-2 (2016), pp. 1058-1094\n  <br> Proceedings of the 40th Intl. Colloquium on Automata, Languages and Programming (ICALP 2013)\n\n  <div id=\"abstract21\" class=\"abstract\" style=\"display:none\">\nThis paper makes two contributions towards determining some well-studied\noptimal constants in Fourier analysis of Boolean functions\nand high-dimensional geometry.\n\n<p>\n\n(1) It has been known since 1994 \\cite{GL:94} that every linear threshold function has squared Fourier mass\nat least $1/2$ on its degree-$0$ and degree-$1$ coefficients.\nDenote the minimum such Fourier mass by $\\mathbf{W}^{\\leq 1}[\\mathbf{LTF}]$, \nwhere the minimum is taken over all $n$-variable linear threshold functions and all $n \\ge 0$.\nBenjamini, Kalai and Schramm \\cite{BKS:99} \nhave conjectured that the true value of $\\mathbf{W}^{\\leq 1}[\\mathbf{LTF}]$ is $2/\\pi$.\nWe make progress on this conjecture by proving that $\\mathbf{W}^{\\leq 1}[\\mathbf{LTF}]\n\\geq 1/2 + c$ for some absolute constant $c&gt;0$.\nThe key ingredient in our proof is a \"robust\" version of the well-known\nKhintchine inequality in functional analysis, which we\nbelieve may be of independent interest.\n\n</p><p>\n\n(2) We give an algorithm with the following property:  given any $\\eta &gt; 0$,\nthe algorithm runs in time $2^{\\mathrm{poly}(1/\\eta)}$ and determines the value of\n$\\mathbf{W}^{\\leq 1}[\\mathbf{LTF}]$ up to an additive error of $\\pm\\eta$.  We give a similar\n$2^{{\\mathrm{poly}(1/\\eta)}}$-time algorithm to determine <i>Tomaszewski's constant</i>\nto within an additive error of $\\pm \\eta$; this is the\nminimum (over all origin-centered hyperplanes $H$) fraction of points\nin $\\{-1,1\\}^n$ that lie within Euclidean distance $1$ of $H$.\nTomaszewski's constant is conjectured to be $1/2$; lower bounds on it\nhave been given by Holzman and Kleitman \\cite{HK92} and\n independently by Ben-Tal, Nemirovski and Roos \n\\cite{BNR02}.\nOur algorithms combine tools from anti-concentration\nof sums of independent random variables, Fourier analysis, and Hermite\nanalysis of linear threshold functions.\n   </p></div>\n</li>\n\n<br>\n\n<li class=\"paper nonparametric_statistics\" id=\"paper20\">\n  Learning Mixtures of Structured Distributions over Discrete Domains\n  <a onclick=\"reveal_abstract(20)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/learning-mixtures.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> S. Chan, I. Diakonikolas, R. Servedio, X. Sun\n  <br> Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2013)\n  <div id=\"abstract20\" class=\"abstract\" style=\"display:none\">\nLet $\\mathfrak{C}$ be a class of probability distributions over the discrete domain $[n] = \\{1,\\dots,n\\}.$\nWe show that if $\\mathfrak{C}$ satisfies a rather general condition -- essentially, that each distribution in\n$\\mathfrak{C}$ can be well-approximated by a variable-width\nhistogram with few bins -- then there is a highly efficient (both in terms of running time and sample complexity)\nalgorithm that can learn any mixture of $k$ unknown distributions from\n$\\mathfrak{C}.$\n\nWe analyze several natural types of distributions over $[n]$,\nincluding log-concave, monotone hazard rate and unimodal distributions,\nand show that they have the required structural property of being\nwell-approximated by a histogram with few bins.\nApplying our general algorithm, we\nobtain near-optimally efficient algorithms for all these mixture\nlearning problems as described below.  More precisely,\n\n<p>\n\n<b>Log-concave distributions:</b>  We learn any mixture of $k$\nlog-concave distributions over $[n]$ using $k \\cdot\n\\tilde{O}(1/\\epsilon^4)$ samples (independent of $n$) and running in time\n$\\tilde{O}(k \\log(n) / \\epsilon^4)$ bit-operations (note that reading a single\nsample from $[n]$ takes $\\Theta(\\log n)$ bit operations).\nFor the special case $k=1$ we give an efficient\nalgorithm using $\\tilde{O}(1/\\epsilon^3)$\nsamples; this generalizes the main result of \\cite{DDS12stoc} from the\nclass of Poisson Binomial distributions to the much broader class of all\nlog-concave distributions.  Our upper bounds are not far from\noptimal since any algorithm for this learning problem requires\n$\\Omega(k/\\epsilon^{5/2})$ samples.\n\n</p><p>\n\n<b>Monotone hazard rate (MHR) distributions:</b>\nWe learn any mixture of $k$ MHR distributions over $[n]$ using\n$O(k \\log (n/\\epsilon)/\\epsilon^4)$ samples and running in time $\\tilde{O}(k\n\\log^2(n) / \\epsilon^4)$ bit-operations.  Any algorithm for this learning problem must use $\\Omega(k \\log(n)/\\epsilon^3)$ samples.\n\n</p><p>\n\n<b> Unimodal distributions:</b>\nWe give an algorithm that learns any mixture of $k$ unimodal distributions\nover $[n]$ using $O(k \\log (n)/\\epsilon^{4})$ samples and running in time\n$\\tilde{O}(k \\log^2(n) / \\epsilon^{4})$ bit-operations.\nAny algorithm for this problem must use $\\Omega(k \\log(n)/\\epsilon^3)$ samples.\n\n</p></div>\n</li>\n\n\n\n<br>\n\n\n<li class=\"paper hypothesis_testing\" id=\"paper19\">\n  Testing $k$-modal Distributions: Optimal Algorithms via Reductions\n  <a onclick=\"reveal_abstract(19)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/testing-reductions.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> C. Daskalakis, I. Diakonikolas, R. Servedio, G. Valiant, P. Valiant\n  <br> Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2013)\n  <div id=\"abstract19\" class=\"abstract\" style=\"display:none\">\nWe give highly efficient algorithms, and almost matching lower bounds, for a range of basic statistical problems \nthat involve testing and estimating the $L_1$ (total variation) distance between two $k$-modal distributions $p$ and $q$ over the discrete domain $\\{1,\\dots,n\\}$.\nMore precisely, we consider the following four problems:  given sample access to an unknown $k$-modal\ndistribution $p$,\n\n<p>\n\nTesting identity to a known or unknown distribution:\n<br>\n(1) Determine whether $p = q$ (for an explicitly given $k$-modal distribution $q$) versus\n$p$ is $\\epsilon$-far from $q$;\n<br>\n\n(2) Determine whether $p=q$ (where $q$ is available via sample access) versus\n$p$ is $\\epsilon$-far from $q$;\n\n</p><p>\n\nEstimating $L_1$ distance (``tolerant testing'') against a known or unknown distribution:\n<br>\n(3) Approximate $d_{TV}(p,q)$ to within additive $\\epsilon$ where $q$ is an explicitly\ngiven $k$-modal distribution $q$;\n\n<br>\n\n(4)Approximate $d_{TV}(p,q)$ to within additive $\\epsilon$ where $q$ is available via sample access.\n\n</p><p>\n\nFor each of these four problems we give sub-logarithmic sample algorithms, that we show are tight up to additive $\\mathrm{poly}(k)$ \nand multiplicative $\\mathrm{polylog}\\log n+\\mathrm{polylog} k$ factors.\nThus our bounds significantly improve the previous results of \\cite{BKR:04}, which were for testing identity of distributions (items (1) and (2) above) in the special cases\n$k=0$ (monotone distributions) and $k=1$ (unimodal distributions) and required $O((\\log n)^3)$ samples. \n</p></div>\n</li>\n\n\n<br>\n\n<li class=\"paper multi_obj_opt\" id=\"paper28\">\n  An Optimal Algorithm for the Efficient Approximation of Convex Pareto Curves\n  <br> I. Diakonikolas, M. Yannakakis\n  <br> Manuscript, 2012\n\n</li>\n<br>\n\n\n<li class=\"paper multi_obj_opt\" id=\"paper18\">\n  Efficiency-Revenue Tradeoffs in Auctions\n  <a onclick=\"reveal_abstract(18)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/auctions-pareto.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> I. Diakonikolas, C.H. Papadimitriou, G. Pierrakos, Y. Singer\n  <br> Proceedings of the 39th Intl. Colloquium on Automata, Languages and Programming (ICALP 2012)\n    <div id=\"abstract18\" class=\"abstract\" style=\"display:none\">\nWhen agents with independent priors bid for a single item, Myerson's optimal auction maximizes expected revenue, whereas Vickrey's second-price auction optimizes social welfare.   We address the natural question of  <i>trade-offs</i> between the two criteria, that is, auctions that optimize, say, revenue under the constraint that the welfare is above a given level.  If one allows for randomized mechanisms, it is easy to see that there are polynomial-time mechanisms that achieve any point in the trade-off (the <i>Pareto curve</i>) between revenue and welfare.  We investigate whether one can achieve the same guarantees using  <i>deterministic</i> mechanisms.  We provide a negative answer to this question by showing that this is a (weakly) NP-hard problem. On the positive side, we provide polynomial-time deterministic mechanisms that approximate with arbitrary precision any point of the trade-off between these two fundamental objectives for the case of two bidders, even when the valuations are correlated arbitrarily.  The major problem left open by our work is whether there is such an algorithm for three or more bidders with independent valuation distributions.\n    </div>\n\n</li>\n<br>\n\n\n\n<li class=\"paper game_theory\" id=\"paper17\">\n  The Inverse Shapley Value Problem\n  <a onclick=\"reveal_abstract(17)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/shapley.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, R. Servedio\n    <br> Games and Economic Behavior, accepted with minor revisions\n  <br> Proceedings of the 39th Intl. Colloquium on Automata, Languages and Programming (ICALP 2012)\n    <div id=\"abstract17\" class=\"abstract\" style=\"display:none\">\nFor $f$ a weighted voting scheme used by $n$ voters to choose between two\ncandidates, the $n$ <i>Shapley-Shubik Indices</i> (or <i>Shapley values</i>)\nof $f$ provide a measure of how\nmuch control each voter can exert over the overall outcome of the vote.\nShapley-Shubik indices were introduced by Lloyd Shapley\nand Martin Shubik in 1954 \\cite{SS54} and are widely studied in\nsocial choice theory as a measure of the \"influence\" of voters.\nThe <i>Inverse Shapley Value Problem</i> is the problem of designing a weighted\nvoting scheme which (approximately) achieves a desired input vector of\nvalues for the Shapley-Shubik indices.  Despite much interest in this problem\nno provably correct and efficient algorithm was known prior to our work.\n\n<p>\n\nWe give the first efficient algorithm with provable performance guarantees for \nthe Inverse Shapley Value Problem.  For any constant $\\epsilon &gt; 0$\nour algorithm runs in fixed poly$(n)$ time (the degree of the\npolynomial is independent of $\\epsilon$) and has the following\nperformance guarantee:  given as input a vector of desired Shapley values,\nif any \"reasonable\" weighted voting scheme \n(roughly, one in which the threshold is not too skewed) \napproximately matches the desired vector of values to within \nsome small error,\nthen our algorithm explicitly outputs a weighted voting scheme that\nachieves this vector of Shapley values to within error $\\epsilon.$\nIf there is a \"reasonable\" voting scheme in which all\nvoting weights are integers at most $\\mathrm{poly}(n)$ that approximately achieves\nthe desired Shapley values, then our algorithm runs in time\n$\\mathrm{poly}(n)$ and outputs a weighted voting scheme that achieves\nthe target vector of Shapley values to within\nerror $\\epsilon=n^{-1/8}.$\n    </p></div>\n</li>\n\n<br>\n\n\n\n<li class=\"paper robust_PAC\" id=\"paper16\">\n  Nearly optimal solutions for the Chow Parameters Problem and low-weight approximation of halfspaces\n  <a onclick=\"reveal_abstract(16)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/chow.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> A. De, I. Diakonikolas, V. Feldman, R. Servedio\n  <br> Journal of the ACM, 61(2), 2014. Invited to Theory of Computing <strong>special issue on Analysis of Boolean functions </strong> (declined)\n  <br> Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC 2012)\n  <br> <strong>IBM Research 2014 Pat Goldberg Math/CS/EE Best Paper Award</strong> \n  <div id=\"abstract16\" class=\"abstract\" style=\"display:none\">\nThe <i>Chow parameters</i> of a Boolean function $f: \\{-1,1\\}^n \\to \\{-1,1\\}$ are its $n+1$ degree-$0$ and\ndegree-$1$ Fourier coefficients.  It has been known since 1961 \\cite{Chow:61, Tannenbaum:61} that the (exact values of the) Chow parameters of\nany linear threshold function $f$ uniquely specify $f$ within the space of all Boolean functions, but until\nrecently \\cite{OS11:chow} nothing was known about efficient algorithms for <i>reconstructing</i> $f$ \n(exactly or approximately) from exact or approximate values of its Chow parameters.  We refer to this reconstruction problem as the <i>Chow Parameters Problem.</i>\n\n<p>\n\nOur main result  is a new algorithm for the Chow Parameters Problem which,\ngiven (sufficiently accurate approximations to) the Chow parameters of any linear threshold function $f$, runs in time\n$\\tilde{O}(n^2)\\cdot (1/\\epsilon)^{O(\\log^2(1/\\epsilon))}$ and\nwith high probability outputs a representation of an LTF  $f'$ that is $\\epsilon$-close to $f$ in Hamming distance.\nThe only previous algorithm \\cite{OS11:chow} had running time $\\mathrm{poly}(n) \\cdot 2^{2^{\\tilde{O}(1/\\epsilon^2)}}.$\n\n</p><p>\nAs a byproduct of our approach, we show that for any linear threshold function $f$ over $\\{-1,1\\}^n$,\nthere is a linear threshold function $f'$ which is $\\epsilon$-close to $f$ and has all weights that are integers of magnitude at most $\\sqrt{n} \\cdot (1/\\epsilon)^{O(\\log^2(1/\\epsilon))}$.\nThis significantly improves the previous best result of~\\cite{DiakonikolasServedio:09} which gave\na $\\mathrm{poly}(n) \\cdot 2^{\\tilde{O}(1/\\epsilon^{2/3})}$ weight bound, and is close to the\nknown lower bound of\n$\\max\\{\\sqrt{n},$ $(1/\\epsilon)^{\\Omega(\\log \\log (1/\\epsilon))}\\}$ \\cite{Goldberg:06b,Servedio:07cc}.\nOur techniques also yield improved algorithms for related problems in learning theory.\n\n</p><p>\nIn addition to being significantly stronger than previous work, our results\nare obtained using conceptually simpler proofs.\nThe two main ingredients underlying our results are (1) a new structural\nresult showing that for $f$ any linear threshold function and $g$ any bounded\nfunction, if the Chow parameters of $f$ are close to the Chow\nparameters of $g$ then $f$ is close to $g$; (2) a new boosting-like algorithm\nthat given approximations to the Chow parameters of a linear threshold function outputs a bounded\nfunction whose Chow parameters are close to those of $f$.\n </p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper15\">\n  Learning Poisson Binomial distributions\n  <a onclick=\"reveal_abstract(15)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/pbds-learning.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> C. Daskalakis, I. Diakonikolas, R. Servedio\n  <br> Invited to Algorithmica  <strong>special issue on New Theoretical Challenges in Machine Learning</strong>\n  <br> Proceedings of the 44th Annual ACM Symposium on Theory of Computing (STOC 2012)\n  <div id=\"abstract15\" class=\"abstract\" style=\"display:none\">\nWe consider a basic problem in unsupervised learning:\nlearning an unknown <i>Poisson Binomial Distribution</i>.\nA Poisson Binomial Distribution (PBD) over $\\{0,1,\\dots,n\\}$\nis the distribution of a sum of $n$ independent Bernoulli\nrandom variables which may have arbitrary, potentially non-equal,\nexpectations. These distributions were first studied by S. Poisson in 1837 \\cite{Poisson:37} and are a natural\n$n$-parameter generalization of the familiar Binomial Distribution.\nSurprisingly, prior to our work this basic learning problem\nwas poorly understood, and known results for it were far from\noptimal.\n\n<p>\n\n\nWe essentially settle the complexity of the learning problem for this\nbasic class of distributions.\nAs our main result we give a highly efficient algorithm which learns to \n$\\epsilon$-accuracy using $\\tilde{O}(1/\\epsilon^3)$ samples <i>independent of $n$</i>. \nThe running time of the algorithm is <i>quasilinear</i> in the\nsize of its input data. This is nearly optimal\nsince any algorithm must use $\\Omega(1/\\epsilon^2)$ samples.\nWe also give positive and negative results for some extensions of this  learning problem.\n</p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper nonparametric_statistics\" id=\"paper14\">\n  Learning $k$-modal distributions via testing\n  <a onclick=\"reveal_abstract(14)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/kmodal-learning.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> C. Daskalakis, I. Diakonikolas, R. Servedio\n  <br> Theory of Computing, 10 (20), 535-570 (2014)\n  <br> Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2012)  \n  <br> <em><font color=\"gray\">Oded Goldreich's Choices <a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/072.html\">#72</a></font> </em>\n  <div id=\"abstract14\" class=\"abstract\" style=\"display:none\">\nA $k$-modal probability distribution over the domain $\\{1,...,n\\}$ is one whose\nhistogram has at most $k$ \"peaks\" and \"valleys.\" Such distributions are\nnatural generalizations of monotone ($k=0$) and unimodal ($k=1$) \nprobability distributions, which  have been intensively studied in probability theory and statistics.\n\nIn this paper we consider the problem of learning an unknown $k$-modal distribution.\nThe learning algorithm is given access to independent samples drawn from the $k$-modal\ndistribution $p$, and must output a hypothesis distribution $\\hat{p}$ such that with high\nprobability the total variation distance between $p$ and $\\hat{p}$ is at most $\\epsilon.$\n\n<p>\n\nWe give an efficient algorithm for this problem that runs in time $\\mathrm{poly}(k,\\log(n),1/\\epsilon)$.\nFor $k \\leq \\tilde{O}(\\sqrt{\\log n})$, the number of samples used by our algorithm is very close (within an\n$\\tilde{O}(\\log(1/\\epsilon))$ factor) to being information-theoretically optimal.  Prior to this\nwork computationally efficient algorithms were known only for the cases $k=0,1$\n\\cite{Birge:87b,Birge:97}.\n\n</p><p>\n\nA novel feature of our approach is that our learning algorithm crucially uses a new <i>property testing</i> algorithm as a key subroutine.  \nThe learning algorithm uses the property tester to efficiently\ndecompose the $k$-modal distribution into $k$ (near)-monotone distributions, which are easier to\nlearn.\n</p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper applied_probability\" id=\"paper26\">\n  Noise Stable Halfspaces are Close to Very Small Juntas\n  <a onclick=\"reveal_abstract(26)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/ltf-bourgain.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> I. Diakonikolas, R. Jaiswal, R. Servedio, L.-Y.Tan, A. Wan\n  <br> Chicago Journal of Theoretical Computer Science, 2015\n  <div id=\"abstract26\" class=\"abstract\" style=\"display:none\">\nBourgain~\\cite{Bourgain:02} showed that any noise stable Boolean function $f$\ncan be well-approximated by a junta.\nIn this note we give an exponential sharpening of the parameters of\nBourgain's result under the additional assumption that $f$ is a halfspace.\n  </div>\n</li>\n\n\n\n\n<br>\n\n\n<li class=\"paper multi_obj_opt\" id=\"paper13\">\n  Supervised Design Space Exploration by Compositional Approximation of Pareto sets\n  <a onclick=\"reveal_abstract(13)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/dac-pareto.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> H.-Y. Liu, I. Diakonikolas, M. Petracca, L.P. Carloni\n  <br> Proceedings of the 48th Design Automation Conference (DAC 2011)\n  <div id=\"abstract13\" class=\"abstract\" style=\"display:none\">\nTechnology scaling allows the integration of billions of transistors on the same\ndie but CAD tools struggle in keeping up with the increasing design complexity. \nDesign productivity for multi-core SoCs increasingly depends on \ncreating and maintaining reusable components and hierarchically\ncombining them to form larger composite cores. \nCharacterizing such composite cores with respect to their power/performance\ntrade-offs is critical for design reuse across various products and relies\nheavily on synthesis tools. \n\n<p>\n\n\nWe present $\\mathrm{CAPS}$, an online adaptive algorithm that efficiently \nexplores the design space of any given core and returns an accurate\ncharacterization of its implementation trade-offs in terms of an approximate\nPareto set.  \nIt does so by supervising the order of the time-consuming logic-synthesis runs\non the core's components.   \nOur algorithm can provably achieve the desired precision on the approximation in \nthe shortest possible time, without having any a-priori information on any \ncomponent.  \nWe also show that, in practice, $\\mathrm{CAPS}$ works even better than what is guaranteed\nby the theory. \n </p></div>\n</li>\n\n\n\n<br>\n\n\n<li class=\"paper other_topics\" id=\"paper12\">\n  Disjoint-Path Facility Location: Theory and Practice\n  <a onclick=\"reveal_abstract(12)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/monitoring.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br> L. Breslau, I. Diakonikolas, N. Duffield, Y.Gu, M.T. Hajiaghayi, D.S. Johnson, H. Karloff, M. Resende, S.Sen\n  <br> Proceedings of the 13th Workshop on Algorithm Engineering and Experiments (ALENEX 2011)\n  <br> Journal version in Operations Research, 2019 <a href=\"https://arxiv.org/abs/1611.01210\">[arxiv]</a>\n\n  <div id=\"abstract12\" class=\"abstract\" style=\"display:none\">\nThis paper is a theoretical and experimental study of two related facility\nlocation problems that emanated from networking. Suppose we are given a\nnetwork modeled as a directed graph $G = (V,A)$, together with\n(not-necessarily-disjoint) subsets $C$ and $F$ of $V$ , where $C$ is a set of\ncustomer locations and $F$ is a set of potential facility locations (and\ntypically $C\\subseteq F$). Our goal is to find a minimum sized subset $F' \\subseteq F$\nsuch that for every customer $c \\in C$ there are two locations $f_1, f_2 \\in F'$\nsuch that traffic from $c$ to $f_1$ and to $f_2$ is routed on disjoint paths\n(usually shortest paths) under the network's routing protocols.\nAlthough we prove that this problem is impossible to approximate in the\nworst case even to within a factor of $2^{\\log^{1-\\epsilon} n}$ for any $\\epsilon&gt;0$\n(assuming no NP-complete language can be solved in quasi-polynomial\ntime), we show that the situation is much better in practice. We\npropose three algorithms that build solutions and determine lower\nbounds on the optimum solution, and evaluate them on several large real\nISP topologies and on synthetic networks designed to reflect real-world\nLAN/WAN network structure. Our main algorithms are (1) an algorithm\nthat performs multiple runs of a straightforward randomized greedy\nheuristic and returns the best result found, (2) a genetic\nalgorithm that uses the greedy algorithm as a subroutine, and (3) a new\n\"Double Hitting Set\" algorithm. All three approaches perform surprising\nwell, although, in practice, the most cost-effective approach is the\nmultirun greedy algorithm. This yields results that average within 0.7%\nof optimal for our synthetic instances and within 2.9% for our\nreal-world instances, excluding the largest (and most realistic) one.\nFor the latter instance, the other two algorithms come into their own,\nfinding solutions that are more than three times better than those of\nthe multi-start greedy approach. In terms of our motivating monitoring\napplication, where every customer location can be a facility location,\nthe results are even better. Here the above Double Hitting Set solution\nis 90% better than the default solution which places a monitor at each\ncustomer location. Our results also show that, on\naverage for our real-world instances, we could save an additional 18%\nby choosing the (shortest path) routes ourselves, rather than taking\nthe simpler approach of relying on the network to choose them for us.\n</div>\n\n</li>\n<br>\n\n\n<li class=\"paper robust_PAC\" id=\"paper11\">\n  Hardness Results for Agnostically Learning Low-Degree Polynomial Threshold Functions\n  <a onclick=\"reveal_abstract(11)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/hardness-learning-ptfs.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, R. O'Donnell, R. Servedio, Y.Wu\n  <br> Proceedings of the 22nd Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2011)\n  <div id=\"abstract11\" class=\"abstract\" style=\"display:none\">\nHardness results for maximum agreement problems have close connections to hardness results for\nproper learning in computational learning theory.  \nIn this paper we prove two hardness results for the problem of finding a low degree polynomial threshold function (PTF) \nwhich has the maximum possible agreement with a given set of labeled examples in $\\mathbf{R}^n \\times \\{-1,1\\}.$\nWe prove that for any constants $d\\geq 1, \\epsilon &gt; 0$,\n\n<p>\n\n(1) Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a degree-$d$ PTF that\nis consistent with a $(1/2 + \\epsilon)$ fraction of a given set of labeled examples in $\\mathbf{R}^n \\times \\{-1,1\\}$, \neven if there exists a degree-$d$ PTF that is consistent with a $1-\\epsilon$ fraction of the examples.\n\n</p><p>\n\n(2) It is NP-hard to find a degree-$2$ PTF that is consistent with\na $(1/2 + \\epsilon)$ fraction of a given set of labeled examples in $\\mathbf{R}^n \\times \\{-1,1\\}$, even if\nthere exists a halfspace (degree-$1$ PTF) that is consistent with a $1 - \\epsilon$ fraction of the\nexamples.\n\n</p><p>\n\nThese results immediately imply the following hardness of learning results: (i) Assuming the\nUnique Games Conjecture, there is no better-than-trivial proper  learning algorithm that agnostically learns degree-$d$ PTFs under arbitrary distributions; \n(ii)  There is no better-than-trivial learning algorithm  that outputs degree-$2$ PTFs and agnostically learns halfspaces (i.e., degree-$1$ PTFs) under arbitrary distributions.\n  </p></div>\n</li>\n\n\n\n\n<br>\n\n\n<li class=\"paper applied_probability\" id=\"paper10\">\n  Bounded Independence Fools Degree-$2$ Threshold Functions\n  <a onclick=\"reveal_abstract(10)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/bifdeg2.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, D. Kane, J. Nelson\n  <br> Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science (FOCS 2010)\n  <div id=\"abstract10\" class=\"abstract\" style=\"display:none\">\n  Let $x$ be a random vector coming from any $k$-wise independent distribution over $\\{-1,1\\}^n$.\nFor an $n$-variate degree-$2$ polynomial $p$, we prove that $\\mathbf{E}[\\mathrm{sgn}(p(x))]$ is determined up to an additive $\\epsilon$ for $k =\n\\mathrm{poly}(1/\\epsilon)$. This gives a large class of explicit pseudo-random generators against such functions and answers an open\nquestion of Diakonikolas et al. (FOCS 2009).\n\n<p>\n\n\nIn the process, we develop a novel analytic technique we dub <i>multivariate FT-mollification</i>. This\nprovides a generic tool to approximate bounded (multivariate) functions by <i>low-degree</i> polynomials (with respect to\nseveral different notions of approximation). A univariate version of the method was introduced by Kane et al. (SODA 2010) in\nthe context of streaming algorithms. In this work, we refine it and generalize it to the multivariate setting. We believe that\nour technique is of independent mathematical interest. To illustrate its generality, we note that it implies a multidimensional\ngeneralization of Jackson's classical result in approximation theory due to (Newman and Shapiro, 1963).\n\n</p><p>\n\n\nTo obtain our main result, we combine the FT-mollification technique with several linear algebraic and probabilistic\ntools. These include the invariance principle of of Mossell, O'Donnell and Oleszkiewicz, anti-concentration bounds for\nlow-degree polynomials, an appropriate decomposition of degree-$2$ polynomials, and a generalized hyper-contractive inequality\nfor quadratic forms which takes the operator norm of the associated matrix into account. Our analysis is quite modular; it\nreadily adapts to show that intersections of halfspaces and degree-$2$ threshold functions are fooled by bounded independence.\nFrom this it follows that $\\Omega(1/\\epsilon^2)$-wise independence derandomizes the Goemans-Williamson hyperplane rounding scheme.\n\n</p><p>\n\n\nOur techniques unify, simplify, and in some cases improve several recent results in the literature concerning threshold\nfunctions. For the case of \"regular\" halfspaces we give a simple proof of an optimal independence bound of\n$\\Theta(1/\\epsilon^2)$, improving upon Diakonikolas et al. (FOCS 2009) by polylogarithmic factors. This yields the first optimal\nderandomization of the Berry-Esseen theorem and -- combined with the results of Kalai et al. (FOCS 2005) -- implies a\nfaster algorithm for the problem of agnostically learning halfspaces.  \n  </p></div>\n\n</li>\n\n<br>\n\n\n<li class=\"paper robust_PAC\" id=\"paper9\">\n  Average Sensitivity and Noise Sensitivity of Polynomial Threshold Functions\n  <a onclick=\"reveal_abstract(9)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/sensitivity-sicomp.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, P. Raghavendra, R. Servedio, L.-Y. Tan\n  <br> SIAM Journal on Computing, 43(1), 231-253 (2014)\n  <br> Proceedings of the 42nd Annual ACM Symposium on Theory of Computing (STOC 2010)\n  <font size=\"3\">(Conference version merged with <a href=\"http://arxiv.org/abs/0909.5175\" class=\"abstract_link\">this</a> paper by Harsha, Klivans and Meka)<font>\n  <div id=\"abstract9\" class=\"abstract\" style=\"display:none\">\n  We give the first non-trivial upper bounds on the Boolean average\n  sensitivity and noise sensitivity of degree-$d$ polynomial threshold\n  functions (PTFs). Our bound on the Boolean average sensitivity of PTFs represents the first progress\n  towards the resolution of a conjecture of Gotsman and Linial \\cite{GL:94}, which states that the symmetric function slicing the\n  middle $d$ layers of the Boolean hypercube has the highest average\n  sensitivity of all degree-$d$ PTFs.  Via the $L_1$ polynomial\n  regression algorithm of Kalai et al. \\cite{KKMS:08}, our bound on\n  Boolean noise sensitivity yields the first polynomial-time\n  agnostic learning algorithm for the broad class of constant-degree\n  PTFs under the uniform distribution.\n\n<p>\n\n  To obtain our bound on the Boolean average sensitivity of PTFs,\n  we generalize the \"critical-index\" machinery of \\cite{Servedio:07cc}\n  (which in that work applies to halfspaces, i.e., degree-$1$ PTFs) to general PTFs.\n  Together with the \"invariance principle\" of \\cite{MOO10},\n  this allows us to essentially reduce the Boolean setting\n  to the Gaussian setting. The main ingredients used to obtain our bound\n  in the Gaussian setting are tail bounds and anti-concentration bounds on\n  low-degree polynomials in Gaussian random variables\n  \\cite{Janson:97,CW:01}.  Our bound on Boolean noise sensitivity is achieved\n  via a simple reduction from upper bounds on average sensitivity of Boolean\n  PTFs to corresponding bounds on noise sensitivity.  </p></div>\n</font></font></li><font size=\"3\"><font>\n\n<br>\n\n\n<li class=\"paper applied_probability\" id=\"paper8\">\n  A Regularity Lemma, and Low-weight Approximators, for low-degree Polynomial Threshold Functions\n  <a onclick=\"reveal_abstract(8)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/regularity-full.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, R. Servedio, L.-Y. Tan, A. Wan\n  <br> Theory of Computing, 10(2), 27-53 (2014)\n  <br> Proceedings of the 25th Annual IEEE Conference on Computational Complexity (CCC 2010)\n  <div id=\"abstract8\" class=\"abstract\" style=\"display:none\">\nWe give a \"regularity lemma\" for degree-$d$ polynomial threshold\nfunctions (PTFs) over the Boolean cube $\\{-1,1\\}^n$.  Roughly\nspeaking, this result shows that every degree-$d$ PTF can be\ndecomposed into a constant number of subfunctions such that almost\nall of the subfunctions are close to being regular PTFs. Here a \"regular\" PTF\nis a PTF $\\mathrm{sign}(p(x))$ where the influence of each variable on the\npolynomial $p(x)$ is a small fraction of the total influence of $p.$\n\n<p>\n\nAs an application of this regularity lemma, we prove that for any constants $d \\geq 1, \\epsilon &gt; 0$, every degree-$d$ PTF over $n$\nvariables can be approximated to accuracy $\\epsilon$ by a constant-degree PTF that has integer weights of total magnitude $O_{\\epsilon,d}(n^d).$\nThis weight bound is shown to be optimal up to logarithmic factors.\n  </p></div>\n</li>\n\n\n<br>\n\n\n\n<li class=\"paper multi_obj_opt\" id=\"paper7\">\n  How Good is the Chord Algorithm?\n  <a onclick=\"reveal_abstract(7)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/chord-full.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>C. Daskalakis, I. Diakonikolas, M. Yannakakis\n  <br> SIAM Journal on Computing, 45(3), pp. 811-858, 2016\n  <br> Proceedings of the 21st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2010)\n  <div id=\"abstract7\" class=\"abstract\" style=\"display:none\">\nThe Chord algorithm is a popular, simple  method for the succinct approximation of curves,\nwhich is widely used, under different names, in a variety of areas, such as,\nmultiobjective and parametric optimization,  computational geometry, and graphics.\nWe analyze the performance of the Chord algorithm, as compared to the\noptimal approximation that achieves a desired accuracy with the minimum number of points.\nWe prove sharp upper and lower bounds, both in the worst case and average case setting.\n  </div>\n</li>\n\n\n\n<br>\n\n\n\n<li class=\"paper applied_probability\" id=\"paper6\">\n  Bounded Independence Fools Halfspaces\n  <a onclick=\"reveal_abstract(6)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/bifh-final.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, P. Gopalan, R. Jaiswal, R. Servedio, E. Viola\n  <br> SIAM Journal on Computing, 39(8), 3441-3462 (2010)\n  <br> Proceedings of the 50th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2009)\n  <div id=\"abstract6\" class=\"abstract\" style=\"display:none\">\nWe show that any distribution on $\\{-1, 1\\}^n$ that is $k$-wise\nindependent fools any halfspace (a.k.a. linear threshold function) $h : \\{-1, 1\\}^n \\to \\{-1, 1\\}$, i.e.,\nany function of the form $h(x) = \\mathrm{sign}(\\sum_{i = 1}^n w_i x_i - \\theta)$ where the $w_1,\\ldots,w_n,\\theta$ are arbitrary real\nnumbers, with error $\\epsilon$ for $k = O(\\epsilon^{-2}\\log^2(1/\\epsilon))$. Our result is tight up\nto $\\log(1/\\epsilon)$ factors. Using standard constructions of $k$-wise independent distributions, we obtain the first\nexplicit pseudorandom generators $G : \\{-1, 1\\}^s \\to \\{-1, 1\\}^n$ that fool halfspaces. \nSpecifically, we fool halfspaces with error $\\epsilon$ and\nseed length $s = k \\cdot \\log n = O(\\log n \\cdot \\epsilon^{-2} \\log^2(1/\\epsilon))$.\n\n<p>\n\n\nOur approach combines classical tools  from real approximation theory with\nstructural results on halfspaces by Servedio (Comput. Complexity 2007).\n  </p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper applied_probability\" id=\"paper5\">\n  Improved Approximation of Linear Threshold Functions\n  <a onclick=\"reveal_abstract(5)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/ltf-approx-cc.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, R. Servedio\n  <br> Computational Complexity, 22(3), 623-677 (2013)\n  <br> Proceedings of the 24th Annual IEEE Conference on Computational Complexity (CCC 2009) \n  <div id=\"abstract5\" class=\"abstract\" style=\"display:none\">\nWe prove two main results on how arbitrary linear threshold\nfunctions $f(x) = \\mathrm{sign}(w\\cdot x - \\theta)$ over the $n$-dimensional\nBoolean hypercube can be approximated by simple threshold functions.\n\n<p>\n\n\nOur first result shows that every $n$-variable threshold function\n$f$ is $\\epsilon$-close to a threshold function depending only on\n$\\mathrm{Inf}(f)^2 \\cdot \\mathrm{poly}(1/\\epsilon)$ many variables, where $\\mathrm{Inf}(f)$\ndenotes the total influence or average sensitivity of $f.$ This is\nan exponential sharpening of Friedgut's well-known theorem\n\\cite{Friedgut:98}, which states that every Boolean function $f$ is\n$\\epsilon$-close to a function depending only on $2^{O(\\mathrm{Inf}(f)/\\epsilon)}$\nmany variables, for the case of threshold functions. We complement\nthis upper bound by showing that $\\Omega(\\mathrm{Inf}(f)^2 + 1/\\epsilon^2)$\nmany variables are required for $\\epsilon$-approximating threshold\nfunctions.\n\n</p><p>\n\n\nOur second result is a proof that every $n$-variable threshold\nfunction is $\\epsilon$-close to a threshold function with integer\nweights at most $\\mathrm{poly}(n) \\cdot 2^{\\tilde{O}(1/\\epsilon^{2/3})}.$ This\nis an improvement, in the dependence on the error\nparameter $\\epsilon$, on an earlier result of \\cite{Servedio:07cc} which\ngave a $\\mathrm{poly}(n) \\cdot 2^{\\tilde{O}(1/\\epsilon^{2})}$ bound.  Our\nimprovement is obtained via a new proof technique that uses strong\nanti-concentration bounds from probability theory. The new technique\nalso gives a simple and modular proof of the original\n\\cite{Servedio:07cc} result, and extends to give low-weight\napproximators for threshold functions under a range of probability\ndistributions other than the uniform distribution.\n  </p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper property_testing\" id=\"paper4\">\n  Efficiently Testing Sparse $GF(2)$ Polynomials\n  <a onclick=\"reveal_abstract(4)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/sparse-poly-testing.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, H. Lee, K. Matulef, R. Servedio, A. Wan\n  <br> Algorithmica, 61(3), 580-605 (2011)\n  <br>  Proceedings of the 35th Intl. Colloquium on Automata, Languages and Programming (ICALP 2008)\n  <div id=\"abstract4\" class=\"abstract\" style=\"display:none\">\nWe give the first algorithm that is both query-efficient and\ntime-efficient for testing whether an unknown function $f: \\{0,1\\}^n \\to\n\\{0,1\\}$ is an $s$-sparse $GF(2)$ polynomial versus $\\epsilon$-far from\nevery such polynomial.  Our algorithm makes $\\mathrm{poly}(s,1/\\epsilon)$\nblack-box queries to $f$ and runs in time $n \\cdot \\mathrm{poly}(s,1/\\epsilon)$.\nThe only previous algorithm for this testing problem \\cite{DLM+:07}\nused $\\mathrm{poly}(s,1/\\epsilon)$ queries, but had running time exponential in\n$s$ and super-polynomial in $1/\\epsilon$.\n\n<p>\n\nOur approach significantly extends the \"testing by implicit\nlearning\" methodology of \\cite{DLM+:07}. The learning component of\nthat earlier work was a brute-force exhaustive search over a concept\nclass to find a hypothesis consistent with a sample of random\nexamples. In this work, the learning component is  a\nsophisticated  exact learning algorithm for sparse $GF(2)$\npolynomials due to Schapire and Sellie \\cite{SchapireSellie:96}. A\ncrucial element of this work, which enables us to simulate the\nmembership queries required by \\cite{SchapireSellie:96}, is an\nanalysis establishing new properties of how sparse $GF(2)$\npolynomials simplify under certain restrictions of \"low-influence\"\nsets of variables.\n  </p></div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper multi_obj_opt\" id=\"paper3\">\n  Succinct Approximate Convex Pareto Curves\n  <a onclick=\"reveal_abstract(3)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/convex-pareto-soda.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, M. Yannakakis\n  <br> Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA 2008)\n  <div id=\"abstract3\" class=\"abstract\" style=\"display:none\">\nWe study the succinct approximation of convex Pareto curves of multiobjective optimization problems.\nWe propose the concept of $\\epsilon$-convex Pareto ($\\epsilon$-CP) set as the appropriate one for the convex setting, and\nobserve that it can offer arbitrarily more compact representations than $\\epsilon$-Pareto sets in this context. We characterize\nwhen an $\\epsilon$-CP can be constructed in polynomial time in terms of an efficient routine $\\textrm{Comb}$ for optimizing\n(exactly or approximately) monotone linear combinations of the objectives. We investigate the problem of computing minimum size\n$\\epsilon$-convex Pareto sets, both for discrete (combinatorial) and continuous (convex) problems, and present general\nalgorithms using a $\\textrm{Comb}$ routine. For bi-objective problems, we show that if we have an exact $\\textrm{Comb}$\noptimization routine, then we can compute the minimum $\\epsilon$-CP for continuous problems (this applies for example to\nbi-objective Linear Programming and Markov Decision Processes), and factor 2 approximation to the minimum $\\epsilon$-CP for\ndiscrete problems (this applies for example to bi-objective versions of polynomial-time solvable combinatorial problems such as\nShortest Paths, Spanning Tree, etc.). If we have an approximate $\\textrm{Comb}$ routine, then we can compute factor 3 and 6\napproximations respectively to the minimum $\\epsilon$-CP for continuous and discrete bi-objective problems.\nWe consider also the case of three and more objectives and present some upper and lower bounds.\n  </div>\n</li>\n\n\n<br>\n\n\n<li class=\"paper property_testing\" id=\"paper2\">\n  Testing for Concise Representations\n  <a onclick=\"reveal_abstract(2)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/testing-concise-full.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, H. Lee, K. Matulef, K. Onak, R. Rubinfeld, R. Servedio, A. Wan\n  <br> Proceedings of the 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2007)\n    <br> <em><font color=\"gray\">Oded Goldreich's Choices <a href=\"http://www.wisdom.weizmann.ac.il/~oded/MC/039.html\">#39</a></font> </em>\n  <div id=\"abstract2\" class=\"abstract\" style=\"display:none\">\nWe describe a general method for testing whether a function on $n$\ninput variables has a concise representation.  The approach combines\nideas from the junta test of Fischer <i>et al.</i> \\cite{FKR+:04} with ideas\nfrom learning theory, and yields property testers that make\npoly$(s/\\epsilon)$ queries (independent of $n$) for Boolean function\nclasses such as $s$-term DNF formulas\n(answering a question posed by Parnas <i>et al.</i> \\cite{PRS02}),\nsize-$s$ decision trees, size-$s$ Boolean formulas,\nand size-$s$ Boolean circuits.\n\n<p>\n\nThe method can be applied to non-Boolean valued function classes as\nwell. This is achieved via a generalization of the notion of\n<i>variation</i> from Fischer <i>et al.</i> to non-Boolean functions. Using\nthis generalization we extend the original junta test of Fischer\n<i> et al.</i> to work for non-Boolean functions, and give\npoly$(s/\\epsilon)$-query testing algorithms for non-Boolean valued\nfunction classes such as size-$s$ algebraic circuits and $s$-sparse\npolynomials over finite fields.\n\n</p><p>\n\nWe also prove an $\\tilde\\Omega(\\sqrt{s})$ query lower bound for\nnon-adaptively testing $s$-sparse polynomials over finite fields of\nconstant size.  This shows that in some instances, our general method\nyields a property tester with query complexity that is optimal (for nonadaptive\nalgorithms) up to a polynomial factor.\n  </p></div>\n</li>\n\n<br>\n\n\n\n<li class=\"paper multi_obj_opt\" id=\"paper1\">\n  Small Approximate Pareto Sets for Biobjective Shortest Paths and Other Problems\n  <a onclick=\"reveal_abstract(1)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/papers/pareto-sicomp.pdf\" class=\"abstract_link\">[pdf]</a>\n  <br>I. Diakonikolas, M. Yannakakis\n  <br> SIAM Journal on Computing, 39(4), 1340-1371 (2009)\n  <br> Proceedings of the 10th Intl. Workshop on Approximation, Randomization, and Combinatorial Optimization (APPROX 2007)\n  <br> <em> <font color=\"gray\">Honorable Mention, George Nicholson student paper competition \n  <a href=\"https://www.informs.org/Recognize-Excellence/Award-Recipients/Ilias-Diakonikolas\">INFORMS society</a>, 2009</font> </em>\n  <div id=\"abstract1\" class=\"abstract\" style=\"display:none\">\nWe investigate the problem of computing a minimum set of solutions that approximates within a specified accuracy $\\epsilon$ the\nPareto curve of a multiobjective optimization problem. We show that for a broad class of  bi-objective problems (containing\nmany important widely studied problems such as shortest paths, spanning tree, matching and many others), we can compute in\npolynomial time an $\\epsilon$-Pareto set that contains at most twice as many solutions as the minimum such set. Furthermore we\nshow that the factor of $2$ is tight for these problems, i.e., it is NP-hard to do better. We present upper and lower bounds\nfor three or more objectives, as well as for the dual problem of computing a specified number $k$ of solutions which provide a\ngood approximation to the Pareto curve.  \n\n  </div>\n</li>\n\n\n<br>\n\n\n</font></font></ol><font size=\"3\"><font>\n</font></font></div><font size=\"3\"><font>\n\n\n\n\n<h2>Thesis</h2>\n<ul>\n\n  Approximation of Multiobjective Optimization Problems\n  <a onclick=\"reveal_abstract(0)\" class=\"abstract_link\">[abstract]</a>\n  <a href=\"http://www.iliasdiakonikolas.org/thesis.pdf\" class=\"abstract_link\">[pdf]</a>\n   <br> <b>Ph.D. Thesis</b>, Dept. of Computer Science, Columbia University, May 2011\n  <br> Awarded with Distinction (for <b>Best Ph.D. thesis</b> in Computer Science)\n  <div id=\"abstract0\" class=\"abstract\" style=\"display:none\">\nWe study optimization problems with multiple objectives. Such problems are pervasive across many diverse disciplines -- in\neconomics, engineering, healthcare, biology, to name but a few -- and heuristic approaches to solve them have already been\ndeployed in several areas, in both academia and industry. Hence, there is a real need for a rigorous investigation of the\nrelevant questions.\n\n<p>\n\nIn such problems we are interested not in a single optimal solution, but in the tradeoff between the different objectives. This\nis captured by the <i>tradeoff</i> or <i>Pareto</i> curve, the set of all feasible solutions whose vector of the various\nobjectives is not dominated by any other solution. Typically, we have a small number of objectives and we wish to plot the\ntradeoff curve to get a sense of the design space. Unfortunately, typically the tradeoff curve has exponential size for\ndiscrete optimization problems even for two objectives (and is typically infinite for continuous problems). Hence, a natural\ngoal in this setting is, given an instance of a multiobjective problem, to efficiently obtain a \"good\" approximation to the\nentire solution space with ``few'' solutions. This has been the underlying goal in much of the research in the multiobjective\narea, with many heuristics proposed for this purpose, typically however without any performance guarantees or complexity\nanalysis.\n\n</p><p>\n\n\nWe develop efficient algorithms for the succinct approximation of the Pareto set for a large class of multiobjective problems.\nFirst, we investigate the problem of computing a minimum set of solutions that approximates within a specified accuracy the\nPareto curve of a multiobjective optimization problem. We provide approximation algorithms with tight performance guarantees\nfor bi-objective problems and make progress for the more challenging case of three and more objectives. Subsequently, we\npropose and study the notion of the approximate convex Pareto set; a novel notion of approximation to the Pareto set, as the\nappropriate one for the convex setting. We characterize when such an approximation can be efficiently constructed and\ninvestigate the problem of computing minimum size approximate convex Pareto sets, both for discrete and convex problems. Next,\nwe turn to the problem of approximating the Pareto set as efficiently as possible. To this end, we analyze the Chord algorithm,\na popular, simple method for the succinct approximation of curves, which is widely used, under different names, in a variety of\nareas, such as, multiobjective and parametric optimization, computational geometry, and graphics.\n</p></div>\n\n</ul>\n\n\n<br>\n\n\n<h2>Professional Activities</h2>\n\n<font size=\"4\"> \t\n\t<ul>\n\n\n<li> <b>Program Committees:</b> \n\n<a href=\"https://icml.cc/\">ICML 2020</a> (Area Chair),\n<a href=\"http://learningtheory.org/colt2020/\">COLT 2020</a> (Senior PC), \n<a href=\"http://acm-stoc.org/stoc2020/\">STOC 2020</a>, \n<a href=\"http://alt2020.algorithmiclearningtheory.org/\">ALT 2020</a>, \n<a href=\"http://www.learningtheory.org/colt2019/\">COLT 2019</a>, \n<a href=\"http://www.learningtheory.org/colt2017/\">COLT 2017</a>,\n<a href=\"http://www.easyconferences.eu/icalp2016/\">ICALP 2016</a>, \n<a href=\"http://people.csail.mit.edu/ronitt/2015cfp.html\">STOC 2015</a>, \n<a href=\"http://icalp2014.itu.dk/\">ICALP 2014</a>, \n<a href=\"http://algo2014.ii.uni.wroc.pl/esa/\">ESA 2014</a></li>\n\n<li>Fall 2018: co-organizer of <a href=\"https://simons.berkeley.edu/data-science-2018-2\">Robust and High-Dimensional Statistics Workshop</a> at the Simons Institute.</li>\n\n<li>In August 2018, I gave a tutorial on Algorithmic High-Dimensional Robust Statistics, as part of the Foundations of Data Science Bootcamp, at the Simons Institute. The slides of my presentation\ncan be found <a href=\"http://www.iliasdiakonikolas.org/simons-tutorial-robust.html\">here</a>.</li>\n\n<li>Fall 2018: visiting scientist at <a href=\"https://simons.berkeley.edu/\">Simons Institute for the Theory of Computing</a>, \n<a href=\"https://simons.berkeley.edu/programs/datascience2018\">Foundations of Data Science Program</a>.</li>\n\n<li>In August 2018, I co-organized a week-long workshop on computational and statistical aspects of \n<a href=\"http://www.iliasdiakonikolas.org/tti-robust.html\">Robust High-Dimensional Statistics</a> sponsored by the \n<a href=\"http://www.ttic.edu/summer-workshop-2018/\">TTI-Chicago Summer Workshop Program</a>. Slides of all talks are available!\n</li>\n<li>Local arrangements co-chair for <a href=\"http://acm-stoc.org/stoc2018/\">STOC 2018</a> TheoryFest.</li>\n\n<li>In June 2017, I co-organized a workshop at <a href=\"http://acm-stoc.org/stoc2017/\">STOC 2017</a> on \n<a href=\"https://users.cs.duke.edu/~rongge/stoc2017ml/stoc2017ml.html\">New Challenges in Machine Learning - Robustness and Nonconvexity</a>.</li>\n\n<li>In February 2017, I organized a session at <a href=\"http://ita.ucsd.edu/workshop/17/?year=17\">ITA</a> on \n<a href=\"http://www.iliasdiakonikolas.org/ita-robust-estimation.html\">High-Dimensional Robust Estimation</a>.</li>\n\n<li> I wrote a book chapter on distribution learning algorithms for the  <a href=\"http://www.crcpress.com/product/isbn/9781482249071\">CRC Handbook of Big Data</a>.</li>\n\n<li>In Fall 2015, I co-organized the <a href=\"http://www.icms.org.uk/workshop.php?id=367\">Distributed Machine Learning and Optimization</a>\nscoping workshop at the <a href=\"https://turing.ac.uk/\">Alan Turing Institute</a>.</li> \n<li>I co-organized the <a href=\"http://www.iliasdiakonikolas.org/stoc-distribution-estimation.html\">Efficient Distribution Estimation</a> workshop at STOC 2014.</li>\n\n<li>During Summers 2015 and 2014, I was a visiting scientist at <a href=\"http://www.csail.mit.edu/\">CSAIL</a>, MIT, with the \n<a href=\"http://toc.csail.mit.edu/\">Theory of Computation</a> group. During Fall 2013, I was a visiting scientist in the <a href=\"http://simons.berkeley.edu/\">Simons Institute for the Theory of Computing</a> at UC Berkeley.</li>\n\n\n\n\t </ul>\n</font>    \n\n\n\n<h2>Recent Teaching</h2>\n<font size=\"4\"> \t\n\t<ul>\n\n\n<li> Spring 2020: <a href=\"http://www.iliasdiakonikolas.org/teaching/Spring20/CS639.html\">Introduction to Computational Learning Theory</a></li>\n\n\n\n<li> Fall 2019: <a href=\"http://www.iliasdiakonikolas.org/teaching/Fall19/CS880.html\">Advanced Learning Theory</a></li>\n\n\n<li> Spring 2018: <a href=\"http://www.iliasdiakonikolas.org/teaching/Spring18/CSCI599.html\">Machine Learning Theory</a> </li>\n\n\n\n<li> Fall 2017: <a href=\"http://www-bcf.usc.edu/~shaddin/cs699fa17/index.html\">Topics in Learning and Game Theory</a> </li>\n\n\n<li> Spring 2017: <a href=\"http://www.iliasdiakonikolas.org/teaching/Spring17/CSCI599.html\">Machine Learning Theory</a> </li>\n\n\n<li> Spring 2016: <a href=\"http://www.iliasdiakonikolas.org/teaching/Spring16/CSCI599.html\">Effective Algorithms in Machine Learning and Statistics</a> </li>\n\n\n</ul>\n</font> \n\n<font size=\"4\">\n\n\n<h2>Advising</h2>\n\n<font size=\"4\"> \t\n\t<ul>\n\n\n<li>PhD students: Nikos Zarifis (August 2019 - present), Anastasia Voloshinov (September 2017 - present), Chrystalla Pavlou (October 2015 - present), Vladimir Nikishkin (graduated 2016) </li>\n<li>Postdocs:  <a href=\"http://www.alistair-stewart.com\">Alistair Stewart</a> (September 2014 - January 2018), <a href=\"http://www.mit.edu/~tgoule/\">Themis Gouleakis</a> (August 2018 - May 2019), \n<a href=\"http://www.mit.edu/~vakilian/\">Ali Vakilian</a> (August 2019 - present)</li>\n<li>MSc students: Paul Carter (2014), Evgeniya Sotirova (2014), Michael Zhang (2015)</li>\n\n\n\t </ul>\n</font>    \n\n\n\n\n<br>\n\n\n\n\n\n<script type=\"text/javascript\">\nvar sc_project=9640586; \nvar sc_invisible=1; \nvar sc_security=\"8215de43\"; \nvar scJsHost = ((\"https:\" == document.location.protocol) ?\n\"https://secure.\" : \"http://www.\");\ndocument.write(\"<sc\"+\"ript type='text/javascript' src='\" +\nscJsHost+\n\"statcounter.com/counter/counter.js'></\"+\"script>\");\n</script><script type=\"text/javascript\" src=\"http://www.statcounter.com/counter/counter.js\"></script>\n<noscript><div class=\"statcounter\"><a title=\"web analytics\"\nhref=\"http://statcounter.com/\" target=\"_blank\"><img\nclass=\"statcounter\"\nsrc=\"http://c.statcounter.com/9640586/0/8215de43/1/\"\nalt=\"web analytics\"></a></div></noscript>\n\n\n\n\n\n\n</font></font></font></body></html>"