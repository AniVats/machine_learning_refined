{"email": ["CS@UCSB"], "image": ["https://cs.ucsb.edu/sites/cs.ucsb.edu/themes/cs_subtheme/css/images/linkedin-icon.png", "https://cs.ucsb.edu/sites/default/files/styles/portrait-full/public/faculty/images/screen_shot_2018-06-13_at_4.10.54_pm.png?itok=T16MhjRN&c=ce411a24d4272f06e697c25b801cbc83", "https://cs.ucsb.edu/sites/cs.ucsb.edu/files/images/events/ical.png", "https://cs.ucsb.edu/sites/cs.ucsb.edu/themes/cs_subtheme/css/images/ucsb-icon.png", "https://cs.ucsb.edu/sites/cs.ucsb.edu/themes/cs_subtheme/css/images/facebook-icon.png", "https://cs.ucsb.edu/sites/cs.ucsb.edu/themes/cs_subtheme/css/images/coe-icon.png", "https://cs.ucsb.edu/sites/cs.ucsb.edu/themes/cs_subtheme/css/images/twitter-icon.png", "https://cs.ucsb.edu/ /sites/cs.ucsb.edu/files/images/news/rss.png"], "research_blurb": ["Yu-Xiang Wang is an Assistant Professor of Computer Science at UCSB. Prior to joining UCSB, he was a scientist with Amazon Web Services\u2019s AI research lab in Palo Alto, CA from 2017 to 2018. Yu-Xiang received his PhD in Statistics and Machine Learning in 2017 from the world\u2019s first Machine Learning Department in the School of Computer Science of Carnegie Mellon University (CMU). Before that, he received his master\u2019s and bachelor\u2019s degrees in Electrical Engineering from National University of Singapore in 2011 and 2013 respectively. \u00a0Yu-Xiang\u2019s research interests revolve around the intersection of machine learning, statistics and optimization with special focus on statistical theory and methodology, differential privacy, large-scale machine learning, reinforcement learning and deep learning.\u00a0\nMy research interests lie in the broad area of statistical machine learning \u2014 a research field that addresses the statistical and computational properties of machine learning algorithms and their optimality guarantees. Specifically, my work focuses on developing provable and practical methods for various challenging \u00a0learning regimes (e.g., high dimensional, heterogeneous, privacy-constrained, sequential, parallel and distributed) and often involves exploiting hidden structures in data (generalized sparsity, union-of-subspace, graph or network structures), balancing various resources (model complexity, statistical power and privacy budgets) as well as developing scalable optimization tools (e.g., those tailored for deep learning).\n"]}