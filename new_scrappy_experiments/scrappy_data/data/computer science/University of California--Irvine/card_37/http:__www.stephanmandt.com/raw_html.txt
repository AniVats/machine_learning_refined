"<html><head>\n\n<link href=\"css/style.css\" rel=\"stylesheet\" type=\"text/css\">\n\n<meta name=\"description\" content=\"Machine Learning Professor at UC Irvine\">\n\n<meta name=\"keywords\" content=\"Stephan Mandt,homepage,Columbia,Princeton,Disney Research, machine learning, David Blei, Achim Rosch,variational inference, stochastic optimization, Bayesian inference\">\n\n<title>Stephan Mandt - Homepage</title>\n\n</head>\n\n<body>\n\n<div align=\"left\">\n<div style=\"padding:20px; width:1050px\">\n\n<table style=\"width: 50%; height: 237px;\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\">\n<tbody>\n<tr>\n<td style=\"vertical-align: top;\">\n<!-- <h1>Stephan Mandt</h1> -->\n<h2><img alt=\"\" src=\"img/stephan.jpeg\" style=\"height: 200px;\" align=\"top\"><br>\n</h2>\n</td>\n<td style=\"vertical-align: top;\">\n<br>\n<div align=\"left\">\nStephan Mandt<br>\nAssistant Professor<br>\nDepartment of Computer Science<br>\nDonald Bren Hall 4072<br>\nUniversity of California, Irvine<br>\nlastname<span id=\"email_at\">@</span>uci.edu<br>\n<br>\nResearch Interests: Machine Learning, <br>\nDeep Learning, Probabilistic Models\n<br>\n<br>\n\n</div>\n</td>\n</tr>\n</tbody>\n</table>\n\n<div align=\"left\">\n<p align=\"center\"> [\n<a href=\"#Bio\">Bio</a>\n |\n<a href=\"#News\">News</a>\n |\n<a href=\"#Teaching\">Teaching</a>\n |\n<a href=\"#Group\">Group</a>\n |\n<a href=\"#Research\">Research</a>\n |\n<a href=\"#Publications\">Publications</a>\n |\n<a href=\"#Talks\">Talks</a>\n |\n<a href=\"CV/CV_Mandt_1.pdf\">CV</a>\n |\n<a href=\"https://scholar.google.com/citations?user=HOrGe7wAAAAJ&amp;hl=en\">Google Scholar</a>\n]</p>\n\n<a name=\"Bio\"></a>\n<h2>Short Bio</h2>\n<p align=\"justify\">\nStephan Mandt is an Assistant Professor of Computer Science at the <a href=\"https://www.ics.uci.edu/\">  University of California, Irvine</a>. \nFrom 2016 until  2018, he was a Senior Researcher and head of the statistical machine learning group at <a href=\"https://disneyresearch.com/\">\nDisney Research</a>, first in Pittsburgh on CMU campus and later in Los Angeles. He held previous positions as a postdoc with\n<a href=\"http://www.cs.columbia.edu/~blei/\"> David Blei</a> at <a href=\"http://datascience.columbia.edu/\">Columbia University</a>\n and as a PCCM Postdoctoral Fellow at <a href=\"http://pccm.princeton.edu/education/postdoctoral-research/\"> Princeton University</a>. \nStephan holds a PhD in <a href=\"http://www.thp.uni-koeln.de/\"> Theoretical Physics</a> from the University of Cologne.\nHe held fellowships by the <a href=\"http://en.wikipedia.org/wiki/Studienstiftung\"> German National Merit Foundation</a>\nand the Kavli Foundation. Stephan has been active as an Area Chair for NeurIPS and ICML and held a visiting researcher\nposition at Google Brain. His research is currently supported by NSF, DARPA, and Qualcomm Research.\n</p>\n\n<br>\n\n<b> Prospective PhD students:</b> I will continue accepting students in 2020. Applications are handled on the department level and are submitted <a href=\"https://grad.uci.edu/academics/degree-programs/phd/ComputerSciencePHD.php\">here</a>. If you decide to apply, you can list my name in your statement of purpose.\n\n\n\n<a name=\"News\"></a>\n<h2>News</h2>\n\n<ul>\n<li> Our paper on extreme classification using a new adversarial softmax approximation got accepted at ICLR 2020. \n</li><li> I'm excited to be a Co-PI on the Darpa SAIL-ON project on novelty detection and characterization.\n</li><li> Support from Qualcomm Research is gratefully acknowledged.\n</li><li> Accepted papers at NeurIPS 2019, EMNLP 2019, UAI 2019, AAAI 2019, and ICRA 2019. \n</li><li> I will serve as an Area Chair for ICML 2020 and currently serve as a metareviewer for AAAI 2020.\n</li><li> I gave an invited talk at the National Academy of Science's <a href=\"http://www.nasonline.org/programs/kavli-frontiers-of-science/\">Kavli Frontiers of Science Symposium</a> about deep generative models (<a href=\"https://vimeo.com/321604747\">see video</a>). \n</li><li> Served as an Area Chair for ICML 2019 and NeurIPS 2019.\n</li><li> I was invited to present at the workshop <a href=\"https://www.kitp.ucsb.edu/activities/machine-c19\">At the Crossroad of Physics and Machine Learning</a> in Santa Barbara.\n</li><li> Our workshop series on Approximate Bayesian Inference was offered as a <a href=\"http://www.approximateinference.org\">Symposium</a> this year, co-located with NeurIPS 2018.\n</li><li> I joined the School of Information and Computer Sciences at the University of California, Irvine.\n</li><li> Four accepted papers at ICML 2018.\n</li><li> Accepted papers at AISTATS 2018, NIPS 2017, UAI 2017, ICML 2017 and CVPR 2017.\n</li><li> Two journal papers accepted in 2017 (MLJ and JMLR).\n</li><li> Accepted papers at AISTATS 2016, ICML 2016, and NIPS 2016.\n</li><li> Co-organizer, NIPS <a href=\"http://www.approximateinference.org\"> Workshop on Advances on Approximate Bayesian Inference</a> (2017, 2016, 2015)\n</li></ul>\n\n\n<a name=\"Teaching\"></a>\n<h2>Teaching</h2>\n  <li>\n  Winter 2020: <a href=\"courses/ML-and-DM.html\">Machine Learning and Data Mining</a> (CS 178)<br>\n  </li><li> \n  Fall 2019: <a href=\"courses/intro-to-ML.html\">Introduction to Machine Learning</a> (CS 273A)<br>\n  </li><li>\n  Spring 2019: <a href=\"courses/deep-generative-models.html\">Deep Generative Models</a> (CS 295)<br>\n\n\n<a name=\"Group\"></a>\n<h2>Group Members</h2>\nCurrent:\n</li><li> <a href=\"https://robamler.github.io/\">Robert Bamler</a>, postdoctoral scholar\n</li><li> <a href=\"https://scholar.google.de/citations?user=z7BnHqoAAAAJ&amp;hl=de\"> Fabian Jirasek</a>, postdoctoral scholar\n</li><li> <a href=\"https://aodongli.github.io/\">Aodong Li</a>, PhD student\n</li><li> <a href=\"https://yiboyang.com/about/\">Yibo Yang</a>, PhD student\n</li><li> Ruihan Yang, PhD student <br><br>\n\nFormer:\n</li><li> <a href=\"https://cheng-zhang.org/\">Cheng Zhang</a>, now at Researcher at Microsoft Research\n</li><li> <a href=\"https://amor.cms.hu-berlin.de/~wenzelfl/\"> Florian Wenzel</a>, now Researcher at Google Brain\n</li><li> <a href=\"https://la.disneyresearch.com/people/salvator-lombardo/\">Salvator Lombardo</a>, now Associate Research Scientist at Disney Research\n\n\n\n\n<a name=\"Research\"></a>\n<h2>Research Interests</h2>\n\n<!--\nKeywords: variational inference; stochastic optimization; word embeddings; deep generative models\n-->\n<p align=\"justify\">\n<!--\nThe field of probabilistic modeling has seen profound changes in the past few years, driven by advances in stochastic optimization, automatic differentiation, easy-to-use neural network packages, and the availability of large data sets. Building on these advances, modern variational inference and related training paradigms enabled a rich class of generative models that had been computationally intractable in the past. In particular, Bayesian deep learning and representation learning approaches have experienced a revival, which, in turn, had crucial implications for modern computer vision (GANs and VAEs) and natural language processing (word and paragraph embeddings). My research activities have driven these advances. My vision is to build algorithms and new expressive models that integrate the benefits of modern representation learning with interpretable probabilistic priors, and to apply these models in the domains of natural language processing, personalization and recommendation systems, computer vision, the sciences, and media analytics. \n-->\nMy primary goal is to develop a new generation of machine learning models by drawing on deep learning, probabilistic graphical models, and approximate Bayesian inference. My research, thus, tries to synthesize representation learning and probabilistic modeling. This results in new flexible and oftentimes interpretable models for unsupervised or semi-supervised learning on large data. <br><br>\n\nMy group has developed several such models for various applications, in particular for sequential data. Dynamic Word Embeddings (ICML'17)\ncombine word embeddings with probabilistic Kalman filters, allowing us to accurately measure how words change their meanings over hundreds\nof years while keeping track of uncertainty due to data sparsity. Factorized Variational Autoencoders (CVPR'17) helped us discover latent\nfactors in audience face reactions to movie screenings. Disentangled Sequential Autoencoders (ICML'18) enabled us to generate artificial\nvideos while gaining partial control over content and dynamics, and give rise to novel video compression techniques (NeurIPS'19).\n<br><br> \n\nMy second goal is to design new learning and inference algorithms which are scalable and generic. I frequently use a methodology termed\nvariational inference, an approximation scheme that allows generative models to be trained on massive scales. Relevant papers in this line\nof research are Stochastic Gradient Descent as Approximate Bayesian Inference (JMLR'17), Variational Tempering (AISTATS'16), Quasi-Monte Carlo\nVariational Inference (ICML'18), Iterative Amortized Inference (ICML'18), and Perturbative Black Box Variational Inference (NIPS'17). We also\nrecently published a review article on this topic.\n<br><br>\n\nI am interested in a wide range of applications, including computer vision, natural language processing, data compression, media analytics, and applications in the sciences.\n\n\n\n\n\n\n\n\n<a name=\"Publications\"></a>\n</p><h2>Representative Publications</h2>\n\n<ul class=\"bib2xhtml\">\n\n  <li>\n  <b> Stochastic Gradient Descent as Approximate Bayesian Inference</b><br>\n  S. Mandt, M. Hoffman, and D. Blei <br>\n  <i> Journal of Machine Learning Research 18, 1-35, 2017.</i>  &nbsp; [<a href=\"http://www.jmlr.org/papers/volume18/17-214/17-214.pdf\">PDF</a>]\n\n  </li><li>\n    <b>Dynamic Word Embeddings</b><br>\n    R. Bamler and S. Mandt <br>\n    <i> International Conference on Machine Learning 2017.</i>  &nbsp; [<a href=\"http://arxiv.org/pdf/1702.08359\">PDF</a>]\n\n</li><li>\n  <b>Deep Generative Video Compression</b><br>\n  J. Han, S. Lombardo, C. Schroers, and S. Mandt <br>\n  <i>Neural Information Processing Systems 2019.</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1810.02845.pdf\">PDF</a>] [<a href=\"files/Han_etal_NeurIPS_2019.pdf\">poster</a>]\n\n</li><li>\n  <b>Disentangled Sequential Autoencoder</b><br>\n  Y. Li and S. Mandt <br>\n    <i>International Conference on Machine Learning 2018.</i> &nbsp; [<a href=\"http://proceedings.mlr.press/v80/yingzhen18a/yingzhen18a.pdf\">PDF</a>]\n\n</li><li>\n  <b>Improving Optimization for Models with Continuous Symmetry Breaking</b><br>\n  R. Bamler and S. Mandt <br>\n  <i>International Conference on Machine Learning 2018.</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1803.03234.pdf\">PDF</a>]\n\n</li><li>\n  <b>Advances in Variational Inference </b><br>\n  C. Zhang, J. B\u00fctepage, H. Kjellstr\u00f6m, and S. Mandt <br>\n    <i>IEEE Transactions on Pattern Analysis and Machine Intelligence.</i> &nbsp; [<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8588399\">PDF</a>][<a href=\"https://arxiv.org/pdf/1711.05597.pdf\">arXiv</a>]\n\n\n</li></ul>\n\n<h2>All Publications</h2>\n<ul class=\"bib2xhtml\">\n\n\n\n\n\n<h3>Preprints</h3>\n<li>\n   <b>How Good is the Bayes Posterior in Deep Neural Networks Really?</b><br>\n  F. Wenzel, K. Roth, B. Veeling, J. Swiatkowski, L. Tran, S. Mandt, J. Snoek, T. Salimans, R. Jenatton, and S. Nowozin<br>\n  <i>arXiv preprint (2020)</i>  &nbsp; [<a href=\"https://arxiv.org/pdf/2002.02405.pdf\">PDF</a>]\n</li><li>\n   <b>The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks</b><br>\n  J. Swiatkowski, K. Roth, B. Veeling, L. Tran, J. Dillon, S. Mandt, J. Snoek, T. Salimans, R. Jenatton, and S. Nowozin<br>\n  <i>arXiv preprint (2020)</i>  &nbsp; [<a href=\"https://arxiv.org/pdf/2002.02655.pdf\">PDF</a>]\n</li><li>\n   <b>Hydra: Preserving Ensemble Diversity for Model Distillation</b><br>\n  L. Tran, B. Veeling, K. Roth, J. Swiatkowski, J. Dillon, J. Snoek, S. Mandt, T. Salimans, S. Nowozin, and R. Jenatton<br>\n  <i>arXiv preprint (2020)</i>  &nbsp; [<a href=\"https://arxiv.org/pdf/2001.04694.pdf\">PDF</a>]\n\n\n\n\n<h3>2020</h3>\n\n</li><li>\n  <b>Machine Learning in Thermodynamics: Prediction of Activity Coefficients by Matrix Completion</b><br>\n  F. Jirasek, R. Alves, J. Damay, R. Vandermeulen, R. Bamler, M. Bortz, S. Mandt, M. Kloft, and H. Hasse <br>\n  <i>The Journal of Physical Chemistry Letters, 11, 2020.</i> &nbsp; [<a href=\"https://pubs.acs.org/doi/10.1021/acs.jpclett.9b03657\">article</a>][<a href=\"papers/jirasek-ml-in-td-2020.pdf\">free PDF</a>]\n\n</li><li>\n  <b>GP-VAE: Deep Probabilistic Time Series Imputation</b><br>\n  V. Fortuin, D. Baranchuk, G. R\u00e4tsch, and S. Mandt <br>\n  <i>Artificial Intelligence and Statistics (AISTATS 2020).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1907.04155.pdf\">PDF</a>]\n\n  </li><li>\n  <b>Extreme Classification via Adversarial Softmax Approximation</b><br>\n  R. Bamler and S. Mandt <br>\n  <i> International Conference on Learning Representations (ICLR 2020).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/2002.06298.pdf\">PDF</a>]\n\n\n<h3>2019</h3>\n\n  </li><li>\n  <b>Tightening Bounds for Variational Inference by Revisiting Perturbation Theory</b><br>\n  R. Bamler, C. Zhang, M. Opper, and S. Mandt <br>\n  <i> Journal of Statistical Mechanics (2019), 124004.</i>  &nbsp; [<a href=\"papers/J_Stat_Mech_2019_124004.pdf\">PDF</a>]\n\n</li><li>\n  <b>Deep Generative Video Compression</b><br>\n  J. Han, S. Lombardo, C. Schroers, and S. Mandt <br>\n  <i>Neural Information Processing Systems (NeurIPS 2019).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1810.02845.pdf\">PDF</a>] [<a href=\"files/Han_etal_NeurIPS_2019.pdf\">poster</a>]\n\n</li><li>\n  <b>Autoregressive Text Generation Beyond Feedback Loops</b><br>\n   F. Schmidt, S. Mandt, and T. Hofmann<br>\n    <i>Conference on Empirical Methods in Natural Language Processing (EMNLP 2019).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1908.11658.pdf\">PDF</a>]\n\n</li><li>\n  <b>Augmenting and Tuning Knowledge Graph Embeddings</b><br>\n   R. Bamler, F. Salehi, and S. Mandt<br>\n    <i>Conference on Uncertainty in Artificial Intelligence (UAI 2019).</i> &nbsp; [<a href=\"papers/UAI_2019.pdf\">PDF</a>]\n\n</li><li>\n  <b>Advances in Variational Inference </b><br>\n  C. Zhang, J. B\u00fctepage, H. Kjellstr\u00f6m, and S. Mandt <br>\n    <i>IEEE Transactions on Pattern Analysis and Machine Intelligence.</i> &nbsp; [<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8588399\">PDF</a>][<a href=\"https://arxiv.org/pdf/1711.05597.pdf\">arXiv</a>]\n</li><li>\n  <b>Active Mini-Batch Sampling using Repulsive Point Processes</b><br>\n  C. Zhang, C. \u00d6ztireli, S. Mandt, and G. Salvi <br>\n    <i>Conference on Artificial Intelligence (AAAI 2019).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1804.02772.pdf\">PDF</a>]\n\n</li><li>\n  <b>Mobile Robotic Painting of Texture</b><br>\n  M. Helou, S. Mandt, A. Krause, and P. Beardsley <br>\n    <i>International Conference on Robotics and Automation (ICRA 2019).</i> &nbsp; [<a href=\"papers/ICRA_2019_learning_to_paint.pdf\">PDF</a>]\n\n</li><li>\n  <b>A Quantum Field Theory of Representation Learning</b><br>\n  R. Bamler and S. Mandt <br>\n    <i>ICML Workshop on Physics for Deep Learning (2019).</i> &nbsp; [<a href=\"papers/Qft-of-representation-learning.pdf\">PDF</a>]\n\n\n<h3>2018</h3>\n\n</li><li>\n  <b>Disentangled Sequential Autoencoder</b><br>\n  Y. Li and S. Mandt <br>\n    <i>International Conference on Machine Learning (ICML 2018).</i> &nbsp; [<a href=\"http://proceedings.mlr.press/v80/yingzhen18a/yingzhen18a.pdf\">PDF</a>]\n\n</li><li>\n  <b>Iterative Amortized Inference</b><br>\n  J. Marino, Y. Yue, and S. Mandt <br>\n  <i>International Conference on Machine Learning (ICML 2018).</i> &nbsp;[<a href=\"http://proceedings.mlr.press/v80/marino18a/marino18a.pdf\">PDF</a>]\n\n</li><li>\n  <b>Quasi Monte Carlo Variational Inference</b><br>\n  A. Buchholz, F. Wenzel, and S. Mandt<br>\n  <i>International Conference on Machine Learning (ICML 2018).</i> &nbsp;[<a href=\"http://proceedings.mlr.press/v80/buchholz18a/buchholz18a.pdf\">PDF</a>]\n\n</li><li>\n  <b>Improving Optimization for Models With Continuous Symmetry Breaking</b><br>\n  R. Bamler and S. Mandt <br>\n    <i>International Conference on Machine Learning (ICML 2018), long talk.</i> &nbsp; [<a href=\"http://proceedings.mlr.press/v80/bamler18a/bamler18a.pdf\">PDF</a>]\n\n</li><li>\n  <b>Continuous Word Embedding Fusion via Spectral Decomposition</b><br>\n  T. Fu, C. Zhang, and S. Mandt <br>\n    <i>The SIGNLL Conference on Natural Language Learning (CoNLL 2018).</i> &nbsp; [<a href=\"papers/CoNLL_2018.pdf\">PDF</a>]\n\n</li><li>\n  <b>Scalable Generalized Dynamic Topic Models</b><br>\n  P. J\u00e4hnichen, F. Wenzel, M. Kloft, and S. Mandt <br>\n    <i>Artificial Intelligence and Statistics (AISTATS 2018).</i> &nbsp; [<a href=\"https://arxiv.org/pdf/1803.07868.pdf\">PDF</a>]\n\n</li><li>\n  <b>Image Anomaly Detection with Generative Adversarial Networks</b><br>\n  L. Deecke, R. Vandermeulen, L. Ruff, S. Mandt, and M. Kloft <br>\n    <i>European Conference on Machine Learning (ECML PKDD 2018).</i> &nbsp; [<a href=\"http://www.ecmlpkdd2018.org/wp-content/uploads/2018/09/223.pdf\">PDF</a>]\n\n</li><li>\n  <b>Learning to Infer</b><br>\n  J. Marino, Y. Yue, and S. Mandt <br>\n    <i>International Conference on Learning Representations (Workshop Track).</i> &nbsp; [<a href=\"papers/ICLR_WS_2018.pdf\">PDF</a>]\n\n</li><li>\n  <b>Quasi Monte Carlo Flows</b><br>\n  F. Wenzel, A. Buchholz, and S. Mandt <br>\n    <i>NeurIPS Bayesian Deep Learning Workshop.</i>&nbsp; [<a href=\"http://bayesiandeeplearning.org/2018/papers/73.pdf\">PDF</a>]\n\n</li><li>\n  <b>Video Compression through Deep Bayesian Learning</b><br>\n  S. Lombardo, J. Han, C. Schroers, and S. Mandt <br>\n    <i>NeurIPS Bayesian Deep Learning Workshop.</i>&nbsp; [<a href=\"http://bayesiandeeplearning.org/2018/papers/57.pdf\">PDF</a>]\n\n\n<h3>2017</h3>\n\n  </li><li>\n  <b> Stochastic Gradient Descent as Approximate Bayesian Inference</b><br>\n  S. Mandt, M. Hoffman, and D. Blei <br>\n  <i> Journal of Machine Learning Research, vol 18(134):1-35, 2017.</i>  &nbsp; [<a href=\"http://www.jmlr.org/papers/volume18/17-214/17-214.pdf\">PDF</a>]\n  [<a href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/optimizer/variational_sgd.py\">code</a>]\n\n  </li><li>\n  <b> Perturbative Black Box Variational Inference </b><br>\n  R. Bamler, C. Zhang, M. Opper, and S. Mandt <br>\n  <i> Neural Information Processing Systems (NIPS 2017)</i>. &nbsp; [<a href=\"https://arxiv.org/pdf/1709.07433.pdf\">PDF</a>][<a href=\"files/Poster_NIPS2017.pdf\">poster</a>]\n\n  </li><li> \n    <b>Dynamic Word Embeddings</b><br>\n    R. Bamler and S. Mandt <br>\n    <i> International Conference on Machine Learning (ICML 2017).</i>  &nbsp; [<a href=\"http://arxiv.org/pdf/1702.08359\">PDF</a>][<a href=\"files/Poster_ICML2017.jpg\">poster</a>]\n\n </li><li>\n  <b> Determinantal Point Processes for Mini-batch Diversification</b><br>\n  C. Zhang, H. Kjellstr\u00f6m, and S. Mandt<br>\n  <i> Uncertainty in Artificial Intelligence (UAI 2017)</i> (plenary talk).  &nbsp; [<a href=\"http://arxiv.org/pdf/1705.00607.pdf\">PDF</a>]\n\n  </li><li>\n   <b> Factorized Variational Autoencoders for Modeling Audience Reactions to Movies </b> <br>\n   Z. Deng, R. Navarathna, P. Carr, S. Mandt, Y. Yue, I. Matthews, and G. Mori <br>\n   <i> Computer Vision and Pattern Recognition (CVPR 2017).</i> &nbsp; [<a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Deng_Factorized_Variational_Autoencoders_CVPR_2017_paper.pdf\">PDF</a>]\n  </li><li>\n    <b>Iterative Inference Models</b><br>\n    J. Marino, Y. Yue, and S. Mandt <br>\n    <i>NIPS 2017 Workshop on Bayesian Deep Learning.</i>  &nbsp; [<a href=\"http://bayesiandeeplearning.org/2017/papers/9.pdf\">PDF</a>]\n\n  </li><li>\n    <b>Bayesian Paragraph Vectors</b><br>\n    G. Ji, R. Bamler, E. Sudderth, and S. Mandt <br>\n    <i>NIPS 2017 Workshop on Approximate Bayesian Inference.</i>  &nbsp; [<a href=\"https://arxiv.org/pdf/1711.03946.pdf\">PDF</a>]\n\n  </li><li>\n    <b> Structured Black Box Variational Inference for Latent Time Series Models</b><br>\n    R. Bamler and S. Mandt <br>\n        <i> ICML 2017 Time Series Workshop</i> (oral). &nbsp; [<a href=\"http://arxiv.org/pdf/1707.01069.pdf\">PDF</a>]\n  </li><li>\n    <b>Diversified Mini-Batch Sampling using Repulsive Point Processes</b><br>\n    C. Zhang, C. \u00d6ztireli, and S. Mandt <br>\n        <i> NIPS 2017 Workshop on Advances in Approximate Bayesian Inference</i>. &nbsp; [<a href=\"http://approximateinference.org/2017/accepted/ZhangEtAl2017a.pdf\">PDF</a>]\n   \n     \n<h3>2016</h3>\n  </li><li>\n  <b>Exponential Family Embeddings</b> <br>\n  M. Rudolph, F.J.R. Ruiz, S. Mandt, and D. Blei <br>\n  <i> Neural Information Processing Systems (NIPS 2016).</i> [<a href=\"https://papers.nips.cc/paper/6571-exponential-family-embeddings.pdf\">PDF</a>]\n\n  </li><li>\n  <b>Balanced Population Stochastic Variational Inference</b> <br>\n  C. Zhang, S. Mandt, and H. Kjellstr\u00f6m <br>\n  <i> NIPS 2016 Workshop on Advances in Approximate Bayesian Inference.</i> [<a href=\"http://www.approximateinference.org/accepted/ZhangEtAl2016.pdf\">PDF</a>]\n\n  </li><li>\n  <b>Huber-Norm Regularization for Linear Prediction Models</b> <br>\n  O. Zadorozhnyi, G. Benecke, S. Mandt, T. Scheffer, M. Kloft <br>\n  <i>European Conference on Machine Learning (ECML 2016).</i> [<a href=\"papers/ECML_2016.pdf\">PDF</a>]\n\n\n  </li><li>\n  <b>A Variational Analysis of Stochastic Gradient Algorithms</b><br>\n  S. Mandt, M. Hoffman, and D. Blei <br>\n  <i> International Conference on Machine Learning (ICML 2016) </i>  &nbsp; [<a href=\"http://jmlr.org/proceedings/papers/v48/mandt16.pdf\">PDF</a>][<a href=\"files/Poster_ICML2016.pdf\">poster</a>][<a href=\"http://techtalks.tv/talks/a-variational-analysis-of-stochastic-gradient-algorithms/62505/\">video</a>] <br>\n[<a href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/optimizer/variational_sgd.py\">code</a>]\n  </li><li>\n  <b> Variational Tempering</b> <br>\n  S. Mandt, J. McInerney, F. Abrol, R. Ranganath, and D. Blei <br>\n <i> Artificial Intelligence and Statistics (AISTATS 2016).</i>  &nbsp; [<a href=\"http://jmlr.org/proceedings/papers/v51/mandt16.pdf\">PDF</a>]\n\n </li><li>\n  <b> Separating Sparse Signals from Correlated Noise in Binary Classification</b><br>\n  S. Mandt, F. Wenzel, S. Nakajima, C. Lippert, and M. Kloft.<br>\n  <i>UAI 2016 Workshop on Causation: Foundation to Application.</i> (oral) &nbsp; [<a href=\"papers/UAI_causality_2016.pdf\">PDF</a>]\n\n\n<h3>2015 </h3>\n\n  </li><li>\n  <b> Sparse Probit Linear Mixed Model</b> <br>\n   S. Mandt, F. Wenzel, S. Nakajima, J. P. Cunningham, C. Lippert, and M. Kloft<br>\n<a href=\"http://rdcu.be/ubi4\"> Machine Learning, 106(9), 1621-1642. </a> &nbsp; [<a href=\"http://arxiv.org/abs/1507.04777\">PDF</a>]\n\n  </li><li> \n  <b>Continuous-Time Limit of Stochastic Gradient Descent Revisited</b> <br>\n  S. Mandt, M. Hoffman, and D. Blei <br>\n   <i> NIPS Workshop on Optimization for Machine Learning (OPT 2015)</i> &nbsp; [<a href=\"http://opt-ml.org/papers/OPT2015_paper_8.pdf\">PDF</a>]\n \n\n  </li><li> \n  <b> Finding Sparse Features in Strongly Confounded Medical Binary Data</b> <br>\n  S. Mandt, F. Wenzel, S. Nakajima, J. P. Cunningham, C. Lippert, and M. Kloft <br>\n <i> NIPS Workshop on Machine Learning in Healthcare (2015). </i> (oral) &nbsp; [<a href=\"papers/Mandtetal_NIPS_HEALTH2015.pdf\">PDF</a>]\n\n  </li><li>\n  <b> Stochastic Differential Equations for Quantum Dynamics of Spin-Boson Networks</b> <br>\n   S. Mandt, D. Sadri, A. Houck, and H. Tureci<br>\n  <a href=\"http://iopscience.iop.org/1367-2630/17/5/053018\">New Journal of Physics 17 (2015) 053018. </a>\n  &nbsp; [<a href=\"http://iopscience.iop.org/1367-2630/17/5/053018/pdf/1367-2630_17_5_053018.pdf\">PDF</a>]\n\n<h3> 2014 </h3>\n\n  </li><li>\n  <b> Smoothed Gradients for Stochastic Variational Inference</b><br>\n   S. Mandt and D. Blei<br>\n  <i> Neural Information Processing Systems (NIPS 2014)</i>\n  &nbsp; [<a href=\"http://papers.nips.cc/paper/5557-smoothed-gradients-for-stochastic-variational-inference.pdf\">PDF</a>]\n\n  </li><li> \n  <b> Probit Regression with Correlated Label Noise: An EM-EP approach</b><br>\n  S. Mandt, F. Wenzel, J. Cunningham, and M. Kloft<br>\n  <i> NIPS Workshop on Advances in Variational Inference (2014)</i> &nbsp; [<a href=\"papers/NIPS_VI_2014.pdf\">PDF</a>]\n\n  </li><li>\n  <b> Comment on \"Consistent thermostatistics forbids negative absolute temperatures\"</b> <br>\n  U. Schneider, S. Mandt, A. Rapp, S. Braun, H. Weimer, I. Bloch, and A. Rosch <br>\n  <i> Arxiv (2014).</i> &nbsp; [<a href=\"http://arxiv.org/pdf/1407.4127\">arXiv</a>]\n\n  </li><li> \n  <b> Damping of Bloch oscillations: Variational solution of the Boltzmann equation beyond linear response</b><br>\n S. Mandt <br>\n  <a href=\"https://journals.aps.org/pra/abstract/10.1103/PhysRevA.90.053624\">Physical Review A 90, 053624 (2014).</a> &nbsp; [<a href=\"http://arxiv.org/pdf/1409.0560\">arXiv</a>]\n\n <h3> Before 2014 </h3>\n\n  </li><li>\n  <b> Relaxation towards negative temperatures in bosonic systems: Generalized Gibbs ensembles and beyond integrability</b><br>\n S. Mandt, A. Feiguin, S. Manmana<br>\n  <a href=\"http://journals.aps.org/pra/abstract/10.1103/PhysRevA.88.043643\">Physical Review A 88, 043643 (2013).</a> &nbsp; [<a href=\"http://arxiv.org/pdf/1307.7188\">arXiv</a>]\n\n </li><li> \n  <b>Ultrakalt und doch hei\u00dfer als unendlich hei\u00df. Erstmals gelang es, ein Quantengas bei negativen absoluten Temperaturen herzustellen</b><br>\n  S. Mandt<br>\n  <a> Popular article on negative temperatures in the monthly proceedings of the German Physical Society. </a><br>\n  <a href=\"http://www.pro-physik.de/details/physikjournalIssue/4375371/PJ_03_2013.html\">Physik Journal 12, March edition (2013)</a> &nbsp; [<a href=\"papers/PhysikjournalMandt2013.pdf\">PDF</a>]\n\n\n  </li><li> \n  <b>Transport and Non-Equilibrium Dynamics in Optical Lattices. From Expanding Atomic Clouds to Negative Absolute Temperatures </b><br>\n  S. Mandt<br>\n  <i> PhD thesis, University of Cologne (2012)</i> &nbsp; [<a href=\"http://kups.ub.uni-koeln.de/4735/\">PDF</a>]\n\n  </li><li>\n  <b> Fermionic transport in a homogeneous Hubbard model: Out-of-equilibrium dynamics with ultracold atoms</b><br>\n  U. Schneider, L. Hackermueller, J.P. Ronzheimer, S. Will, S. Braun, T. Best, I. Bloch, E. Demler, S. Mandt, D. Rasch, A. Rosch<br>\n  <a href=\"http://www.nature.com/nphys/journal/v8/n3/full/nphys2205.html\">Nature Physics 8, 213-218 (2012).</a> &nbsp; [<a href=\"http://arxiv.org/pdf/1005.3545\">arXiv</a>]<br>\n  Press: <a href=\"http://scitechdaily.com/dynamics-of-a-system-of-ultracold-potassium-atoms/\">SciTechDaily</a>, <a href=\"http://www.pro-physik.de/details/news/1500951/Anziehende_Abstossung__oder_umgekehrt.html\">Pro-Physik (in German)</a>\n\n  </li><li> \n  <b> Interacting Fermionic Atoms in Optical Lattices Diffuse Symmetrically Upwards and Downwards in a Gravitational Potential </b><br>\n  S. Mandt, A. Rapp, A. Rosch<br>\n  <a href=\"http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.106.250602\">Physical Review Letters 106, 250602 (2011).</a> &nbsp; [<a href=\"http://arxiv.org/pdf/1101.4508\">arXiv</a>]<br>\n   Press: <a href=\"http://www.nature.com/news/quantum-gas-goes-below-absolute-zero-1.12146#/b4\"> Nature </a>\n\n  </li><li> \n  <b> Equilibration rates and negative absolute temperatures for ultracold atoms in optical lattices</b><br>\n  A. Rapp, S. Mandt, A. Rosch<br>\n  <a href=\"http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.105.220405\">Physical Review Letters 105, 220405 (2010).</a> &nbsp; [<a href=\"http://arxiv.org/pdf/1008.0468\">arXiv</a>]<br>\n  Press: <a href=\"http://www.nature.com/news/quantum-gas-goes-below-absolute-zero-1.12146#/b4\"> Nature </a>, <a href=\"https://www.sciencenews.org/article/negative-temperature-infinitely-hot\"> New Scientist</a>, <a href=\"https://www.sciencenews.org/article/negative-temperature-infinitely-hot\">Science News</a> <br>\n  Experimental realization of T&lt;0 based on our theory: <a href=\"http://science.sciencemag.org/content/339/6115/52.short\">Braun et. al., Science 2013</a>\n\n\n  </li><li> \n  <b> Zooming in on local level statistics by supersymmetric extension of free probability </b><br>\n  S. Mandt, M.R. Zirnbauer <br>\n  <a href=\"http://iopscience.iop.org/1751-8121/43/2/025201\"> J. Phys. A 43 (2010) 025201.</a> &nbsp; [<a href=\"http://arxiv.org/pdf/0908.1877\">arXiv</a>]\n\n  </li><li> \n  <b>Symmetric Spaces Toolkit</b><br>\n  H. Sebert and  S. Mandt<br>\n  <i>Lecture notes, SFB/TR 12, Langeoog (2007)</i> &nbsp; [<a href=\"papers/SebertMandt2007.pdf\">PDF</a>]\n\n</li></ul>\n\n\n\n<a name=\"Talks\"></a>\n<h2>Invited Talks</h2>\n<ul>\n\n  <li><b>CS Colloquium, EPFL Lausanne</b><br>\n    Lausanne, Switzerland. October 2017.\n  </li><li><b>ML Lunch Seminar, Carnegie Mellon University</b><br>\n    Pittsburgh, PA. September 2017.\n  </li><li><b>Disney Data Analytics Conference</b><br>\n    Orlando, FL. August 2017.\n  </li><li><b>CS Colloquium, University of Southern California</b><br>\n    Los Angeles, CA. April 2017.\n  </li><li><b>CS Colloquium, ETH Z\u00fcrich </b><br>\nZ\u00fcrich, Switzerland. March 2017.\n  </li><li><b>\n      ML and Friends Seminar, UMass Amherst </b><br>\nAmherst, MA. February 2017.\n  </li><li><b> AI Seminar, Carnegie Mellon University</b><br>\n    Pittsburgh, PA. September 2016.\n  </li><li> <b>California Institute of Technology </b><br>\n    Pasadena, CA. August 2016.\n  </li><li> <b> Data Science Colloquium, Rutgers University </b> <br>\n    Newark, NJ. April 2016.\n  </li><li> <b> Google Research </b> <br>\n    Mountain View, CA. April 2016.\n  </li><li> <b> Microsoft</b> <br>\n    Sunnyvale, CA. March 2016.\n  </li><li> <b> CS Colloquium, University of Rhode Island</b> <br>\n    Kingston, RI. March 2016.\n  </li><li> <b> Disney Research</b> <br>\n    Pittsburgh, Pennsylvania. March 2016.\n  </li><li> <b> CS Colloquium, University of Colorado </b> <br> \n       Boulder, Colorano. January 2016.\n  </li><li> <b> National Renewable Energy Laboratory </b><br> \n       Golden, Colorado. January 2016.\n  </li><li> <b>Adobe Research</b><br>\n       San Francisco, CA. June 2015.\n  </li><li> <b>Human Longevity Inc.</b><br> \n       Mountain View, CA. June 2015.\n  </li><li> <b>Dagstuhl Seminar</b><br>\n       Machine Learning with Interdependent and Non-identically Distributed Data.<br>\n       Dagstuhl, Germany. April 2015. \n  </li><li> <b>Humboldt University of Berlin</b><br>\n       Machine Learning Seminar, Berlin, Germany. February 2015.\n  </li><li> <b>Technical University of Berlin</b><br>\n       Machine Learning Seminar, Berlin, Germany. February 2015.\n  </li><li> <b>D-Wave Systems Inc.</b><br> \n       Burnaby, Canada. January 2015.\n  </li><li> <b>University of British Columbia</b><br>\n       Machine Learning Seminar, Vancouver, Canada. January 2015.\n  </li><li> <b>IBM Research</b><br>\n       Physical Sciences Seminar, Yorktown Heights, USA. October 2014.\n  </li><li> <b>Emergent Phenomena in the Dynamics of Quantum Matter</b><br>\n       (QUANTMAT 2014), New York, USA. April 2014.\n  </li><li> <b>University of Otago</b><br>\n       Theoretical Physics Seminar, Dunedin, New Zealand. February 2013.\n  </li><li> <b>Princeton Center for Theoretical Science</b><br>\n       Princeton University, Princeton, USA. March 2012.\n  </li><li> <b>Finite Temperature Non-Equilibrium Superfluid Systems</b><br>\n       (FINESS 2011), Heidelberg, Germany - September 2011.\n  </li><li> <b>University of Colorado</b><br>\n       Theoretical Physics Seminar, Boulder, CO. September 2010.\n  </li><li> <b>\u00c9cole Polytechnique</b><br>\n       Theoretical Physics Seminar, Palaiseau, France. March 2010.\n\n\n\n</li></ul>\n\n\n<!-- Google analytics -->\n\n<script async=\"\" src=\"//www.google-analytics.com/analytics.js\"></script><script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-56767395-1', 'auto');\n  ga('send', 'pageview');\n\n</script>\n\n\n\n\n\n</li></div></div></div></body></html>"