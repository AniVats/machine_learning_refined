"<html lang=\"en\"><head>\n<title>Carl Vondrick - Columbia University - Computer Vision and Machine Learning</title>\n<meta charset=\"utf-8\">\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n\n<link href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO\" crossorigin=\"anonymous\">\n\n<script async=\"\" src=\"https://www.google-analytics.com/analytics.js\"></script><script src=\"https://code.jquery.com/jquery-3.1.1.slim.min.js\" integrity=\"sha384-A7FZj7v+d/sdmMqp/nOQwliLvUsJfDHW+k9Omg/a/EheAdgtzNs3hpfag6Ed950n\" crossorigin=\"anonymous\"></script>\n\n<style>\n.publogo { margin-right : 20px; }\n.publication { padding-left : 10px; padding-top : 10px; margin-bottom : 10px; }\n.publication strong a { color : #000; text-decoration  :none; }\n.publication strong a:hover { text-decoration : underline; }\n.publication .links a { margin-right : 15px; }\nul{ list-style:none; padding:0; margin:0; }\nul ul { padding-top : 6px; list-style : disc; padding-left : 40px; }\na, a:hover{ color: rgb(0, 114, 206); }\nul li { padding-bottom:6px; }\n.nameplate {\n  display: flex;\n  flex-direction: column;\n  justify-content: center;\n}\n.topbar { background-color:rgb(240,240,240); }\n.darker { background-color:rgb(240,240,240); }\nh4, h5 { color:#424242; }\nh4 { margin-top : 30px; }\n\n.papers-selected h5, .papers-selected h4 { display : none; }\n.papers-selected .publication { display : none; }\n.paperhi-only { display : none; }\n.papers-selected .paperhi { display : flex; }\n.papers-selected .paperlo { display : none; }\n\n@media (prefers-color-scheme: dark) {\n  body {\n    background-color : rgb(40, 40, 40); \n    color : #fff;\n  }\n  .topbar, .darker {\n    background-color : rgb(20, 20, 20);\n    color : #fff;\n  }\n  h4, h5 { color : #ccc; }\n  a, a:hover{ color: rgb(97, 185, 255) }\n  .publication strong a { color : #fff; }\n}\n</style>\n\n<link rel=\"shortcut icon\" href=\"/images/favicon.png\">\n\n<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n \n  ga('create', 'UA-1592615-30', 'auto', 'webwwwall');\n  ga('webwwwall.send', 'pageview');\n</script>\n</head>\n<body>\n\n<div class=\"topbar\">\n<div class=\"container\">\n<div>\n\n  <div class=\"row mt-0\">\n  <div class=\"col-8 col-sm-6 col-md-4 col-lg-3 pl-4 pt-4 pt-md-0\"><a href=\"photo-me2.jpg\"><img alt=\"\" src=\"photo-me.jpg\" id=\"me\" class=\"float-left img-fluid\"></a></div>\n \n\n      <div class=\"col-12 col-md-8  pl-4 py-4 py-md-1 nameplate\">\n        <h3>Carl Vondrick</h3>\n              <p class=\"my-0\" style=\"line-height:1.2;\">\n              Assistant Professor<br>\n       Department of Computer Science <br>\n       Columbia University <br>\n       <br>\n       Office: 618 CEPSR<br>\n       Address: 530 West 120th St, New York, NY 10027<br>\n       Email: vondrick@cs.columbia.edu\n              </p>    \n     </div>\n\n  </div>\n\n</div>\n</div>\n</div>\n\n<div class=\"container\">\n<div>\n\n  <div class=\"row\">\n    <div class=\"col-lg-6 order-0 col-12 px-4 papers-container papers-selected\">\n\n      <h4 class=\"paperhi\">Research</h4>\n\n      <p>Our group studies computer vision and machine learning. By training\n      machines to observe and interact with their surroundings, we aim to \n      create robust and versatile models for perception. We often investigate visual models that capitalize on\n      large amounts of unlabeled data and transfer across tasks and modalities.\n      Other interests include scene dynamics, sound and language and beyond, interpretable models, high-level reasoning, and\n      perception for robotics.</p>\n\n\n    </div>\n    <div class=\"col-lg-6 order-0 col-12 px-4 papers-container papers-selected\">\n      <h4 class=\"paperhi\">News</h4>\n      <ul id=\"news\">\n          <li>Checkout the second <a href=\"https://sites.google.com/view/luv2020\">Workshop on Learning from Unlabeled Video</a> at CVPR 2020! Previous years <a href=\"https://sites.google.com/view/luv2019\">here</a> and <a href=\"https://sites.google.com/view/self-supervised-icml2019\">here</a>.</li>\n          <li>Blog post: by learning to colorize videos, <a href=\"https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html\">visual tracking emerges</a>.</li>\n          <li>Checkout our new <a href=\"http://sound-of-pixels.csail.mit.edu/\">audio-visual source separation demo</a>.</li>\n      </ul>\n    </div>\n\n    <div class=\"col-12 px-4 pb-4 papers-container papers-selected\">\n      <h4 class=\"paperhi\">Lab Members</h4>\n      <ul>\n      <li>PhD Students: <a href=\"http://www.didacsuris.com\">D\u00eddac Sur\u00eds</a></li>\n      <li>PhD Affiliates: <a href=\"http://www.cs.columbia.edu/~bchen/\">Boyuan Chen</a>, <a href=\"http://www.cs.columbia.edu/~mcz/\">Chengzhi Mao</a></li>\n      <li>Undergraduates/Masters: <a href=\"https://basile.be/about-me/\">Basile Van Hoorick</a>, <a href=\"http://cv.cs.columbia.edu/dave/\">Dave Epstein</a>, Lovish Chum, Ruoshi Liu, <a href=\"https://stethio.com/about/\">Suman Mulumudi</a></li>\n      </ul>\n    </div>\n  </div>\n</div>\n</div>\n\n<div class=\"darker\">\n<div class=\"container\">\n<div>\n  <div class=\"row\">\n    <div class=\"order-0 col-12 px-4 papers-container papers-selected\">\n     \n<script>\n$(document).ready(function() {\n  $('.paperhi button').click(function() {\n     $('.papers-container').removeClass('papers-selected');\n  });\n  $('.paperlo button').click(function() {\n     $('.papers-container').addClass('papers-selected');\n  });\n\n});\n</script>\n     \n       <h4 class=\"paperlo\">All Papers\n       <button type=\"button\" class=\"ml-3 btn btn-dark\">Show representative papers</button>\n       </h4>\n       <h4 class=\"paperhi paperhi-only\">Representative Papers\n       <button type=\"button\" class=\"ml-3 btn btn-dark\">Show all papers</button>\n       </h4>\n       \n        \n        <h5 class=\"pt-2 pb-1\">2019</h5>\n\n          <div class=\"publication media paperhi\">\n          <a href=\"https://arxiv.org/pdf/1911.11237.pdf\"><img alt=\"\" src=\"expert.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1911.11237.pdf\">Learning to Learn Words from Narrated Video</a></strong><br>\n        D\u00eddac Sur\u00eds*, Dave Epstein*, Heng Ji, Shih-Fu Chang, Carl Vondrick<br>\n          <em>arXiv</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1911.11237.pdf\">Paper</a> <a href=\"https://expert.cs.columbia.edu\">Project Page</a> <a href=\"https://github.com/cvlab-columbia/expert\">Code</a></span> \n          </p></div>\n          </div>\n \n          <div class=\"publication media paperhi\">\n          <a href=\"https://arxiv.org/pdf/1911.11206.pdf\"><img alt=\"\" src=\"oops.gif\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1911.11206.pdf\">Oops! Predicting Unintentional Action in Video</a></strong><br>\n          Dave Epstein, Boyuan Chen, Carl Vondrick<br>\n          <em>CVPR 2020</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1911.11206.pdf\">Paper</a> <a href=\"https://oops.cs.columbia.edu\">Project Page</a> <a href=\"https://oops.cs.columbia.edu/data/\">Data</a> <a href=\"https://github.com/cvlab-columbia/oops\">Code</a></span> \n          </p></div>\n          </div>\n\n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/abs/1910.07882\"><img alt=\"\" src=\"hideseek.gif\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1910.07882\">Visual Hide and Seek</a></strong><br>\n          Boyuan Chen, Shuran Song, Hod Lipson, Carl Vondrick<br>\n          <em>arXiv</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1910.07882\">Paper</a> <a href=\"http://www.cs.columbia.edu/~bchen/visualhideseek/\">Project Page</a></span> \n          </p></div>\n          </div>\n \n\n\n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/abs/1909.00900\"><img alt=\"\" src=\"tla.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1909.00900\">Metric Learning for Adversarial Robustness</a></strong><br>\n          Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray<br>\n          <em>NeurIPS 2019</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1909.00900\">Paper</a></span> \n          </p></div>\n          </div>\n \n\n\n\n          <div class=\"publication media paperhi\">\n          <a href=\"https://arxiv.org/abs/1904.01766\"><img alt=\"\" src=\"videobert.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1904.01766\">VideoBERT: A Joint Model for Video and Language Representation Learning</a></strong><br>\n          Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid<br>\n          <em>ICCV 2019</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1904.01766\">Paper</a> <a href=\"https://ai.googleblog.com/2019/09/learning-cross-modal-temporal.html\">Blog</a></span> \n          </p></div>\n          </div>\n \n\n\n\n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/pdf/1811.11683.pdf\"><img alt=\"\" src=\"multi.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1811.11683.pdf\">Multi-level Multimodal Common Semantic Space for Image-Phrase Grounding</a></strong><br>\n          Hassan Akbari, Svebor Karaman, Surabhi Bhargava, Brian Chen, Carl Vondrick, Shih-Fu Chang<br>\n          <em>CVPR 2019</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1811.11683.pdf\">Paper</a></span> \n          </p></div>\n          </div>\n \n\n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/abs/1904.04231\"><img alt=\"\" src=\"relationalforecast.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1904.04231\">Relational Action Forecasting</a></strong><br>\n          Chen Sun, Abhinav Shrivastava, Carl Vondrick, Rahul Sukthankar, Kevin Murphy, Cordelia Schmid<br>\n          <em>CVPR 2019</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1904.04231\">Paper</a></span> \n          </p></div>\n          </div>\n \n        \n           <div class=\"publication media\">\n          <a href=\"http://moments.csail.mit.edu/\"><img alt=\"\" src=\"moments.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"http://moments.csail.mit.edu/\">Moments in Time Dataset: one million videos for event understanding</a></strong><br>\n          Mathew Monfort et al<br>\n          <em>PAMI 2019</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1801.03150.pdf\">Paper</a> <a href=\"http://moments.csail.mit.edu/\">Project Page</a></span> \n          </p></div>\n          </div>\n        \n        <h5 class=\"pt-2 pb-1\">2018</h5>\n        \n          <div class=\"publication media paperhi\">\n          <a href=\"https://arxiv.org/pdf/1806.09594.pdf\"><img alt=\"\" src=\"colorization.gif\" class=\"publogo img-fluid float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1806.09594.pdf\">Tracking Emerges by Colorizing Videos</a></strong><br>\n          Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, Kevin Murphy<br>\n          <em>ECCV 2018</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1806.09594.pdf\">Paper</a> <a href=\"https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html\">Blog</a></span>\n          </p></div>\n          </div>       \n  \n        \n          <div class=\"publication media paperhi\">\n          <a href=\"https://arxiv.org/abs/1804.03160\"><img alt=\"\" src=\"soundofpixels.png\" class=\"publogo img-fluid float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1804.03160\">The Sound of Pixels</a></strong><br>\n          Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba<br>\n          <em>ECCV 2018</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1804.03160\">Paper</a> <a href=\"http://sound-of-pixels.csail.mit.edu/\">Project Page</a></span>\n          </p></div>\n          </div>    \n          \n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/pdf/1807.10982.pdf\"><img alt=\"\" src=\"actioncontext.png\" class=\"publogo img-fluid float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1807.10982.pdf\">Actor-centric Relation Network</a></strong><br>\n          Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy, Rahul Sukthankar, Cordelia Schmid<br>\n          <em>ECCV 2018</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1807.10982.pdf\">Paper</a></span>\n          </p></div>\n          </div>     \n        \n       \n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/abs/1705.08421\"><img alt=\"\" src=\"ava.gif\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/abs/1705.08421\">AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions</a></strong><br>\n          Chunhui Gu et al<br>\n          <em>CVPR 2018</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/abs/1705.08421\">Paper</a> <a href=\"https://research.google.com/ava/\">Project Page</a></span> \n          </p></div>\n          </div>\n       \n        \n        <h5 class=\"pt-2 pb-1\">2017</h5>\n        \n          <div class=\"publication media\">\n          <a href=\"videogaze.pdf\"><img alt=\"\" src=\"videogaze.png\" class=\"publogo float-left rounded g\" width=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"videogaze.pdf\">Following Gaze in Video</a></strong><br>\n          Adria Recasens, Carl Vondrick, Aditya Khosla, Antonio Torralba<br>\n          <em>ICCV 2017</em><br>\n          <span class=\"links\"><a href=\"videogaze.pdf\">Paper</a></span> \n          </p></div>\n          </div>\n        \n          <div class=\"publication media\">\n          <a href=\"transformer.pdf\"><img alt=\"\" src=\"transformer.jpg\" class=\"publogo img-fluid float-left rounded\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"transformer.pdf\">Generating the Future with Adversarial Transformers</a></strong><br> \n          Carl Vondrick, Antonio Torralba<br>\n          <em>CVPR 2017</em><br>\n          <span class=\"links\"><a href=\"transformer.pdf\">Paper</a> <a href=\"transformer\">Project Page</a></span>\n          </p></div>\n          </div>\n\n          <div class=\"publication media\">\n          <a href=\"cmplaces_pami.pdf\"><img alt=\"\" src=\"adapt.gif\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"cmplaces_pami.pdf\">Cross-Modal Scene Networks</a></strong><br>\n          Yusuf Aytar*, Lluis Castrejon*,  Carl Vondrick, Hamed Pirsiavash, Antonio Torralba<br>\n          <em>PAMI 2017</em><br>\n          <span class=\"links\"><a href=\"cmplaces_pami.pdf\">Paper</a> <a href=\"http://projects.csail.mit.edu/cmplaces/\">Project Page</a></span>\n          </p></div>\n          </div>\n          \n          <div class=\"publication media\">\n          <a href=\"see-hear-read/\"><img alt=\"\" src=\"seereadhear.jpg\" class=\"publogo img-fluid float-left rounded\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"see-hear-read/\">See, Hear, and Read: Deep Aligned Representations</a></strong><br> \n          Yusuf Aytar, Carl Vondrick, Antonio Torralba<br>\n          <em>arXiv 2017</em><br>\n          <span class=\"links\"><a href=\"see-hear-read/paper.pdf\">Paper</a> <a href=\"http://people.csail.mit.edu/yusuf/see-hear-read/\">Project Page</a></span>\n          </p></div>\n          </div>\n\n\n         <h5 class=\"pt-2 pb-1\">2016</h5>\n         \n          <div class=\"publication media paperhi\">\n          <a href=\"tinyvideo/\"><img alt=\"\" src=\"tinyvideo.gif\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"tinyvideo/\">Generating Videos with Scene Dynamics</a></strong><br> \n          Carl Vondrick, Hamed Pirsiavash, Antonio Torralba<br>\n          <em>NeurIPS 2016</em><br>\n          <span class=\"links\"><a href=\"tinyvideo/paper.pdf\">Paper</a> <a href=\"tinyvideo\">Project Page</a> <a href=\"https://github.com/cvondrick/videogan\">Code</a> <a href=\"http://www.nbcnews.com/mach/technology/deep-learning-predicts-future-n690851\">NBC</a>  <a href=\"https://www.scientificamerican.com/article/spoiler-alert-artificial-intelligence-can-predict-how-scenes-will-play-out/\">Scientific American</a> <a href=\"https://www.newscientist.com/article/mg23231020-100-ai-learns-to-predict-the-future-by-watching-2-million-videos/\">New Scientist</a> <a href=\"http://news.mit.edu/2016/creating-videos-of-the-future-1129\">MIT News</a></span>\n          </p></div>\n          </div>  \n          \n          <div class=\"publication media paperhi\">\n          <a href=\"http://projects.csail.mit.edu/soundnet/\"><img alt=\"\" src=\"soundnet.png\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"http://projects.csail.mit.edu/soundnet/\">SoundNet: Learning Sound Representations from Unlabeled Video</a></strong><br> \n          Yusuf Aytar*, Carl Vondrick*, Antonio Torralba<br>\n          <em>NeurIPS 2016</em><br>\n          <span class=\"links\"><a href=\"soundnet.pdf\">Paper</a> <a href=\"http://projects.csail.mit.edu/soundnet/\">Project Page</a> <a href=\"https://github.com/cvondrick/soundnet\">Code</a> <a href=\"http://www.marketplace.org/shows/marketplace-tech/marketplace-tech-monday-december-5-2016\">NPR</a> <a href=\"https://www.newscientist.com/article/2111363-binge-watching-videos-teaches-computers-to-recognise-sounds/\">New Scientist</a> <a href=\"weekjunior.pdf\">Week Junior</a> <a href=\"http://news.mit.edu/2016/computer-learns-recognize-sounds-video-1202\">MIT News</a></span>\n          </p></div>\n          </div>\n              \n        \n          <div class=\"publication media paperhi\">\n          <a href=\"prediction\"><img alt=\"\" src=\"predictionlogo.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"prediction\">Anticipating Visual Representations with Unlabeled Video</a></strong><br>\n          Carl Vondrick, Hamed Pirsiavash, Antonio Torralba<br>\n          <em>CVPR 2016</em><br>\n          <span class=\"links\"><a href=\"prediction.pdf\">Paper</a> <a href=\"http://web.mit.edu/vondrick/prediction/\">Project Page</a> <a href=\"http://www.npr.org/sections/alltechconsidered/2016/07/12/485144229/a-computer-binge-watched-tv-and-learned-to-predict-what-happens-next\">NPR</a> <a href=\"http://money.cnn.com/2016/06/21/technology/ai-kissing/\">CNN</a> <a href=\"http://bigstory.ap.org/article/7ec97c70223e49b98fbc7df2ff824d59/how-do-you-teach-human-interaction-robot-lots-tv\">AP</a> <a href=\"http://www.wired.com/2016/06/mit-algorithm-predicts-future-watching-tv/\">Wired</a> <a href=\"https://www.facebook.com/colbertlateshow/videos/889597367851682/\">Stephen Colbert</a> <a href=\"https://www.csail.mit.edu/teaching_machines_to_predict_the_future\">MIT News</a> </span>\n          </p></div>\n          </div>\n\n          <div class=\"publication media\">\n          <a href=\"intention.pdf\"><img alt=\"\" src=\"why-logo.png\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"intention.pdf\">Predicting Motivations of Actions by Leveraging Text</a></strong><br>\n          Carl Vondrick, Deniz Oktay, Hamed Pirsiavash, Antonio Torralba<br>\n          <em>CVPR 2016</em><br>\n          <span class=\"links\"><a href=\"intention.pdf\">Paper</a> <a href=\"motivations_clean.zip\">Dataset</a></span>\n          </p></div>\n          </div>  \n        \n   \n          <div class=\"publication media\">\n          <a href=\"https://arxiv.org/pdf/1612.01175.pdf\"><img alt=\"\" src=\"mistaken.gif\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"https://arxiv.org/pdf/1612.01175.pdf\">Who is Mistaken?</a></strong><br>\n          Benjamin Eysenbach, Carl Vondrick, Antonio Torralba<br>\n          <em>arXiv 2016</em><br>\n          <span class=\"links\"><a href=\"https://arxiv.org/pdf/1612.01175.pdf\">Paper</a> <a href=\"http://people.csail.mit.edu/bce/mistaken/\">Project Page</a></span>\n          </p></div>\n          </div>  \n      \n \n        \n\n       \n\n          <div class=\"publication media\">\n          <a href=\"adaptation.pdf\"><img alt=\"\" src=\"cmplaces.png\" class=\"publogo img-fluid float-left rounded g\" width=\"100\" height=\"100\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"adaptation.pdf\">Learning Aligned Cross-Modal Representations from Weakly Aligned Data</a></strong><br>\n          Lluis Castrejon*, Yusuf Aytar*, Carl Vondrick, Hamed Pirsiavash, Antonio Torralba<br>\n          <em>CVPR 2016</em><br>\n          <span class=\"links\"><a href=\"adaptation.pdf\">Paper</a> <a href=\"http://projects.csail.mit.edu/cmplaces/\">Project Page</a> <a href=\"http://monday.csail.mit.edu/cmr/demo.php\">Demo</a></span>\n          </p></div>\n          </div> \n        \n          <div class=\"publication media paperhi\">\n          <a href=\"ihog\"><img alt=\"\" src=\"ihogduck.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"ihog\">Visualizing Object Detection Features</a></strong><br> \n          Carl Vondrick, Aditya Khosla, Hamed Pirsiavash, Tomasz Malisiewicz, Antonio Torralba<br>\n          <em>IJCV 2016</em><br>\n          <span class=\"links\"><a href=\"ihog/ijcv.pdf\">Paper</a>  <a href=\"ihog/\">Project Page</a> <a href=\"ihog/slides.pdf\">Slides</a>  <a href=\"http://web.mit.edu/newsoffice/2013/teaching-computers-to-see-by-learning-to-see-like-computers-0919.html\">MIT News</a></span>\n          </p></div>\n          </div>\n          \n          <h5 class=\"pt-2 pb-1\">2015</h5>\n    \n\n          <div class=\"publication media\">\n          <a href=\"bigdata.pdf\"><img alt=\"\" src=\"moredata.jpg\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"bigdata.pdf\">Do We Need More Training Data?</a></strong><br>\n          Xiangxin Zhu, Carl Vondrick, Charless C. Fowlkes, Deva Ramanan<br>\n          <em>IJCV 2015</em><br>\n          <span class=\"links\"><a href=\"bigdata.pdf\">Paper</a>   <a href=\"http://vision.ics.uci.edu/datasets/data_10X_release.tar\">Dataset</a></span>\n          </p></div>\n          </div>\n\n          <div class=\"publication media\">\n          <a href=\"imagination/paper.pdf\"><img alt=\"\" src=\"cihead.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"imagination/paper.pdf\">Learning Visual Biases from Human Imagination</a></strong><br>\n          Carl Vondrick, Hamed Pirsiavash, Aude Oliva, Antonio Torralba<br>\n          <em>NeurIPS 2015</em><br>\n          <span class=\"links\"><a href=\"imagination/paper.pdf\">Paper</a> <a href=\"imagination/\">Project Page</a> <a href=\"http://www.technologyreview.com/view/532231/random-image-experiment-reveals-the-building-blocks-of-human-imagination/\">Technology Review</a></span>\n          </p></div>\n          </div>       \n\n\n          <div class=\"publication media\">\n          <a href=\"gaze.pdf\"><img alt=\"\" src=\"gaze.png\" width=\"100\" height=\"100\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"gaze.pdf\">Where are they looking?</a></strong><br>\n          Adria Recasens*, Aditya Khosla*, Carl Vondrick, Antonio Torralba<br>\n          <em>NeurIPS 2015</em><br>\n          <span class=\"links\"><a href=\"gaze.pdf\">Paper</a> <a href=\"http://gazefollow.csail.mit.edu/\">Project Page</a> <a href=\"http://gazefollow.csail.mit.edu/demo.html\">Demo</a></span>\n          </p></div>\n          </div>\n\n          <h5 class=\"pt-2 pb-1\">2014</h5>\n\n\n          <div class=\"publication media\">\n          <a href=\"quality.pdf\"><img alt=\"\" src=\"action.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"quality.pdf\">Assessing the Quality of Actions</a></strong><br>\n          Hamed Pirsiavash, Carl Vondrick, Antonio Torralba<br>\n          <em>ECCV 2014</em><br>\n          <span class=\"links\"><a href=\"quality.pdf\">Paper</a> <a href=\"http://www.csee.umbc.edu/~hpirsiav/quality.html\">Project Page</a></span> \n          </p></div>\n          </div>\n\n\t  <h5 class=\"pt-2 pb-1\">2013</h5>\n\n\n          <div class=\"publication media\">\n          <a href=\"ihog\"><img alt=\"\" src=\"ihogduck.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"ihog\">HOGgles: Visualizing Object Detection Features</a></strong><br>\n          Carl Vondrick, Aditya Khosla, Tomasz Malisiewicz, Antonio Torralba<br>\n          <em>ICCV 2013</em><br>\n          <span class=\"links\"><a href=\"ihog/iccv.pdf\">Paper</a>  <a href=\"ihog/\">Project Page</a> <a href=\"ihog/slides.pdf\">Slides</a>  <a href=\"http://web.mit.edu/newsoffice/2013/teaching-computers-to-see-by-learning-to-see-like-computers-0919.html\">MIT News</a></span>\n          </p></div>\n          </div>\n          \n           <h5 class=\"pt-2 pb-1\">2012</h5>\n\n\n          <div class=\"publication media\">\n          <a href=\"largetrain.pdf\"><img alt=\"\" src=\"moredata.jpg\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"largetrain.pdf\">Do We Need More Training Data or Better Models for Object Detection?</a></strong><br>\n          Xiangxin Zhu, Carl Vondrick, Deva Ramanan, Charless C. Fowlkes<br>\n          <em>BMVC 2012</em><br>\n          <span class=\"links\"><a href=\"largetrain.pdf\">Paper</a>  <a href=\"http://vision.ics.uci.edu/datasets/data_10X_release.tar\">Dataset</a></span>\n          </p></div>\n          </div>\n\n  \n          <div class=\"publication media\">\n          <a href=\"vatic/ijcv.pdf\"><img alt=\"\" src=\"vatic/ijcv-logo-2.png\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"vatic/ijcv.pdf\">Efficiently Scaling Up Crowdsourced Video Annotation</a></strong><br>\n          Carl Vondrick, Donald Patterson, Deva Ramanan<br>\n          <em>IJCV 2012</em><br>\n          <span class=\"links\"><a href=\"vatic/ijcv.pdf\">Paper</a> <a href=\"vatic/\">Project Page</a></span> \n          </p>\n          </div>\n          </div>\n          \n           <h5 class=\"pt-2 pb-1\">2011</h5>\n\n\n          <div class=\"publication media\">\n          <a href=\"vatic/videoalearn.pdf\"><img alt=\"\" src=\"alearn.jpg\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"vatic/videoalearn.pdf\">Video Annotation and Tracking with Active Learning</a></strong><br>\n          Carl Vondrick, Deva Ramanan<br>\n          <em>NeurIPS 2011</em><br>\n          <span class=\"links\"><a href=\"vatic/videoalearn.pdf\">Paper</a> <a href=\"active/\">Project Page</a></span></p>\n          </div>\n          </div>\n\n\n          <div class=\"publication media\">\n          <a href=\"vatic/virat.pdf\"><img alt=\"\" src=\"virat.jpg\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"vatic/virat.pdf\">A Large-scale Benchmark Dataset for Event Recognition</a></strong><br>\n          Sangmin Oh, et al.<br>\n          <em>CVPR 2011</em><br>\n          <span class=\"links\"><a href=\"vatic/virat.pdf\">Paper</a> <a href=\"http://www.viratdata.org/\">Project Page</a></span></p>\n          </div>\n          </div>\n          \n          <h5 class=\"pt-2 pb-1\">2010</h5>\n\n\n          <div class=\"publication media\">\n          <a href=\"vatic/scalingup.pdf\"><img alt=\"\" src=\"vatic/logo.jpg\" class=\"publogo img-fluid float-left rounded g\"></a>\n          <div class=\"media-body\">\n          <p><strong><a href=\"vatic/scalingup.pdf\">Efficiently Scaling Up Video Annotation with Crowdsourced Marketplaces</a></strong><br>\n          Carl Vondrick, Deva Ramanan, Donald Patterson<br>\n          <em>ECCV 2010</em><br>\n          <span class=\"links\"><a href=\"vatic/scalingup.pdf\">Paper</a> <a href=\"vatic/\">Project Page</a></span> \n          </p>\n          </div>\n          </div>\n\n\n\n      </div>\n      </div>\n</div>\n</div>\n</div>\n\n<div class=\"container\">\n<div>\n\n\n  <div class=\"row\">\n\n    <div class=\"col-lg-6 col-12 px-4 order-1\">\n\n\n      <h4>Teaching</h4>\n      <ul>\n        <li><a href=\"https://sites.google.com/view/w4731computervision-fall2019/home\">Computer Vision, Fall 2019</a></li>\n        <li><a href=\"cs6998\">Advanced Computer Vision, Spring 2019</a></li>\n        <li><a href=\"http://w4731.cs.columbia.edu\">Computer Vision, Fall 2018</a></li>\n      </ul>\n\n\n            <h4>Talks</h4>\n\n            <ul>\n            <li><a href=\"https://www.youtube.com/watch?v=rXoJzj-s0DM&amp;feature=youtu.be&amp;t=3367\">Color and Sound</a></li>\n            \t<li><a href=\"http://view.vzaar.com/10438482/player\">SoundNet</a></li>\n                <li><a href=\"http://view.vzaar.com/8635753/player\">Visual Anticipation</a></li>\n            \t<li><a href=\"http://techtalks.tv/talks/hoggles-visualizing-object-detection-features/59386/\">Visualizing Object Detection Features</a></li>\n            </ul>\n\n           <h4>Code and Data</h4>\n           <ul>\n           \t<li><a href=\"https://github.com/cvondrick/soundnet\">SoundNet</a></li>\n           \t<li><a href=\"https://github.com/cvondrick/videogan\">Video Generator</a></li>\n           \t<li><a href=\"http://projects.csail.mit.edu/cmplaces/\">Cross-Modal Scenes</a></li>\n           \t<li><a href=\"http://gazefollow.csail.mit.edu/\">Gaze Following</a></li>\n           \t<li><a href=\"http://www.csee.umbc.edu/~hpirsiav/quality.html\">Action Assessment</a></li>\n           \t<li><a href=\"https://github.com/cvondrick/ihog\">HOGgles</a></li>\n           \t<li><a href=\"https://github.com/cvondrick/vatic\">Video Annotation on MTurk</a></li>\n            </ul>\n\n\n            <h4>Service</h4>\n            <ul>\n              <li>Area Chair, CVPR 2018-2020</li>\n              <li>Area Chair, ICLR 2020</li>\n              <li>Area Chair, NeurIPS 2019</li>\n              <li>Area Chair, ICML 2019</li>\n              <li>Organizer, Workshop on <a href=\"https://sites.google.com/view/multimodalvideo/home?authuser=0\">Multimodal Video Analysis</a>, ICCV 2019</li>\n              <li>Organizer, Workshop on <a href=\"https://sites.google.com/view/self-supervised-icml2019\">Self-supervised Learning</a>, ICML 2019</li>\n              <li>Organizer, Workshop on <a href=\"https://sites.google.com/view/luv2019\">Learning from Unlabeled Video</a>, CVPR 2019</li>\n              <li>Organizer, Tutorial on <a href=\"https://sites.google.com/view/unsupervisedvisuallearning/home\">Unsupervised Visual Learning</a>, CVPR 2018</li>\n            </ul>\n            \n            \n    </div>\n    <div class=\"col-lg-6 col-12 px-4 order-1 pb-4\">\n           \n           <h4>Links</h4>\n           <ul>\n           \t<li><a href=\"cv.pdf\">Curriculum Vitae</a></li>\n           \t<li><a href=\"http://scholar.google.com/citations?user=3MzhkFIAAAAJ&amp;hl=en\">Google Scholar</a></li>\n           \t<li><a href=\"https://twitter.com/cvondrick\">Twitter</a></li>\n           \t<li><a href=\"https://github.com/cvondrick\">Github</a></li>\n            </ul>\n\n            <h4>Funding</h4>\n            <ul>\n              <li><a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1925157&amp;HistoricalAwards=false\">NSF NRI</a>, 2019-2022</li>\n              <li><a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1850069&amp;HistoricalAwards=false\">NSF CRII</a>, 2019-2021</li>\n              <!--<li><a href=\"https://www.darpa.mil/news-events/2018-10-11\">DARPA MCS</a>, 2019-2023</li>-->\n              <li><a href=\"https://www.darpa.mil/work-with-us/ai-next-campaign\">DARPA GAILA</a>, 2019-2020</li>\n              <li><a href=\"https://ara.amazon-ml.com/recipients/#2018\">Amazon Gift</a>, 2018-2019</li>\n            </ul>\n\n\n            <h4>Press</h4> \n\n\t     <ul>\n            <li><a href=\"http://www.npr.org/sections/alltechconsidered/2016/07/12/485144229/a-computer-binge-watched-tv-and-learned-to-predict-what-happens-next\">NPR</a></li>\n            <li><a href=\"http://money.cnn.com/2016/06/21/technology/ai-kissing/\">CNN</a></li>\n            <li><a href=\"https://www.apnews.com/7ec97c70223e49b98fbc7df2ff824d59\">Associated Press (AP)</a></li>\n            <li><a href=\"http://www.nbcnews.com/mach/technology/deep-learning-predicts-future-n690851\">NBC News</a></li>\n            <li><a href=\"https://www.wired.com/2016/06/mit-algorithm-predicts-future-watching-tv/\">Wired</a></li>\n            <li><a href=\"https://www.scientificamerican.com/article/spoiler-alert-artificial-intelligence-can-predict-how-scenes-will-play-out/\">Scientific American</a></li>\n            <li><a href=\"https://www.newscientist.com/article/2111363-binge-watching-videos-teaches-computers-to-recognise-sounds/\">New Scientist</a></li>\n            <li><a href=\"weekjunior.pdf\">Week Junior</a></li>\n            <li><a href=\"https://www.youtube.com/watch?v=pGrNwtHxMqs\">Late Show with Stephen Colbert</a></li>\n            <li><a href=\"http://www.forbes.com/sites/janetwburns/2016/06/22/mit-computers-binge-watch-desperate-housewives-the-office-to-learn-about-hugs/#7adca64a3892\">Forbes</a></li>\n            <li><a href=\"http://www.newsweek.com/artificial-intelligence-algorithm-predicts-future-473118\">Newsweek</a></li>\n            <li><a href=\"http://www.popsci.com/binge-watching-tv-helps-computer-predict-human-behavior\">PopSci</a></li>\n            <li><a href=\"http://www.theverge.com/2016/9/12/12886698/machine-learning-video-image-prediction-mit\">The Verge</a></li>\n            <li><a href=\"http://motherboard.vice.com/read/researchers-taught-a-machine-how-to-generate-the-next-frames-in-a-video\">Vice</a></li>\n            <li><a href=\"https://www.engadget.com/2016/11/28/ai-creates-videos-of-the-future/\">Engagdet</a></li>\n            <li><a href=\"https://www.technologyreview.com/s/532231/random-image-experiment-reveals-the-building-blocks-of-human-imagination/\">Technology Review</a></li>\n            <li><a href=\"http://www.marketplace.org/shows/marketplace-tech/marketplace-tech-monday-december-5-2016\">NPR Marketplace</a></li>\n\t    </ul>\n\n    </div>\n\n  </div>\n\n</div>\n</div>\n\n\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.bundle.min.js\" integrity=\"sha384-pjaaA8dDz/5BgdFUPX6M/9SUZv4d12SUPF0axWc+VRZkx5xU3daN+lYb49+Ax+Tl\" crossorigin=\"anonymous\"></script>\n\n\n\n</body></html>"