"<html lang=\"en\" dir=\"ltr\"><head>\n\t\t<title>UMN VIP</title>\n\t\t<meta charset=\"iso-8859-1\">\n\t\t<link rel=\"stylesheet\" href=\"styles/layout.css\" type=\"text/css\">\n\t\t<script src=\"scripts/jquery-1.8.2.min.js\"></script>\n\t\t<script src=\"scripts/jquery-defaultvalue.js\"></script>\n\t\t<script src=\"scripts/jquery-browsercheck.js\"></script>\n\t\t<!--[if lt IE 9]>\n<link rel=\"stylesheet\" href=\"styles/ie.css\" type=\"text/css\">\n<script src=\"scripts/ie/html5shiv.min.js\"></script>\n<![endif]-->\n\t\t<!-- homepage scripts -->\n\t\t<script src=\"scripts/jquery-slides.min.js\"></script>\n\t\t<script>\n\t\t\t$(function() {\n\t\t\t\t$('#slider').slides({\n\t\t\t\t\tpreload: true,\n\t\t\t\t\tpreloadImage: 'images/slider/controls/loading.gif',\n\t\t\t\t\tplay: 5000,\n\t\t\t\t\tpause: 2500,\n\t\t\t\t\thoverPause: true\n\t\t\t\t});\n\t\t\t});\n\t\t</script>\n\t\t<!-- / homepage scripts -->\n\t</head>\n\t<body>\n\t\t<div class=\"wrapper row1\">\n\t\t\t<header id=\"header\" class=\"clear\">\n\t\t\t\t<hgroup>\n\t\t\t\t\t<h1>\n\t\t\t\t\t\t<a href=\"index.html\">UMN VIP</a>\n\t\t\t\t\t</h1>\n\t\t\t\t\t<h2>Visual Information Processing Lab</h2>\n\t\t\t\t</hgroup>\n\t\t\t\t<nav>\n\t\t\t\t\t<ul class=\"clear\">\n\t\t\t\t\t\t<li class=\"first active\">\n\t\t\t\t\t\t\t<a href=\"index.html\">Home</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"people.html\">People</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"research.html\">Research</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"publications.html\">Publications</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"courses.html\">Teaching</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"jobs.html\">Jobs</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t<a href=\"contact.html\">Contact</a>\n\t\t\t\t\t\t</li>\n\t\t\t\t\t</ul>\n\t\t\t\t</nav>\n\t\t\t</header>\n\t\t</div>\n\t\t<!-- content -->\n\t\t<div class=\"wrapper row3\">\n\t\t\t<div id=\"container\" class=\"clear\">\n\t\t\t\t<div id=\"homepage\">\n\t\t\t\t\t<!-- ########################################################################################## -->\n\t\t\t\t\t<!-- ########################################################################################## -->\n\t\t\t\t\t<!-- ########################################################################################## -->\n\t\t\t\t\t<!-- ########################################################################################## -->\n\t\t\t\t\t<section id=\"shout\">\n\t\t\t\t\t\t<ul class=\"clear\">\n\t\t\t\t\t\t\t<li class=\"one_quarter first\">\n\t\t\t\t\t\t\t\t<img src=\"images/people/catherine_zhao_cropped.jpg\" width=\"100%\">\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li class=\"one_quarter\">\n\t\t\t\t\t\t\t\t<h1>Catherine Qi Zhao</h1>\n\t\t\t\t\t\t\t\t<p>Assistant Professor<br>\n\t\t\t\t\t\t\t\t\tDepartment of Computer Science and Engineering<br>\n\t\t\t\t\t\t\t\t\tUniversity of Minnesota<br>\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\tOffice: Keller 5-213<br>\n\t\t\t\t\t\t\t\t\tPhone: (612) 301-2115<br>\n\t\t\t\t\t\t\t\t\tEmail: qzhao at cs.umn.edu<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<!-- ########################################################################################## -->\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<!-- / Slider -->\n\t\t\t\t\t\t\t<li class=\"two_quarter\">\n\t\t\t\t\t\t\t\t<h1> \n\t\t\t\t\t\t\t\t</h1>\n\t\t\t\t\t\t\t\t<p>My research is in the areas of computer vision, machine learning, computational neuroscience, and healthcare. In particular, I am interested in providing theoretical foundations and computational innovations in computer vision, inventing new machine learning methods inspired by many AI problems including vision, brain sciences, and medical sciences, and building intelligent systems that leverage both artificial and human intelligence. \n\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p>My recent focus includes understanding neural networks and developing networks and other AI techniques for emerging applications in healthcare and brain science, for example, to understand and identify neurodevelopmental disorders with visual behaviors, and to decode human motor intention with amputee patients based on peripheral nerve neural recordings.</p>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t</ul>\n\t\t\t\t\t</section>\n\t\t\t\t\t<section id=\"service\">\n\t\t\t\t\t\t<h1>News</h1>\n\t\t\t\t\t\t<ul>\n<li>we received an NSF RI grant to study attention and reasoning. <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1908711&amp;HistoricalAwards=false\">[link]</a>\n</li><li>our work on autism screening with multi-modality information is accepted to ICCV.</li>\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n<li>I will be an area chair for CVPR 2020 and doctoral consortium chair for WACV 2020 and CVPR 2023. \n\t\t\t\t\t\t\t</li>\n<li>we received an NSF S&amp;AS grant to develop intelligent UAVs with active vision. <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1849107&amp;HistoricalAwards=false\">[link]</a>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>our work on shallowing deep neural networks is out. <a href=\"publications/pdf/shallowing2018.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>I will be an area chair for WACV 2019, CVPR 2019, and IJCAI 2019. \n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>our work on emotion and attention is out. \n\t\t\t\t\t\t\t\t<a href=\"emotionalattention.html\">[project page]</a>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>we received an NSF SHF grant to develop efficient time-based deep neural networks. <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1763761&amp;HistoricalAwards=false\">[link]</a>\n\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>we are organizing the \n\t\t\t\t\t\t\t\t<a href=\"http://salicon.net/challenge-2017/\">3rd LSUN Saliency Challenge</a>, in conjuection with CVPR 2017. \n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>the new book I edited is out! -- it provides an overview of vision from various perspectives, ranging from neuroscience to cognition, and from computational principles to engineering. \n\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t<a href=\"http://www.springer.com/book/9789811002113\">\n\t\t\t\t\t\t\t\t\t<img src=\"images/book_cover.jpg\" height=\"120\"></a>\n\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>our work on autism photo is out in \n\t\t\t\t\t\t\t\t<a href=\"http://www.cell.com/current-biology/abstract/S0960-9822(16)31000-4\">Current Biology</a>.</li>\n\t\t\t\t\t\t\t<li>I joined the University of Minnesota Twin Cities as an assistant professor.</li>\n\t\t\t\t\t\t\t<li>commentary about our autism work appears in \n\t\t\t\t\t\t\t\t<a href=\"http://www.cell.com/neuron/abstract/S0896-6273%2815%2900927-7\">Neuron</a>!</li>\n\t\t\t\t\t\t\t<li>press release at \n\t\t\t\t\t\t\t\t<a href=\"http://www.businessinsider.com/how-autistic-people-see-the-world-2015-10\">Business Insider</a>, \n\t\t\t\t\t\t\t\t<a href=\"http://www.huffingtonpost.com/entry/autism-study-photos_562a51dfe4b0443bb563ad29\">Huffington Post</a>, \n\t\t\t\t\t\t\t\t<a href=\"http://medicalxpress.com/news/2015-10-probing-mysterious-perceptual-world-autism.html\">MedicalXpress</a>, \n\t\t\t\t\t\t\t\t<a href=\"http://www.dailymail.co.uk/sciencetech/article-3286740/How-people-autism-world-Gaze-condition-bypasses-faces-details-colour-contrast.html\">Daily Mail</a>, \n\t\t\t\t\t\t\t\t<a href=\"http://www.futurity.org/autism-visual-perception-1032512-2/\">Futurity</a>, \n\t\t\t\t\t\t\t\t<a href=\"https://www.ece.nus.edu.sg/stfpage/eleqiz/\">NUS</a>, and \n\t\t\t\t\t\t\t\t<a href=\"http://www.caltech.edu/news/probing-mysterious-perceptual-world-autism-48543\">Caltech</a>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>our work is on the cover of \n\t\t\t\t\t\t\t\t<a href=\"http://www.cell.com/neuron/issue?pii=S0896-6273(14)X0045-0\">Neuron</a>! \n\t\t\t\t\t\t\t\t<a href=\"publications/pdf/neuron15.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t<a href=\"neuron.html\">\n\t\t\t\t\t\t\t\t\t<img src=\"images/proj_autism_neuron/cover.jpg\" height=\"120\"></a>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<p align=\"right\">\n\t\t\t\t\t\t\t<a href=\"news.html\">More</a>\n\t\t\t\t\t\t</p>\n\t\t\t\t\t</section>\n\t\t\t\t\t<!-- Latest -->\n\t\t\t\t\t<section id=\"latest\">\n\t\t\t\t\t\t<ul class=\"clear\">\n\t\t\t\t\t\t\t<li class=\"two_quarter first\">\n\t\t\t\t\t\t\t\t<h1>Database</h1>\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"http://salicon.net\">SALICON database</a>. Saliency in Context - a large-scale attention database on MS COCO images. Jiang et al. CVPR \n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/pdf/salicon_cvpr15.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/bib/jiang2015salicon.txt\">\n\t\t\t\t\t\t\t\t\t\t\t[bib]</a>\n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t<a href=\"emotionalattention.html\">EMOd database</a>. EMOtional attention dataset- a database with rich sentiment and semantic attributes (4302 objects, 33 high-level attributes). Fan et al. CVPR \n\t\t\t\t\t\t\t\t\t<a href=\"publications/pdf/emoattention_cvpr18.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t\t\t<a href=\"publications/bib/fan2018emoattention.txt\">\n\t\t\t\t\t\t\t\t\t\t[bib]</a>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"predicting.html\">OSIE database</a>. Object and Semantic Images and Eye-tracking database - a database for object and semantic saliency (700 images, 5551 objects with fine contour and semantic attribute labeling). Xu et al. JoV \n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/pdf/saliency_jov14.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/bib/xu2014predicting.txt\">\n\t\t\t\t\t\t\t\t\t\t\t[bib]</a>\n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"crowd.html\">EyeCrowd database</a>. Eye Fixations in Crowd database - a database for saliency in crowd. Jiang et al. ECCV \n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/pdf/crowd_eccv14.pdf\">[pdf]</a>\n\t\t\t\t\t\t\t\t\t\t<a href=\"publications/bib/jiang2014crowd.txt\">\n\t\t\t\t\t\t\t\t\t\t\t[bib]</a>\n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t<p align=\"right\">\n\t\t\t\t\t\t\t\t\t<a href=\"resources.html\">More</a>\n\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li class=\"two_quarter\">\n\t\t\t\t\t\t\t\t<h1>Code</h1>\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"emotionalattention.html\">Context-Adaptive Saliency Network</a>. Code for network implementation of context-adaptive saliency prediction, and evaluation metrics. \n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"https://github.com/NUS-VIP/predicting-human-gaze-beyond-pixels\">Saliency with Objects and Semantics</a>. Code for object and semantic feature computation, model training with SVM, saliency prediction, and evaluation measures. \n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"https://github.com/NUS-VIP/saliency-in-crowd\">Saliency in Crowd</a>. Code for crowd feature computation, crowd stats calculation, model training with MKL, saliency prediction, and evaluations. \n\t\t\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t\t\t<a href=\"deep_saliency.html\">Multi-Layer Sparse Network</a>. Code for multi-layer sparse network, model training, and saliency prediction.</li>\n\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t<p align=\"right\">\n\t\t\t\t\t\t\t\t\t<a href=\"resources.html\">More</a>\n\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<!--<i class=\"fl_right\"><a href=\"http://wiki-nusvip.rhcloud.com/doku.php\">Wiki</a></i>-->\n\t\t\t\t\t</section>\n\t\t\t\t\t<!-- / Latest -->\n\t\t\t\t</div>\n\t\t\t\t<!-- / content body -->\n\t\t\t</div>\n\t\t</div>\n\t\t<!-- Copyright -->\n\t\t<div class=\"wrapper row5\">\n\t\t\t<footer id=\"copyright\" class=\"clear\">\n\t\t\t\t<p class=\"fl_left\">Copyright \u00a9 2018 - All Rights Reserved - \n\t\t\t\t\t<a href=\"#\">UMN VIP</a>\n\t\t\t\t</p>\n\t\t\t</footer>\n\t\t</div>\n\t\n</body></html>"