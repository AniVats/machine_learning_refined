"<html><head lang=\"en\">\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  <title>Jacob Andreas @ MIT</title>\n  <link href=\"css/bootstrap.min.css\" rel=\"stylesheet\">\n  <link href=\"css/style.css\" rel=\"stylesheet\">\n  <link href=\"css/research.css\" rel=\"stylesheet\">\n</head>\n<body>\n<div class=\"content\">\n  <h2 class=\"name\">Jacob Andreas</h2>\n  <img class=\"margin\" src=\"figs/head1_crop.jpg\" width=\"100%\">\n\n  <p>\n    <b>I'm interested in language as a communicative and computational tool.</b>\n    People learn to understand and generate novel utterances from remarkably\n    little data. Having learned language, we use it acquire new conceps and to\n    structure our reasoning. Current machine learning techniques fall short of\n    human abilities in both their capacity to <i>learn language</i> and <i>learn\n    from language</i> about the rest of the world. My research aims to (1)\n    understand the computational foundations of efficient language learning,\n    and (2) build general-purpose intelligent systems that can communicate\n    effectively with humans and learn from human guidance.\n  </p>\n\n  <p>\n    I'm the \n    <a href=\"https://www.x.org/wiki/XConsortium/\">X Consortium</a>\n    Career Development Assistant Professor at MIT in \n    <a href=\"http://www.eecs.mit.edu/\">EECS</a> and\n    <a href=\"http://www.eecs.mit.edu/\">CSAIL</a>.\n    I did my PhD work at Berkeley, where I was a member of the \n    <a href=\"http://nlp.cs.berkeley.edu/\">Berkeley NLP Group</a>\n    and the \n    <a href=\"http://bair.berkeley.edu/\">Berkeley AI Research Lab</a>.  \n    I've also spent time with the\n    <a href=\"http://www.cl.cam.ac.uk/research/nl/\"> Cambridge NLIP Group</a>, \n    and the \n    <a href=\"http://ccls.columbia.edu\">Center for Computational Learning Systems</a>\n    and\n    <a href=\"http://www.cs.columbia.edu/nlp\">NLP Group</a>\n    at Columbia.\n  </p>\n\n  <p>\n    <strong>Prospective students</strong>: apply through the MIT <a href=\"https://gradapply.mit.edu/eecs/apply/login/?next=/eecs/\"> graduate\n    admissions portal</a> in the fall. Please read my <a href=\"advising.html\">advising statement</a> if you're considering applying.\n    <strong>Prospective visitors</strong>: I do not currently have openings for\n    visiting researchers. (I'm afraid I generally can't respond to individual\n    emails about either grad admissions or visitor positions / internships.) \n  </p>\n\n  <p>\n    <a href=\"mailto:jda@mit.edu\">jda@mit.edu</a>,\n    <a href=\"docs/jda_cv.pdf\">Curriculum vit\u00e6</a>,\n    <a href=\"http://scholar.google.com/citations?user=dnZ8udEAAAAJ\">Google scholar</a>,\n    <a href=\"http://jacobandreas.net\">elsewhere</a>\n  </p>\n\n  <hr>\n\n  <h3>\n    <a href=\"talks.html\">Talks</a> / \n    <a href=\"pubs.html\">Papers</a> /\n    <a href=\"bio.html\">Bio</a>\n  </h3>\n\n  <hr>\n\n  Some current research directions:\n\n  <div class=\"highlight\">\n    <!--<img src=\"figs/l3.jpg\" class=\"margin\">-->\n    <h4>Learning from language</h4>\n    <p>\n      Much of humans' abstract knowledge comes from abstract descriptions, but\n      almost all machine learning research focuses on learning from\n      comparatively low-level demonstrations or interactions. How do we enable\n      more natural and efficient learning from natural language supervision\n      instead?\n    </p>\n    <p>\n      <b>Papers</b>:<br>\n      <a href=\"https://arxiv.org/pdf/1611.01796\">\n        Modular multitask reinforcement learning with policy sketches\n      </a> (ICML 2017)<br>\n      <a href=\"https://arxiv.org/abs/1711.00482\">\n        Learning with latent language\n      </a> (NAACL 2018)\n    </p>\n  </div>\n\n  <div class=\"highlight\">\n    <!--<img src=\"figs/neuralese.jpg\" class=\"margin\">-->\n    <h4>Interpretation and explanation</h4>\n    <p>\n      How can we help humans understand the features and representational\n      strategies that black-box machine learning algorithms discover? To what\n      extent do these strategies reflect abstractions that we already have\n      names for?\n    </p>\n    <p>\n      <b>Papers</b>:<br>\n      <a href=\"https://arxiv.org/pdf/1704.06960\">\n        Translating neuralese\n      </a> (ACL 2017)<br>\n      <a href=\"https://arxiv.org/pdf/1707.08139\">\n        Analogs of linguistic structure in deep representations\n      </a> (EMNLP 2017)\n    </p>\n  </div>\n\n  <div class=\"highlight\">\n    <!--<img src=\"figs/compositionality.jpg\" class=\"margin\">-->\n    <h4>Compositionality and generalization</h4>\n    <p>\n      Compositionality and modularity are core features of representational\n      systems in language, software and biology. But what problem do\n      compositional representations actually solve? <!--What do we gain from\n      explicitly encouraging compositional structure in free-form computational\n      models?--> Can we use descriptions of abstract compositional structure in\n      one domain (e.g. language) to learn modular representations in another\n      (e.g. vision)?\n    </p>\n    <p>\n      <b>Papers</b>:<br>\n      <a href=\"https://arxiv.org/abs/1511.02799\">\n        Neural module networks\n      </a> (CVPR 2016)<br>\n      <a href=\"https://arxiv.org/abs/1902.07181\">\n        Measuring compositionality in representation learning\n      </a> (ICLR 2019)\n    </p>\n  </div>\n\n  <p>\n    I'm also interested in\n    <a href=\"https://arxiv.org/abs/1705.03919\">trees</a>,\n    <a href=\"https://www.aclweb.org/anthology/P13-1091/\">graphs</a>, \n    <a href=\"https://arxiv.org/abs/1604.00562\">games</a>,\n    and\n    <a href=\"https://papers.nips.cc/paper/5432-unsupervised-transcription-of-piano-music\">pianos</a>.\n  </p>\n\n  <hr>\n\n  <p class=\"offset\">\n    Collaboration graph trivia: My Erd\u0151s number is at most three \n    (J Andreas <a href=\"https://arxiv.org/abs/1711.02301\">to</a> \n    R Kleinberg <a href=\"https://dl.acm.org/citation.cfm?id=1255444\">to</a> \n    L Lov\u00e1sz <a href=\"https://www.sciencedirect.com/science/article/pii/B9780720422627500181\">to</a>\n    P Erd\u0151s). \n    My Kevin Bacon number (and consequently my Erd\u0151s-Bacon number) remains\n    <a href=\"http://jacobandreas.net/misc/bacon.html\">lamentably undefined</a>,\n    but my Kevin Knight number (since apparently that's \n    <a href=\"http://www.cs.bgu.ac.il/~yoavg/uni/\">a thing</a>) \n    is one. I have never starred in a film with Kevin Knight. Noam Chomsky is\n    my great-great-grand-advisor (J Andreas to D Klein to C Manning to J\n    Bresnan to N Chomsky).\n  </p>\n\n  <hr>\n\n  <p>\n    Opinionated bibliographies on:<br>\n    <a href=\"https://github.com/jacobandreas/bibs/blob/master/nmn.md\">\n      module networks</a><br>\n    <a href=\"https://github.com/jacobandreas/bibs/blob/master/language_behavior.md\">\n      language and behavior</a><br>\n  </p><p>\n\n  </p><hr>\n\n  <p class=\"credit\">Photo: Lillie Paquette / MIT School of Engineering</p>\n\n  </div></body></html>"