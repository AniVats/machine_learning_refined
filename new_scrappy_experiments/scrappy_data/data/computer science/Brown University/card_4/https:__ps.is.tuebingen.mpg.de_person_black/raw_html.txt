"<html lang=\"en\" class=\"\"><head>\n\n\t<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\n\t<title>Michael Black | Perceiving Systems - Max Planck Institute for Intelligent Systems</title>\n\n\t<meta name=\"keywords\" content=\"Computer Vision, Optical Flow, Human Motion, Computer Graphics, Computational Neuroscience, Motion Capture\">\n\t<meta name=\"description\" content=\"Michael J. Black home page\">\n  \t<meta name=\"author\" content=\"Jon Williams\">\n\n  \t<meta property=\"og:title\" content=\"Michael Black | Perceiving Systems - Max Planck Institute for Intelligent Systems\">\n\t<meta property=\"og:description\" content=\"Michael J. Black home page\">\n\t<meta property=\"og:type\" content=\"website\">\n\t<meta property=\"og:url\" content=\"https://is.tuebingen.mpg.de/\">\n\n\t<meta charset=\"utf-8\">\n\t<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n\n\t<meta name=\"google-site-verification\" content=\"ECPD8vOhMbNjxILeTmFab1vP33FWWRBf-0RbcRl4eFo\">\n\n\t<link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"/assets/favicon-32ffa17fc0e0588a030547e20f53c4be.png\">\n\t<link rel=\"shortcut icon\" href=\"../../docs-assets/ico/favicon.png\">\n\n\t<script type=\"text/javascript\" defer=\"\" async=\"\" src=\"https://piwik.tuebingen.mpg.de/piwik.js\"></script><script src=\"https://maps.google.com/maps/api/js?key=AIzaSyCeKJQacsNJKGU9wfdBV47C8-zFarcIGhg&amp;sensor=true\"></script>\n\n\t<style>.file-input-wrapper { overflow: hidden; position: relative; cursor: pointer; z-index: 1; }.file-input-wrapper input[type=file], .file-input-wrapper input[type=file]:focus, .file-input-wrapper input[type=file]:hover { position: absolute; top: 0; left: 0; cursor: pointer; opacity: 0; filter: alpha(opacity=0); z-index: 99; outline: 0; }.file-input-name { margin-left: 8px; }</style><link rel=\"stylesheet\" media=\"all\" href=\"/assets/application-0305eaf3c92e5ebe456ae2431ba48338.css\" data-turbolinks-track=\"true\">\n\t<script src=\"/assets/application-f97070958acf7f1ba122c2195da742dc.js\" data-turbolinks-track=\"true\"></script><style>.cke{visibility:hidden;}</style>\n\n\t<script type=\"text/x-mathjax-config;executed=true\">\n\t\tMathJax.Hub.Config({\n\t\t\t\"HTML-CSS\": {\n\t\t\t\tstyles: {\n\t\t\t\t\t\".MathJax .mo, .MathJax .mi\": {color: \"inherit ! important\"}\n\t\t\t\t}\n\t\t\t},\n\t\t\ttex2jax: {\n\t\t\t\tinlineMath: [ ['$','$'] ],\n\t\t\t\tprocessEscapes: true\n\t\t\t}\n\t\t});\n\t</script>\n\t<script src=\"/mathjax/MathJax.js?config=TeX-AMS_HTML-full.js\" type=\"text/javascript\"></script>\n\t\n\t<meta name=\"csrf-param\" content=\"authenticity_token\">\n<meta name=\"csrf-token\" content=\"Ktam488/ikXNpCmCIS8amO+pf1+cTE4l6WoHqTiq0xdzSls7MfeHcBPO2gWpyYfOMhu40h2nnG5vcXD9YU2Eug==\">\n<style type=\"text/css\">.fancybox-margin{margin-right:0px;}</style><style type=\"text/css\">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}\n.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}\n.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}\n.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}\n.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}\n</style><style type=\"text/css\">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n#MathJax_About.MathJax_MousePost {outline: none}\n.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n.MathJax_MenuItem {padding: 1px 2em; background: transparent}\n.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}\n.MathJax_MenuActive .MathJax_MenuArrow {color: white}\n.MathJax_MenuArrow.RTL {left: .5em; right: auto}\n.MathJax_MenuCheck {position: absolute; left: .7em}\n.MathJax_MenuCheck.RTL {right: .7em; left: auto}\n.MathJax_MenuRadioCheck {position: absolute; left: .7em}\n.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}\n.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}\n.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}\n.MathJax_MenuDisabled {color: GrayText}\n.MathJax_MenuActive {background-color: #606872; color: white}\n.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}\n.MathJax_ContextMenu:focus {outline: none}\n.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}\n#MathJax_AboutClose {top: .2em; right: .2em}\n.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}\n.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}\n.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}\n.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}\n.MathJax_MenuClose:hover span {background-color: #CCC!important}\n.MathJax_MenuClose:hover:focus {outline: none}\n</style><style type=\"text/css\">.MathJax_Preview .MJXf-math {color: inherit!important}\n</style><style type=\"text/css\">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}\n.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}\n</style><style type=\"text/css\">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}\n#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}\n#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}\n#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}\n</style><style type=\"text/css\">.MathJax_Preview {color: #888}\n#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}\n#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}\n.MathJax_Error {color: #CC0000; font-style: italic}\n</style><style type=\"text/css\">.MJXp-script {font-size: .8em}\n.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}\n.MJXp-bold {font-weight: bold}\n.MJXp-italic {font-style: italic}\n.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}\n.MJXp-largeop {font-size: 150%}\n.MJXp-largeop.MJXp-int {vertical-align: -.2em}\n.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}\n.MJXp-display {display: block; text-align: center; margin: 1em 0}\n.MJXp-math span {display: inline-block}\n.MJXp-box {display: block!important; text-align: center}\n.MJXp-box:after {content: \" \"}\n.MJXp-rule {display: block!important; margin-top: .1em}\n.MJXp-char {display: block!important}\n.MJXp-mo {margin: 0 .15em}\n.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}\n.MJXp-denom {display: inline-table!important; width: 100%}\n.MJXp-denom > * {display: table-row!important}\n.MJXp-surd {vertical-align: top}\n.MJXp-surd > * {display: block!important}\n.MJXp-script-box > *  {display: table!important; height: 50%}\n.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}\n.MJXp-script-box > *:last-child > * {vertical-align: bottom}\n.MJXp-script-box > * > * > * {display: block!important}\n.MJXp-mphantom {visibility: hidden}\n.MJXp-munderover {display: inline-table!important}\n.MJXp-over {display: inline-block!important; text-align: center}\n.MJXp-over > * {display: block!important}\n.MJXp-munderover > * {display: table-row!important}\n.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}\n.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}\n.MJXp-mtr {display: table-row!important}\n.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}\n.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}\n.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}\n.MJXp-mlabeledtr {display: table-row!important}\n.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}\n.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}\n.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}\n.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}\n.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}\n.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}\n.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}\n.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}\n.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}\n.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}\n.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}\n.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}\n.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}\n.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}\n</style><style type=\"text/css\">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}\n.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}\n.MathJax .MJX-monospace {font-family: monospace}\n.MathJax .MJX-sans-serif {font-family: sans-serif}\n#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}\n.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}\n.MathJax:focus, body :focus .MathJax {display: inline-table}\n.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: 5000em; max-height: 5000em; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}\nimg.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}\n.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}\n.MathJax nobr {white-space: nowrap!important}\n.MathJax img {display: inline!important; float: none!important}\n.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}\n.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}\n.MathJax_Processed {display: none!important}\n.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}\n.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}\n.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}\n.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}\n#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}\n@font-face {font-family: MathJax_Blank; src: url('about:blank')}\n.MathJax .mo, .MathJax .mi {color: inherit ! important}\n.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}\n</style><script type=\"text/javascript\" charset=\"UTF-8\" src=\"https://maps.google.com/maps-api-v3/api/js/40/2a/common.js\"></script><script type=\"text/javascript\" charset=\"UTF-8\" src=\"https://maps.google.com/maps-api-v3/api/js/40/2a/util.js\"></script><script type=\"text/javascript\" charset=\"UTF-8\" src=\"https://maps.googleapis.com/maps/api/js/AuthenticationService.Authenticate?1shttps%3A%2F%2Fps.is.tuebingen.mpg.de%2Fperson%2Fblack&amp;4sAIzaSyCeKJQacsNJKGU9wfdBV47C8-zFarcIGhg&amp;callback=_xdc_._fasflo&amp;key=AIzaSyCeKJQacsNJKGU9wfdBV47C8-zFarcIGhg&amp;token=24146\"></script></head>\n\n<body class=\"header  header-location default-link eupopup eupopup-bottom\"><div style=\"visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;\"><div id=\"MathJax_Hidden\"></div></div><div id=\"MathJax_Message\" style=\"display: none;\"></div>\n\n\t<div class=\"wrapper\">\n\t\t<!-- header -->\n\t\t\t<div id=\"headerColorBar\" class=\"header-color-bar header-color-bar-ps\" %=\"\"></div>\n\n\n<div id=\"unstickyheader\" class=\"unstickyheader-signed-out\">\n\n\t<div class=\"topbar\">\n\t\t<div class=\"container\">\n\t\t\t<div class=\"header-image-container\">\n\t\t\t\t<a href=\"https://ps.is.tuebingen.mpg.de\"><img class=\"header-image\" src=\"/assets/header/header_logo_is_ps-ecca0972a1365961b0ccfc4ad4059734.png\" alt=\"Header logo is ps\"></a>\n\t\t\t</div>\n\n\t\t\t<div class=\"pull-right\">\n\t\t\t\t\n\t\t\t\t<div class=\"social-icons-header-container hidden-xs\">\n\t\t\t\t\t<ul class=\"social-icons pull-right\">\n\n\t\t\t\t\t\t\t\t<!-- IW-594 -->\n\t\t\t\t\t\t\t\t<li><a href=\"https://www.facebook.com/PerceivingSystems\" data-original-title=\"Facebook\" class=\"rounded-x custom-social-facebook\" target=\"_blank\"></a></li>\n\t\t\t\t\t\t\t\t<li><a href=\"https://twitter.com/PerceivingSys\" data-original-title=\"Twitter\" class=\"rounded-x custom-social-twitter\" target=\"_blank\"></a></li>\n\t\t\t\t\t\t\t\t<li><a href=\"https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw\" data-original-title=\"Youtube\" class=\"rounded-x custom-social-youtube\" target=\"_blank\"></a></li>\n\t\t\t\t\t</ul>\n\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div>\n</div>\n</div>\n\n\t<div id=\"stickyheader\">\t\t\n\n\t\t\t<div class=\"header-lower\">\n\t\t\t\t\n\t\t\t\t<div class=\"navbar navbar-default custom-navbar-default mega-menu\" role=\"navigation\">\n\t\t\t\t\t<div class=\"container\">\n\n\t\t\t\t\t\t<div class=\"navbar-header\">\n\t\t\t\t\t\t\t<button type=\"button\" class=\"navbar-toggle custom-navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-responsive-collapse\">\n\t\t\t\t\t\t\t\t<span class=\"sr-only\">Toggle navigation</span>\n\t\t\t\t\t\t\t\t<span class=\"fa fa-bars\"></span>\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</div>\n\n\t\t\t\t\t\t<div class=\"collapse navbar-collapse custom-navbar-collapse navbar-responsive-collapse\">\n\t\t\t\t\t\t\t<ul id=\"mainNav\" class=\"nav navbar-nav custom-navbar-nav overflow\">\n\n\t\t\t\t\t\t\t\t<li class=\"dropdown\"><a href=\"https://ps.is.tuebingen.mpg.de\">Perceiving Systems</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li class=\"sperate-link-toc-item-bottom\"><a href=\"https://is.mpg.de\">Institute Home</a></li><li><a href=\"https://am.is.tuebingen.mpg.de\">Autonomous Motion</a></li><li><a href=\"https://ei.is.tuebingen.mpg.de\">Empirical Inference</a></li><li><a href=\"https://hi.is.mpg.de\">Haptic Intelligence</a></li><li><a href=\"https://mms.is.mpg.de\">Modern Magnetic Systems</a></li><li><a href=\"https://ps.is.tuebingen.mpg.de\">Perceiving Systems</a></li><li><a href=\"https://pi.is.mpg.de\">Physical Intelligence</a></li><li class=\"sperate-link-toc-item-top\" '=\"\"><a href=\"https://avg.is.tuebingen.mpg.de\">Autonomous Vision</a></li><li class=\"\" '=\"\"><a href=\"https://al.is.tuebingen.mpg.de\">Autonomous Learning</a></li><li class=\"\" '=\"\"><a href=\"https://dlg.is.mpg.de\">Dynamic Locomotion</a></li><li class=\"\" '=\"\"><a href=\"https://ev.is.tuebingen.mpg.de\">Embodied Vision</a></li><li class=\"\" '=\"\"><a href=\"https://ics.is.mpg.de\">Intelligent Control Systems</a></li><li class=\"\" '=\"\"><a href=\"https://bio.is.mpg.de\">Locomotion in Biorobotic and Somatic Systems</a></li><li class=\"\" '=\"\"><a href=\"https://pf.is.mpg.de\">Micro, Nano, and Molecular Systems</a></li><li class=\"\" '=\"\"><a href=\"https://mg.is.tuebingen.mpg.de\">Movement Generation and Control</a></li><li class=\"\" '=\"\"><a href=\"https://pio.is.tuebingen.mpg.de\">Physics for Inference and Optimization</a></li><li class=\"\" '=\"\"><a href=\"https://plg.is.tuebingen.mpg.de\">Probabilistic Learning Group</a></li><li class=\"\" '=\"\"><a href=\"https://pn.is.tuebingen.mpg.de\">Probabilistic Numerics</a></li><li class=\"\" '=\"\"><a href=\"https://re.is.tuebingen.mpg.de\">Rationality Enhancement</a></li><li class=\"\" '=\"\"><a href=\"https://slt.is.tuebingen.mpg.de\">Statistical Learning Theory</a></li></ul></li><li class=\"dropdown\"><a class=\"dropdown-toggle\" href=\"/research\">Research</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li class=\"sperate-link-toc-item-bottom\"><a href=\"/research\">Overview</a></li><li><a href=\"/research_fields/virtual-humans\">Virtual Humans and Animals</a></li><li><a href=\"/research_fields/seeing-understanding-people\">Seeing and Understanding People</a></li><li><a href=\"/research_fields/understanding-scenes\">Scenes, Structure and Motion</a></li><li><a href=\"/research_fields/behavior-goals-action\">Behavior, Goals and Action</a></li><li><a href=\"/research_fields/0-learning-and-inference\">Learning and Inference</a></li><li><a href=\"/research_fields/beyond-mocap\">Beyond Mocap</a></li><li><a href=\"/research_fields/medicine-and-psychology\">Human Health</a></li><li><a href=\"/research_fields/datasets-and-code\">Datasets and Code</a></li><li><a href=\"/research_fields/robot-perception-group\">Robot Perception Group</a></li><li><a href=\"/research_fields/holistic-vision-group\">Holistic Vision Group</a></li><li><a href=\"/research_fields/completed\">Completed</a></li></ul></li><li><a href=\"/publications\">Publications</a></li><li><a href=\"/code\">Code/Data</a></li><li><a href=\"/people\">People</a></li><li class=\"dropdown\"><a href=\"#\" class=\"dropdown-toggle\" data-toggle=\"dropdown\">News &amp; Events</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li><a href=\"/news\">News</a></li><li><a href=\"/events\">Events</a></li><li><a href=\"/talks\">Talks</a></li><li><a href=\"/awards\">Awards</a></li></ul></li><li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"/pages/facilities\">Facilities</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li><a href=\"/pages/4d-capture\">4D Scanner</a></li><li><a href=\"/pages/4d-dynamic-face-scanner\">4D Face Scanner</a></li><li><a href=\"/pages/4d-foot-scanner\">4D Foot Scanner</a></li><li><a href=\"/pages/3dcapture\">3D Scanner</a></li><li><a href=\"/pages/motion-capture\">Vicon Motion Capture</a></li><li><a href=\"/pages/inertial-motion-capture\">Inertial Motion Capture </a></li><li><a href=\"/pages/outdoor-aerial-motion-capture-system\">Outdoor Motion Capture System</a></li><li><a href=\"/pages/video-capture\">Video Capture</a></li><li><a href=\"/pages/computing\">Computing</a></li><li><a href=\"/pages/espresso\">Espresso</a></li></ul></li><li></li><li class=\"dropdown\"><a href=\"#\" class=\"dropdown-toggle\" data-toggle=\"dropdown\">Career</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li><a href=\"/why\">Why MPI</a></li><li><a href=\"/jobs\">Current Jobs</a></li><li><a href=\"/jobs/phd\">Ph.D Applicants</a></li><li><a href=\"/jobs/undergraduate\">Undergraduate Interns</a></li><li class=\"sperate-link-toc-item-top\"><a href=\"/jobs/participate\">Trials <small class=\"text-muted-custom\">(Participants Wanted)</small></a></li><li class=\"sperate-link-toc-item-top\"><a href=\"/jobs/art\">Art <small class=\"text-muted-custom\">(Artist in Residence)</small></a></li></ul></li><li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"/pages/about\">About</a><ul class=\"dropdown-menu dropdown-menu-ps\"><li><a href=\"/department\">Department</a></li><li><a href=\"/principles\">Principles</a></li><li><a href=\"/pages/startups\">Startups</a></li></ul></li><li><a href=\"/contact\">Contact</a></li>\n\t\t\t\t\t\t\t</ul>\n\n\t\t\t\t\t\t</div>\n\n\t\t\t\t\t</div>\n\t\t\t\t</div>    \n\t\t\t</div>      \n\t\t</div>\n\n\t\t<!-- department hover header image (homepage) -->\n\t\t\n\t\t<!-- sliders -->\n\n\t\t<!-- search pages -->\n\n\t\t<!-- employee headers -->\n\t\t\t\n<div class=\"hidden-xs\">\n\t<div class=\"breadcrumbs t-header\">\n\t\t<div class=\"container\">\n\t\t\t<h1 class=\"pull-left\"></h1>\n\t\t</div>\n\t</div>\n\t<div id=\"sticky-t-toc\" class=\"t-toc \">\n\t</div>\n</div>\n\n\n\n\n\t\t<!-- why headers -->\n\n\t\t<!-- job headers -->\n\n\t\t<!-- construction header -->\n\n\t\t<!-- purchase styled header -->\n\n\t\t<!-- user sign in -->\n\n\n\t\t<!-- institute contact map -->\n\n\t\t<!-- content -->\n\t\t\t\t<div class=\"container custom-container\">\n\t\t\t\t\t<!-- DEVISE -->\n<!-- \n\t3. Ensure you have flash messages in app/views/layouts/application.html.erb. For example:\n\t\t<p class=\"notice\"></p>\n\t\t<p class=\"alert\"></p>\n-->\n\n\n\n\t\t\t\t\t\n\n\n\n<div>\n\t<div class=\"content t-content profile-content\">\n  <div class=\"row\">\n    <div class=\"col-md-3 t-profile-image-container\">\n      <img class=\"img-responsive img-bordered t-img-bordered t-employee-image\" alt=\"Michael Black\" src=\"/uploads/employee/image/18/Ports_160922_1261headcrop2.jpg\">\n\n      \n      <h1 class=\"page-h1-h3-title margin-bottom-5\">Michael Black<br></h1> \n\n      <!-- <small class=\"t-sub-employee-title\"> -->\n      <span class=\"text-muted\">\n              Director\n      </span>  \n      <p>\n       \n      </p>\n      <p>\n              \n                </p><div class=\"custom-table-row\">\n\n                    <div class=\"custom-table-cell t-employee-details-table-cell\">\n                      <i class=\"fa fa-circle departmentBullet color-ps\"></i>\n                    </div>\n                  <div class=\"custom-table-cell t-employee-details-table-cell\">\n           \n                      <a href=\"https://ps.is.tuebingen.mpg.de\">Perceiving Systems</a>\n                  </div>\n                </div>\n      <p></p>\n      <p>\n        </p><div class=\"custom-table-row\">\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-map-marker\"></i>\n          </div>\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            \n            Office: N3.019<br>\n            Max-Planck-Ring 4<br>\n\n            72076\n            T\u00fcbingen\n\n            <br>\n\n            <!-- IW-214 -->\n            Germany\n\n          </div>\n        </div>\n      <p></p>\n\n      <!-- IW-565 -->\n\n\n      <p>\n        </p><div class=\"custom-table-row\">\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-phone\"></i>\n          </div>\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            +49 7071 601 1801\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-envelope\"></i>\n          </div>\n          <div class=\"custom-table-cell\">\n            <a href=\"mailto:black@tue.mpg.de\">black@tue.mpg.de</a><script id=\"mail_to-sc0wwjpv\">eval(decodeURIComponent('%76%61%72%20%73%63%72%69%70%74%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%67%65%74%45%6c%65%6d%65%6e%74%42%79%49%64%28%27%6d%61%69%6c%5f%74%6f%2d%73%63%30%77%77%6a%70%76%27%29%3b%76%61%72%20%61%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%45%6c%65%6d%65%6e%74%28%27%61%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%68%72%65%66%27%2c%20%27%6d%61%69%6c%74%6f%3a%62%6c%61%63%6b%40%74%75%65%2e%6d%70%67%2e%64%65%27%29%3b%61%2e%61%70%70%65%6e%64%43%68%69%6c%64%28%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%54%65%78%74%4e%6f%64%65%28%27%62%6c%61%63%6b%40%74%75%65%2e%6d%70%67%2e%64%65%27%29%29%3b%73%63%72%69%70%74%2e%70%61%72%65%6e%74%4e%6f%64%65%2e%69%6e%73%65%72%74%42%65%66%6f%72%65%28%61%2c%73%63%72%69%70%74%29%3b'))</script>\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-skype\"></i>\n          </div>\n          <div class=\"custom-table-cell\">\n            michael_j_black\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-globe\"></i>\n          </div>\n          <div class=\"custom-table-cell t-employee-details-table-cell\">\n            <a target=\"_blank\" href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Black%2C+M+J\">https://arxiv.org/search/cs?sear...</a>\n          </div>\n        </div>\n      <p></p>\n      <hr class=\"employeeShowHr\">\n      <p>\n        </p><div class=\"custom-table-row\">\n          <div class=\"custom-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-linkedin\"></i>\n          </div>\n          <div class=\"custom-table-cell\">\n            <a target=\"_blank\" href=\"https://de.linkedin.com/in/michael-black-41751114\">Find me on Linkedin</a>\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-google\"></i>  \n          </div>\n          <div class=\"custom-table-cell\">\n            <a target=\"_blank\" href=\"http://scholar.google.com/citations?hl=en&amp;user=6NjbexEAAAAJ&amp;oi=sra\">Find me on Google Scholar</a>\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-github\"></i>  \n          </div>\n          <div class=\"custom-table-cell\">\n            <a target=\"_blank\" href=\"https://github.com/MichaelJBlack\">GitHub Repository</a>\n          </div>\n        </div>\n        <div class=\"custom-table-row\">\n          <div class=\"custom-table-cell\">\n            <i class=\"profile-table-icons users-table-row-icon fa fa-twitter\"></i>\n          </div>\n          <div class=\"custom-table-cell\">\n            <a target=\"_blank\" href=\"https://twitter.com/Michael_J_Black\">Follow me on Twitter</a>\n          </div>\n        </div>\n      <p></p>\n      <p>\n      </p>\n    </div>\n    <div class=\"col-md-9 t-employee-toc-container\">\n\n      <!-- TABS - HEADER -->\n      \n      <ul id=\"employee_tabs\" class=\"nav nav-pills tabs-ps\" role=\"tablist\">\n\n        <!-- OVERVIEW -->\n        \n        <!-- DYNMAIC TABS -->\n        <li class=\"active\"><a data-toggle=\"tab\" role=\"tab\" href=\"#biography\">Biography</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#research\">Research</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#students\">Students</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#postdocs\">PostDocs</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#data\">Data</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#code\">Code</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#affiliations\">Affiliations</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#contact\">Contact</a></li>\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#talks\">Talks</a></li>\n        \n        <!-- VIDEOS -->\n\n        <!-- PROJECTS -->\n\n        <!-- PUBLICATIONS -->\n        <li class=\"\"><a data-toggle=\"tab\" role=\"tab\" href=\"#publications\">Publications</a></li>\n      </ul>\n      <!-- TABS - CONTENT -->\n      \n      <div id=\"custom-link-container\">\n        <div class=\"tab-v1\">\n          <div class=\"tab-content t-tab-container\">\n            <div id=\"employee_tab_content\" class=\"tab-content\">\n              <!-- TABS -->   \n              <!-- PROFILE/OVERVIEW -->\n\n              <!-- DYNAMIC TABS -->\n              <div id=\"biography\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul active\">\n                <p></p><p>&nbsp;</p>\n\n<h4>Curriculum Vitae</h4>\n\n<p style=\"margin-left:40px\"><a href=\"http://files.is.tue.mpg.de/black/resume.pdf\">[pdf]</a></p>\n\n<h4>Conflict of Interest Disclosure</h4>\n\n<p style=\"margin-left:40px\">This includes any coporate activity within the last 5 years involving more than $5000 where I have a personal or professional interest or any role in which I have corporate responsibility.</p>\n\n<ul style=\"margin-left:40px\">\n\t<li>Corporate research funding (unrestricted): Intel, NVIDIA, Adobe, Facebook, and Amazon.</li>\n\t<li>Financial interests (stock): Amazon, Meshcapade.</li>\n\t<li>Commercial licensing of MPI technology where I am a co-inventor.</li>\n\t<li>Side employment: Amazon (20%, current), Body Labs Inc (20%, 2013-2017).</li>\n\t<li>Corporate boards: Body Labs Inc. (2013-2017).</li>\n</ul>\n\n<h4>Citations</h4>\n\n<p style=\"margin-left:40px\"><a href=\"http://scholar.google.com/citations?hl=en&amp;user=6NjbexEAAAAJ&amp;oi=sra\">Google Scholar citations</a></p>\n\n<p style=\"margin-left:40px\"><a href=\"https://www.researchgate.net/profile/Michael_Black6\">Research Gate</a></p>\n\n<p style=\"margin-left:40px\"><a href=\"https://www.semanticscholar.org/author/Michael-J.-Black/2105795\">Semantic Scholar</a></p>\n\n<p style=\"margin-left:40px\"><a href=\"https://dblp.uni-trier.de/pers/hd/b/Black:Michael_J=\">DBLP</a></p>\n\n<h4>Biography</h4>\n\n<p style=\"margin-left:40px\">Michael J. Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. in computer science from Yale University (1992). After research at NASA Ames and post-doctoral research at the University of Toronto, he joined the Xerox Palo Alto Research Center in 1993 where he later managed the Image Understanding Area and founded the Digital Video Analysis group. From 2000 to 2010 he was on the faculty of Brown University in the Department of Computer Science (Assoc. Prof. 2000-2004, Prof. 2004-2010). He is a founding director at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany, where he leads the Perceiving Systems department.&nbsp; He is also a Distinguished Amazon Scholar (VP),&nbsp;an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University.</p>\n\n<p style=\"margin-left:40px\">Black is a foreign member of the Royal Swedish Academy of Sciences. He is a recipient of the 2010 Koenderink Prize for Fundamental Contributions in Computer Vision and the 2013 Helmholtz Prize for work that has stood the test of time. His work has won several paper awards including the IEEE Computer Society Outstanding Paper Award (CVPR'91). His work received Honorable Mention for the Marr Prize in 1999 and 2005. His early work on optical flow has been widely used in Hollywood films including for the Academy-Award-winning effects in \u201cWhat Dreams May Come\u201d and \u201cThe Matrix Reloaded.\u201d He has contributed to several influential datasets including the <a href=\"http://vision.middlebury.edu/flow/\">Middlebury Flow dataset</a>, <a href=\"http://humaneva.is.tue.mpg.de/\">HumanEva</a>, and the <a href=\"http://sintel.is.tue.mpg.de/\">Sintel dataset</a>. Black has coauthored over 200 peer-reviewed scientific publications.</p>\n\n<p style=\"margin-left:40px\">He is also active in commercializing scientific results, is&nbsp;an inventor on 10 issued&nbsp;patents, and has advised multiple startups. He uniquely combines computer vision, graphics, and machine learning to solve problems in the clothing industry.&nbsp;In 2013, he&nbsp;co-founded&nbsp;Body Labs Inc., which used computer vision, machine&nbsp;learning, and graphics technology licensed from his lab to commercialize \"the body as a digital platform.\"&nbsp; Body Labs was&nbsp;acquired by Amazon&nbsp;in 2017.</p>\n\n<p style=\"margin-left:40px\">Black's research interests in machine vision include optical flow estimation, 3D shape models, human shape and motion analysis, robust statistical methods, and probabilistic models of the visual world. In computational neuroscience his work focuses on probabilistic models of the neural code and applications of neural decoding in neural prosthetics.</p>\n\n<h4>Short version</h4>\n\n<p style=\"margin-left:40px\">Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). After post-doctoral research at the University of Toronto, he worked at Xerox PARC as a member of research staff and area manager. From 2000 to 2010 he was on the faculty of Brown University in the Department of Computer Science (Assoc. Prof. 2000-2004, Prof. 2004-2010). He is one of the founding directors at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany, where he leads the Perceiving Systems department.&nbsp;He is also a Distinguished Amazon Scholar (VP),&nbsp;an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize for Fundamental Contributions in Computer Vision, and the 2013 Helmholtz Prize for work that has stood the test of time. He is a foreign member of the Royal Swedish Academy of Sciences. In 2013 he&nbsp;co-founded&nbsp;Body Labs Inc., which was acquired by Amazon&nbsp;in 2017.</p>\n\n<h4>Even shorter version</h4>\n\n<p style=\"margin-left:40px\">Michael Black received his B.Sc. from the University of British Columbia (1985), his M.S. from Stanford (1989), and his Ph.D. from Yale University (1992). He has held positions at the University of Toronto, Xerox PARC, and Brown Unviversity. He is one of the founding directors at the Max Planck Institute for Intelligent Systems in T\u00fcbingen, Germany, where he leads the Perceiving Systems department. He is a Distinguished Amazon Scholar,&nbsp;an Honorarprofessor at the University of Tuebingen, and Adjunct Professor at Brown University. His work has won several awards including the IEEE Computer Society Outstanding Paper Award (1991), Honorable Mention for the Marr Prize (1999 and 2005), the 2010 Koenderink Prize, and the 2013 Helmholtz Prize. He is a foreign member of the Royal Swedish Academy of Sciences. In 2013 he&nbsp;co-founded&nbsp;Body Labs Inc., which was acquired by Amazon&nbsp;in 2017.</p>\n\n<h4>Head shot</h4>\n\n<p style=\"margin-left:40px\"><a href=\"http://files.is.tue.mpg.de/black/images/headShot2016.jpg\">[png]</a></p>\n\n<h4>Education</h4>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.yale.edu\" target=\"_blank\"><strong>Yale University, </strong></a> <em>New Haven, CT</em><br>\nPh.D., Computer Science, 1992.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.stanford.edu\" target=\"_blank\"><strong>Stanford University,</strong> </a> <em>Stanford, CA</em><br>\nM.S., Computer Science, 1989.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.ubc.ca\" target=\"_blank\"><strong>The University of British Columbia, </strong></a> <em>Vancouver, BC</em><br>\nB.Sc., Honours Computer Science, 1985.</p>\n\n<h4>Selected Awards/Honors</h4>\n\n<p style=\"margin-left:40px\"><strong>Alumni Research Award</strong><br>\nUniversity of British Columbia, Department of Computer Science, 2018.</p>\n\n<p style=\"margin-left:40px\"><strong>Royal Swedish Academy of Sciences</strong><br>\nForeign member, Class for Engineering Sciences, since June 2015.</p>\n\n<p style=\"margin-left:40px\"><strong>2013</strong>&nbsp;<strong>Helmholtz Prize</strong><br>\nfor the paper: Black, M. J., and Anandan, P., \"A framework for the robust estimation of optical flow,''&nbsp;IEEE International Conference on Computer Vision, ICCV<span style=\"color:#000000\">, pages 231-236, Berlin, Germany. May 1993.</span></p>\n\n<p style=\"margin-left:40px\"><strong>2010</strong>&nbsp;<strong>Koenderink Prize for Fundamental Contributions in Computer Vision,</strong><br>\nwith Sidenbladh, H.&nbsp;and Fleet, D. J. for the paper&nbsp;\"Stochastic tracking of 3D human figures using 2D image motion,''&nbsp;European Conference on Computer Vision, 2000.&nbsp;</p>\n\n<p style=\"margin-left:40px\"><strong>Best Paper Award, Eurographics 2017,</strong> for the paper&nbsp;\"Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs\", by von Marcard, T.,&nbsp;Rosenhahn, B.,&nbsp; Black, M. J.,&nbsp;Pons-Moll, G.</p>\n\n<p style=\"margin-left:40px\"><strong><span style=\"background-color:#ffffff\">\"Dataset Award\" at the Eurographics Symposium on Geometry Processing 2016</span></strong><span style=\"background-color:#ffffff\"><strong>,</strong></span><span style=\"background-color:#ffffff; color:#555555\"> with </span>F. Bogo, J. Romero, and M. Loper<strong>, </strong><span style=\"background-color:#ffffff; color:#555555\">&nbsp;for the paper \"FAUST: Dataset and evaluation for 3D&nbsp;mesh registration,\" CVPR 2014.&nbsp;</span></p>\n\n<p style=\"margin-left:40px\"><span style=\"background-color:#ffffff; color:#555555\"><strong>Best Paper Award, International Conference on 3D Vision (3DV), 2015,</strong>&nbsp;with&nbsp;</span>A.&nbsp;O.&nbsp;Ulusoy and&nbsp;A.&nbsp;Geiger<strong>,&nbsp;</strong><span style=\"background-color:#ffffff; color:#555555\">&nbsp;for the paper \"Towards Probabilistic Volumetric Reconstruction using Ray Potentials.\"</span></p>\n\n<p style=\"margin-left:40px\"><strong>Best Paper Award, INI-Graphics Net, 2008, First Prize Winner of Category Research,</strong><br>\nwith S. Roth for the paper &nbsp;\"Steerable random fields.\"</p>\n\n<p style=\"margin-left:40px\"><strong>Best Paper Award,</strong> Fourth International Conference on&nbsp;Articulated Motion and Deformable Objects (AMDO-e 2006),&nbsp;with L. Sigal for the paper \"Predicting 3D people from 2D pictures.''</p>\n\n<p style=\"margin-left:40px\"><strong>Marr Prize, Honorable Mention</strong>, Int. Conf. on Computer&nbsp;Vision, ICCV-2005, Beijing, China, Oct. 2005&nbsp;with S. Roth for the paper \"On the spatial statistics of optical flow.''</p>\n\n<p style=\"margin-left:40px\"><strong>Marr Prize, Honorable Mention</strong>, Int. Conf. on Computer&nbsp;Vision, ICCV-99, Corfu, Greece, Sept. 1999&nbsp;with D. J. Fleet for the paper \"Probabilistic detection and tracking of motion discontinuities.''</p>\n\n<p style=\"margin-left:40px\"><strong>IEEE Computer Society, Outstanding Paper Award,&nbsp;</strong>Conference on Computer Vision and Pattern Recognition,&nbsp;Maui, Hawaii, June 1991&nbsp;with P. Anandan for the paper \"Robust dynamic motion estimation over time.''</p>\n\n<p style=\"margin-left:40px\"><strong>Commendation and Chief's Award</strong>, Henrico County Division of Police,<br>\nCounty of Henrico, Virginia, April 19, 2007.</p>\n\n<p style=\"margin-left:40px\"><strong>University of Maryland, Invention of the Year, 1995</strong>,&nbsp;\"Tracking and Recognizing Facial Expressions,''&nbsp;with Y. Yacoob.</p>\n\n<p style=\"margin-left:40px\">University of Toronto, <strong>Computer Science Students' Union Teaching Award</strong> for 1992-1993.</p>\n\n<h4>Employment and Positions Held</h4>\n\n<p style=\"margin-left:40px\"><strong><a href=\"http://is.mpg.de\">Max Planck Institute for Intelligent Systems</a></strong><br>\n<em>T\u00fcbingen, Germany</em><br>\nDirector, 1/11 - present<br>\nManaging Director, 2/13 - 6/15, 3/18 - 11/18</p>\n\n<p style=\"margin-left:40px\"><strong>Amazon</strong><br>\n<em>T\u00fcbingen, Germany</em><br>\nDistinguished Amazon Scholar, 11/17 - present</p>\n\n<p style=\"margin-left:40px\"><strong><a href=\"http://www.uni-tuebingen.de/en/faculties/faculty-of-science/departments/computer-science/department.html\" target=\"_top\">Eberhard Karls Universit\u00e4t T\u00fcbingen, Faculty of Science, Department of Computer Science</a></strong><br>\n<em>T\u00fcbingen, Germany</em><br>\nHonorarprofessor, 05/22/12 - present</p>\n\n<p style=\"margin-left:40px\"><strong><a href=\"http://bodylabs.com\">B</a></strong><a href=\"http://bodylabs.com\"><strong>ody Labs Inc.</strong></a><br>\n<em>New York, NY, USA</em><br>\nCo-founder, Science Advisor, Member of the Board, 01/13&nbsp;- 10/2017</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.ee.ethz.ch/\"><strong>ETH Z\u00fcrich, Dept. of Information Technology and Electrical Engineering</strong></a><br>\n<em>Z\u00fcrich, Switzerland</em><br>\nVisting Professor, 04/2014 - 04/2016</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://ee.stanford.edu/\" target=\"_top\"><strong>Stanford University, Electrical Engineering</strong></a><br>\n<em>Stanford, CA</em><br>\nVisiting Professor, 5/11-4/12, 7/12-7/13</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu\" target=\"_blank\"><strong>Brown University, Department of Computer Science,</strong> </a><br>\n<em>Providence, RI</em><br>\nAdjunct Professor (Research), 1/11-present<br>\nProfessor, 7/04-12/10<br>\nAssociate Professor, 7/00-6/04</p>\n\n<p style=\"margin-left:80px\">My research addressed&nbsp;the problem of estimating and explaining motion in image sequences. I developed methods detecting and tracking 2D and&nbsp;3D human motion including the introduction of particle filtering for 3D human tracking and belief propagation for 3D human pose estimation. &nbsp;I worked on&nbsp;probabilistic models of images include the&nbsp;high-order Field of Experts model. &nbsp;I worked on 3D human shape estimation from images and video and developed applications of this technology. &nbsp;I also developed mathematical models for decoding neural signals. &nbsp;This included the first uses of particle filtering and Kalman filtering for decoding motor cortical neural activity and the first point-and-click cortical neural brain-machine-interface for people with paralysis.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.parc.xerox.com\" target=\"_blank\"><strong>Xerox Palo Alto Research Center,</strong></a><br>\n<em>Palo Alto, CA</em><br>\nArea Manager, Image Understanding Area, 1/96-7/00<br>\nMember of Research Staff, 9/93-12/95</p>\n\n<p style=\"margin-left:80px\">Research included&nbsp;modeling image changes (motion, illumination, specularity, occlusion, etc.) in video as a mixture&nbsp;of causes. I developed methods of&nbsp;motion <em>explanation; </em>that is, the extraction of mid-level or high-level concepts from motion.<em>&nbsp;</em>This included the modeling and recognition of motion \"features\"&nbsp;(occlusion boundaries, moving bars, etc.), human facial expressions and gestures, and motion \"texture\"&nbsp;(plants, fire, water, etc.). &nbsp;I applied these methods to problems in&nbsp;video indexing, motion for video annotation, teleconferencing, and gestural user interfaces. Other research included&nbsp;robust learning of image-based models, regularization with transparency, anisotropic diffusion, and the recovery of multiple shapes from transparent textures.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.toronto.edu\" target=\"_blank\"><strong>University of Toronto,</strong> </a><br>\n<em>Toronto, Ontario</em><br>\nAssistant Professor, Department of Computer Science, (8/92 - 9/93).</p>\n\n<p style=\"margin-left:80px\">Research included the application of mixture models to optical flow, detection and tracking of surface discontinuities using motion information, and robust surface recovery in dynamic environments.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.yale.edu\" target=\"_blank\"><strong>Yale University,</strong> </a> (9/89-8/92)<br>\n<em>New Haven, CT</em><br>\nResearch Assistant, Department of Computer Science.</p>\n\n<p style=\"margin-left:80px\">Research in the recovery of optical flow, incremental estimation, temporal continuity, applications of robust statistics to optical flow, the relationship between robust statistics and line processes, the early detection of motion discontinuities, and the role of representation in computer vision.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://vision.arc.nasa.gov\" target=\"_blank\"><strong>NASA Ames Research Center,</strong> </a> (6/90-8/92)<br>\n<em>Moffett Field, CA</em><br>\nVisiting Researcher, Aerospace Human Factors Research Division.</p>\n\n<p style=\"margin-left:80px\">Developed motion estimation algorithms in the context of an autonomous Mars landing and nap-of-the-earth helicopter flight and studied the psychophysical implications of a temporal continuity assumption.</p>\n\n<p style=\"margin-left:40px\"><strong>Advanced Decision Systems,</strong> (12/86-6/89)<br>\n<em>Mountain View, CA</em><br>\nComputer Scientist, Image Understanding Group.</p>\n\n<p style=\"margin-left:80px\">Research on spatial reasoning for robotic vehicle route planning and terrain analysis. Vision research including perceptual grouping, object-based translational motion processing, the integration of vision and control for an autonomous vehicle, object modeling using generalized cylinders, and the development of an object-oriented vision environment.</p>\n\n<p style=\"margin-left:40px\"><strong>GTE Government Systems,</strong> (6/85-12/86)<br>\n<em>Mountain View, CA</em><br>\nEngineer, Artificial Intelligence Group.</p>\n\n<p style=\"margin-left:80px\">Developed expert systems for multi-source data fusion and fault location.</p>\n\n<p style=\"margin-left:40px\"><strong>Miscellaneous,</strong> ('78-'85)</p>\n\n<p style=\"margin-left:80px\">Summer undergraduate researcher at UBC; park ranger's assistant; volunteer firefighter, busboy; and probably my worst job: cleaning dog kennels.</p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"research\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><h4>Research Interests</h4>\n\n<p>I am interested in motion. &nbsp;What does motion tell us about the structure of the world and how can we compute this from video? &nbsp;How do humans and animals move? &nbsp;How does the brain control complex movement? &nbsp;My work combines computer&nbsp;vision, graphics and neuroscience to develop new models and algorithms to capture and analyze the motion of the world.</p>\n\n<p>My <strong>Computer Vision</strong> research addresses:</p>\n\n<ul>\n\t<li>the estimation of scene structure and physical properties from video;</li>\n\t<li>articulated <a href=\"../../theme/Bodies_in_Motion\">human motion</a>&nbsp;pose estimation and tracking;</li>\n\t<li>the estimation of human <a href=\"../../theme/Body_Shape\">body shape </a>from images and video;</li>\n\t<li>the representation and detection of motion discontinuities;</li>\n\t<li>the estimation of <a href=\"../../theme/Scenes_in_Motion\">optical flow</a>;</li>\n\t<li>vision as inverse graphics.</li>\n</ul>\n\n<p>My <strong>Graphics&nbsp;research</strong> addresses:</p>\n\n<ul>\n\t<li>virtual humans;</li>\n\t<li>next-generation motion capture;</li>\n\t<li>articulated and non-rigid shape representation;</li>\n\t<li>human and animal shape and motion capture;</li>\n\t<li>human animation, AR/VR;</li>\n\t<li>capture and animation of clothing.</li>\n</ul>\n\n<p>My&nbsp;<strong>Computational Neuroscience research</strong>&nbsp;addresses:</p>\n\n<ul>\n\t<li>modeling the neural control of reaching and grasping;</li>\n\t<li>novel neural decoding algorithms;</li>\n\t<li>neural prostheses and cortical brain-machine interfaces;</li>\n\t<li>markless animal&nbsp;motion capture.</li>\n</ul>\n\n<p>I also work on industrial applications in <strong>Fashion Science</strong>:</p>\n\n<ul>\n\t<li>Body scanning and measurement;</li>\n\t<li>clothing sizing;</li>\n\t<li>cloth capture and modeling;</li>\n\t<li>virtual try-on.</li>\n</ul>\n\n<p>What is maybe unique about my work is the combination of the these themes. &nbsp;For example I study human motion from the inside (decoding neural activity in paralyzed humans) and the outside (with novel motion capture techniques). &nbsp;</p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"students\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><h3>Current PhD students:</h3>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/nathanasiou\">Nikos Athanasiou</a>, Max Planck Graduate Center for Computer and Information Science</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/yfeng\">Yao Feng</a>,&nbsp;MPI-ETH Center for Learning Systems, Co-supervised with Marc Pollefeys</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/mkeller2\">Marilyn Keller</a>,&nbsp;MPI for Intelligent Systems, Co-supervised with Sergi Pujades</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/lmueller2\">Lea M\u00fcller</a>, Int. Max Planck Research School, Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/mkocabas\">Muhammad Kocabas</a>,&nbsp;MPI-ETH Center for Learning Systems, Co-supervised with <a href=\"https://ait.ethz.ch/people/hilliges/\">Otmar Hilliges</a></p>\n\n<p style=\"margin-left:40px\"><a href=\"http://ps.is.tue.mpg.de/person/otaheri\">Omid Taheri,</a>&nbsp; MPI for Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tue.mpg.de/person/qma\">Qianli Ma</a>,&nbsp; Int. Max Planck Research School, Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/vchoutas\">Vassilis Choutas</a>,&nbsp; MPI-ETH Center for Learning Systems, Co-supervised with Luc van Gool</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tue.mpg.de/person/aosman\">Ahmed Osman</a>,&nbsp; Int. Max Planck Research School, Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/mhassan\">Mohamed Hassan</a>,&nbsp; MPI for Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/pghosh\">Partha Ghosh</a>,&nbsp; Int. Max Planck Research School, Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/ssanyal\">Soubhik&nbsp;Sanyal</a>,&nbsp; Int. Max Planck Research School, Intelligent Systems, T\u00fcbingen</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/nrueegg\">Nadine R\u00fcegg</a>, MPI-ETH Center for Learning Systems, Co-supervised with <a href=\"http://www.prs.igp.ethz.ch/content/specialinterest/baug/institute-igp/photogrammetry-and-remote-sensing/en/group/people/person-detail.html?persid=143986\">Konrad Schindler</a></p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/eprice\">Eric Price</a>, MPI for Intelligent Systems, T\u00fcbingen, Co-supervised with Aamir Ahmed</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://is.tuebingen.mpg.de/employees/yhuang2\">Yinghao Huang</a>, MPI for Intelligent Systems, T\u00fcbingen</p>\n\n<h3>Former PhD students:</h3>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/aranjan\">Anurag Ranjan</a>, Apple AI Research,<br>\nThesis: <a href=\"https://ps.is.tuebingen.mpg.de/publications/ranjan-thesis\">Towards Geometric Understanding of Motion</a>, University of T\u00fcbingen, Dec 2019</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ps.is.tuebingen.mpg.de/person/dcudeiro\">Daniel Cudeiro</a>,&nbsp; Deceased</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://ps.is.tuebingen.mpg.de/person/wulff\">Jonas Wulff</a>, Postdoctoral Researcher, MIT CSAIL<br>\nThesis: <a href=\"https://ps.is.tue.mpg.de/publications/wulff-2018-thesis\">Model-based Optical Flow: Layers, Learning, and Geometry</a>, University of T\u00fcbingen, April 2018</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://ps.is.tue.mpg.de/person/loper\">Matthew Loper</a>, Amazon,<br>\nThesis:&nbsp;<a href=\"https://ps.is.tuebingen.mpg.de/publications/loper-phd-2017\">Human Shape Estimation using Statistical Body Models</a>, University of T\u00fcbingen, May 2017</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu/people/zuffi/\" target=\"_blank\">Silvia Zuffi</a>, <span style=\"background-color:#ffffff\">Research Scientist, IMATI-CNR, Institute for Applied Mathematics and Information Technologies, Milan Italy</span><br>\n<span style=\"background-color:#fafafa; color:#2b2b2b\">Thesis:&nbsp;<a href=\"https://ps.is.tuebingen.mpg.de/uploads_file/attachment/attachment/199/thesis.pdf\">Shape Models of the Human Body for Distributed Inference</a>,&nbsp;</span><span style=\"background-color:#fafafa; color:#2b2b2b\">Brown University, May 2015</span></p>\n\n<p style=\"margin-left:40px\"><a href=\"http://ps.is.tuebingen.mpg.de/person/tsoli\">Aggeliki Tsoli</a>, Post doctoral researcher, FORTH Institute, Crete,<br>\nThesis:&nbsp;<em><a href=\"http://files.is.tue.mpg.de/atsoli/thesis.pdf\">Modeling the Human body in 3D: Data Registration and Human Shape Representation</a>,</em>&nbsp;Department of Computer Science, Brown University, May 2014</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://www.cs.bgu.ac.il/~orenfr/\" target=\"_blank\">Oren Freifeld</a>, Assistant Professor, Dept. of Computer Science, Ben-Gurion Univ., Israel,<br>\nThesis:&nbsp;<a href=\"http://www.dam.brown.edu/people/freifeld//phd/ThesisOrenFreifeld.pdf\"><em>Statistics on Manifolds with Applications to Modeling Shape Deformations</em></a>, Division of Applied Mathematics, Brown University, August 2013</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu/people/pguan/\" target=\"_blank\">Peng Guan</a>, Senior Software Engineer, Google,<br>\nThesis:&nbsp;<em><a href=\"http://www.cs.brown.edu/~pguan/publications/thesis.pdf\">Virtual Human Bodies with Clothing and Hair: From Images to Animation</a>,&nbsp;</em>&nbsp;Department of Computer Science, Brown University, December 2012</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://research.nvidia.com/users/deqing-sun\" target=\"_blank\">Deqing Sun</a>, Senior Research Scientist, NVIDIA Research,<br>\nThesis:&nbsp;<a href=\"http://www.cs.brown.edu/~dqsun/pubs/Deqing_Sun_dissertation.pdf\"><em>From Pixels to Layers: Joint Motion Estimation&nbsp;and Segmentation</em></a>, &nbsp;Department of Computer Science, Brown University, July 2012</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu/people/alb/\" target=\"_blank\">Alexandru Balan</a>, Xbox Incubation Researcher, Microsoft<br>\nThesis:&nbsp;<a href=\"http://www.cs.brown.edu/people/black/Papers/BalanThesis2010.pdf\"><em>Detailed Human Shape and Pose from Images</em></a>, Department of Computer Science, Brown University, May 2010</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu/people/ls/\" target=\"_blank\">Leonid Sigal,</a>&nbsp; Associate Professor of Computer Science, Univ. of British Columbia (UBC)<br>\nThesis:&nbsp;<a href=\"http://www.cs.brown.edu/~ls/Publications/Sigal_Thesis.pdf\"><em>Continuous-state graphical models for object localization, pose estimation and tracking</em></a>&nbsp;Department of Computer Science, Brown University, May 2008</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.gris.informatik.tu-darmstadt.de/~sroth/\" target=\"_blank\">Stefan Roth</a>, Professor, Dept. of Computer Science, TU Darmstadt<br>\nThesis:&nbsp;<a href=\"http://www.cs.brown.edu/people/roth/pubs/roth-dissertation.pdf\" target=\"_top\"><em>High-order Markov random fields for low-level vision</em></a>. Dept. of Computer Science, Brown University,<br>\nMay 2007 Winner of the&nbsp;<em>Joukowsky Family Foundation Outstanding Dissertation Award</em></p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.stat.columbia.edu/~fwood/\" target=\"_blank\">Frank Wood</a><a href=\"http://www.cs.brown.edu/people/fwood/\" target=\"_top\">,</a>&nbsp;Associate Professor of Computer Science, Univ. of British Columbia (UBC)<br>\nThesis:&nbsp;<em>Nonparametric Bayesian modeling of neural data.</em>&nbsp;Department of Computer Science, Brown University</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://web.itu.edu.tr/hulyayalcin/Home.htm\" target=\"_blank\">Hulya Yalcin</a>, Assistant Professor,&nbsp;<span style=\"color:#333333\">Department of Electronics and Communications Engineering,&nbsp;</span>Istanbul Technical University, Turkey<br>\nThesis:&nbsp;<em>Implicit models of moving and static surfaces</em>, Division of Engineering, Brown University, May 2004</p>\n\n<p style=\"margin-left:40px\"><a href=\"https://ani.stat.fsu.edu/~wwu/\" target=\"_blank\">Wei Wu</a>, Associate Professor, Dept. of Statistics, Florida State<br>\nThesis:&nbsp;<em>Statistical models of neural coding in motor cortex,&nbsp;</em>Division of Applied Math, Brown University. Co-supervised with David Mumford. May 2004.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.cmu.edu/~ftorre/\" target=\"_blank\">Fernando De la Torre</a><a href=\"http://www.salleurl.edu/~ftorre/\" target=\"_top\">,</a>&nbsp;Research Associate Professor, CMU and Facebook,<br>\nThesis:&nbsp;<em>Robust subspace learning for computer vision,</em>&nbsp;La Salle School of Engineering. Universitat Ramon Llull, Barcelona, Spain. Jan. 2002</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.csc.kth.se/~hedvig/\" target=\"_blank\">Hedvig Kjellstrom (nee Sidenbladh)</a><a href=\"http://www.nada.kth.se/~hedvig/index_en.html\" target=\"_top\">,</a>&nbsp;Professor&nbsp;of Comptuer Science, KTH, Sweden<br>\nThesis:&nbsp;<a href=\"http://www.nada.kth.se/~hedvig/publications/thesis.pdf\"><em>Probabilistic Tracking and Reconstruction of 3D Human Motion in Monocular Video Sequences.</em></a>&nbsp;Dept. of Numerical Analysis and Computer Science, KTH, Stockholm, Sweden 2001</p>\n\n<p style=\"margin-left:40px\">Shanon Ju<br>\nThesis:&nbsp;<a href=\"http://www.cs.brown.edu/people/black/Papers/juThesis.pdf\"><em>Estimating image motion in layers: The Skin and Bones model.</em></a>&nbsp;University of Toronto. Jan. 1999</p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"postdocs\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><h4>Post doctoral researchers:</h4>\n\n<ul>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/achandrasekaran\">Arjun Chandrasekaran</a>, MPI for Intelligent Systems, Jan 2020&nbsp;- present</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/jyang\">Jinlong Yang</a>, MPI for Intelligent Systems, July&nbsp;2019&nbsp;- present</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/chuang2\">Chun-Hao Paul Huang</a>, MPI for Intelligent Systems, June&nbsp;2019&nbsp;- present</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/dtzionas\">Dimitris Tzionas</a>, MPI for Intelligent Systems, Oct.&nbsp;2016&nbsp;- present</li>\n\t<li><a href=\"https://sites.google.com/site/bolkartt/\">Timo Bolkart</a>, MPI for Intelligent Systems, Oct.&nbsp;2016&nbsp;- present</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/aahmad\">Aamir Ahmad</a>, MPI for Intelligent Systems, Sept.&nbsp;2016&nbsp;- present</li>\n</ul>\n\n<h4>Former post doctoral Researchers:</h4>\n\n<ul>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/stang\">Siyu Tang</a>, Assistant Professor at ETH Z\u00fcrich.</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/spujades\">Sergi Pujades</a>, Associate Professor at&nbsp;Universit\u00e9 Grenoble Alpes.</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/aquiros\">Alejandra Quiros-Ramirez</a>, Postdoc, Dept. of Psychology, Univ. of Konstanz.</li>\n\t<li><a href=\"http://ps.is.tuebingen.mpg.de/person/sevilla\">Laura Sevilla</a>, Lecturer Reader in Image and Vision Computing, University of Edinburgh.</li>\n\t<li><a href=\"http://ps.is.tuebingen.mpg.de/person/ulusoy\">Ali Osman Ulusoy</a>,&nbsp; (jointly with A. Geiger), Microsoft.</li>\n\t<li><a href=\"http://ps.is.tue.mpg.de/person/kong\">Naejin Kong</a>,&nbsp;Samsung, Korea.</li>\n\t<li><a href=\"https://ps.is.tuebingen.mpg.de/person/fbogo\">Federica Bogo</a>, Microsoft Research, Zurich.</li>\n\t<li><a href=\"http://ps.is.tuebingen.mpg.de/person/pons-moll\">Gerard Pons-Moll,</a>&nbsp;Research Scientist,&nbsp;MPI for Informatik.</li>\n\t<li><a href=\"http://ps.is.tuebingen.mpg.de/person/akhter\">Ijaz Akhter</a>, postdoctoral researcher,&nbsp;Australia Nationa University, Canbera.</li>\n\t<li><a href=\"http://ps.is.tuebingen.mpg.de/person/zuffi\">Silvia Zuffi</a>,&nbsp;<span style=\"background-color:#ffffff; color:#555555\">R</span><span style=\"background-color:#ffffff; color:#555555\">esearch Scientist, IMATI-CNR, Institute for Applied Mathematics&nbsp;(Milano)</span><span style=\"background-color:#ffffff\">.</span></li>\n\t<li><a href=\"http://star.comnet.com.sg/yeosy.php?display=1\">Si Yong Yeo</a>, Scientist, A*Star, Singapore.</li>\n\t<li><a href=\"http://ps.is.tue.mpg.de/person/garcia%20cifuentes\">Cristina Garcia Cifuentes</a>,&nbsp;<strong>&nbsp;</strong>Amazon.</li>\n\t<li><a href=\"http://igm.univ-mlv.fr/~cwang/index.php\">Chaohui Wang</a>, A<span style=\"color:#444444\">ssistant Professor,&nbsp;</span>Laboratoire d'Informatique Gaspard Monge, Universit\u00e9 Paris-Est, Paris, France</li>\n\t<li><a href=\"http://ps.is.tue.mpg.de/person/hauberg\">S\u00f8ren Hauberg</a><a href=\"http://www2.compute.dtu.dk/~sohau/\">,</a>&nbsp;Associate Professor, Technical University of Denmark&nbsp;(DTU), Copenhagen.</li>\n\t<li><a href=\"http://ps.is.tue.mpg.de/person/jhuang\">Hueihan Jhuang</a>, industry, Taiwan.</li>\n\t<li><a href=\"http://ps.is.tue.mpg.de/person/romero\">Javier Romero</a>,&nbsp; Applied Scientist, Amazon.</li>\n\t<li><a href=\"http://ttic.uchicago.edu/~gregory/\" target=\"_blank\">Gregrory Shakhnarovich</a>, Associate Professor, Toyota Technological Institute at Chicago.&nbsp;</li>\n\t<li><a href=\"http://bci.unist.ac.kr/\" target=\"_blank\">Sung-Phil Kim</a>, Associate Professor, School of Design and Human Engineering, UNIST, Korea.</li>\n\t<li><a href=\"http://perso.telecom-bretagne.eu/ronanfablet/\" target=\"_blank\">Ronan Fablet</a>, Professor, Telecom Bretange.</li>\n</ul>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"data\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><h4>Datasets and evalautions</h4>\n\n<p>I belive that computer vision is advanced by careful evaluation and comparison. &nbsp;Consequently I have been involved in building several public datasets and evaluation websites.</p>\n\n<p>&nbsp;</p>\n\n<h4>FAUST human scan dataset and registration benchmark</h4>\n\n<p>&nbsp;</p>\n\n<p style=\"margin-left:40px\">3D human scans of multiple people in multiple poses with accurate ground truth correspondences:&nbsp;<a href=\"http://faust.is.tue.mpg.de/\" target=\"_blank\">FAUST site</a>.</p>\n\n<p>&nbsp;</p>\n\n<h4>MPI-Sintel optical flow dataset and evaluation</h4>\n\n<p>&nbsp;</p>\n\n<p style=\"margin-left:40px\">Optical flow benchmark based on the animate film Sintel: <a href=\"http://sintel.is.tue.mpg.de/\" target=\"_blank\">MPI-Sintel site</a>.</p>\n\n<p>&nbsp;</p>\n\n<h4>JHMDB: Joint-annotated Human Motion Database</h4>\n\n<p>&nbsp;</p>\n\n<p style=\"margin-left:40px\">Annotated videos for action recognition: <a href=\"http://jhmdb.is.tue.mpg.de/\" target=\"_blank\">JHMDB site</a>.</p>\n\n<p>&nbsp;</p>\n\n<h4>Middlebury Optical Flow Benchmark</h4>\n\n<p>&nbsp;</p>\n\n<p style=\"margin-left:40px\">Image sequences, ground truth flow, and evaluation are all available on the <a href=\"http://vision.middlebury.edu/flow/eval/\" target=\"_blank\">Middlebury Flow site</a>.</p>\n\n<p>&nbsp;</p>\n\n<h4>HumanEva</h4>\n\n<p>&nbsp;</p>\n\n<div>\n<p style=\"margin-left:40px\">Multi-camera imagery with ground truth 3D human pose: <a href=\"http://humaneva.is.tue.mpg.de/\">HumanEva&nbsp;site</a>.</p>\n&nbsp;\n\n<h4>Lee Walking Sequence</h4>\n\n<div>\n<p style=\"margin-left:40px\">Multi-camera imagery with ground truth 3D human pose. &nbsp;This predates HumanEva and the imagery is grayscale only. &nbsp; There is also software for partical filter tracking on the site.</p>\n</div>\n\n<p style=\"margin-left:40px\"><a href=\"http://cs.brown.edu/~ls/Software/index.html\">http://cs.brown.edu/~ls/Software/index.html</a></p>\n</div>\n\n<p>&nbsp;</p>\n\n<h4>Archival Image Sequences</h4>\n\n<p>&nbsp;</p>\n\n<p style=\"margin-left:40px\">My old Brown site has several image sequences used in my older publications. &nbsp;These include some classic sequences such as Yosemite, the Pepsi can, the SRI tree sequence, and the Flower Garden sequence.</p>\n\n<p style=\"margin-left:40px\"><a href=\"http://www.cs.brown.edu/~black/images.html\" target=\"_blank\">Data</a></p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"code\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p>\n</p><h4>Optical Flow Code (C and Matlab):</h4>\n<p style=\"margin-left: 40px; \">\n  <br>\n  </p><h5>1. The most recent and most accurate optical flow code in Matlab</h5>\n<p></p>\n<p style=\"margin-left: 80px; \">\n  <a href=\"http://www.cs.brown.edu/~dqsun/code/flow_code.zip\">Download</a>\n</p>\n<p style=\"margin-left: 40px; \">\n   This code is descrbed in\n</p>\n<p style=\"margin-left: 80px; \">\n <strong>A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles behind Them</strong><br>\n Sun, D., Roth, S., and Black, M.J.<br>\n  <em>International Journal of Computer Vision (IJCV)</em>, 106(2):115-137, 2014.<br>\n <a href=\"http://download.springer.com/static/pdf/371/art%253A10.1007%252Fs11263-013-0644-x.pdf?auth66=1390289264_f25a96b131a34103341f40170def7ef9&amp;ext=.pdf\">(pdf)</a>\n</p>\n<p style=\"margin-left: 80px; \">\n      <strong>Secrets of optical flow estimation and their principles</strong><br>\n      Sun, D., Roth, S., and Black, M. J.,<br>\n      <em>IEEE Conf. on Computer Vision and Pattern Recog</em>., CVPR, June 2010.<br>\n     <a href=\"http://www.cs.brown.edu/people/dqsun/pubs/cvpr_2010_flow.pdf\">(pdf)</a>\n</p>\n<p style=\"margin-left: 40px; \">\n   This method implements many of the currently best known techniques for accurate optical flow and was once ranked #1 on the Middlebury evaluation (June 2010).\n</p>\n<p style=\"margin-left: 40px; \">\n    The software is made available for research pupropses. Please read the<a href=\"http://www.cs.brown.edu/~black/BrownFlowCopyright2010.html\">&nbsp;copyright statement&nbsp;</a>and contact me for commerical licensing.\n</p>\n<p style=\"margin-left: 40px; \">\n <br>\n  </p><h5>2. Matlab implmentation of the Black and Anandan dense optical flow method</h5>\n The Matlab flow code is easier to use and more accurate than the original C code. The objective function being optimized is the same but the Matlab version uses more modern optimization methods:\n<p></p>\n<p style=\"margin-left: 80px; \">\n   <a href=\"http://www.cs.brown.edu/~dqsun/research/software.html\">Matlab implementation of Black and Anandan robust dense optical flow algorithm</a>\n</p>\n<p style=\"margin-left: 40px; \">\n The method in 1 above is more accurate and also implements Black and Anandan plus much more.\n</p>\n<p style=\"margin-left: 40px; \"> \n  <br>\n  </p><h5>3. Original Black and Anandan method implemented in C</h5>\n<p></p>\n<p style=\"margin-left: 40px; \">\n The optical flow software here has been used by a number of graphics companies to make special effects for movies.&nbsp; This software is provided for research purposes only; any sale or use for commercial purposes is strictly prohibited.&nbsp;\n</p>\n<p style=\"margin-left: 40px; \">\n   <a href=\"mailto:black@is.mpg.de?subject=Request%20for%20Black%20%26%20Anandan%20flow%20code%20for%20research%20purposes\">Contact me </a>for the password to download the software, stating that it is for research purposes.\n</p>\n<p style=\"margin-left: 40px; \">\n   Please contact me if you wish to use this code for commercial purpose.\n</p>\n<p style=\"margin-left: 40px; \">\n   If you are a commercial enterprise and would like assistance in using optical flow in your application, please contact me at my consulting address&nbsp;<a href=\"mailto:black@opticalflow.com\">black@opticalflow.com</a>.\n</p>\n<p style=\"margin-left: 40px; \">\n    This is EXPERIMENTAL software. It is provided to illustrate some ideas in the robust estimation of optical flow. Use at your own risk. No warranty is implied by this distribution.\n</p>\n<p style=\"margin-left: 40px; \">\n    <a href=\"http://www.cs.brown.edu/~black/copyrightNotice.htm\">Copyright notice.</a>\n</p>\n<p style=\"margin-left: 40px; \">\n   There are two versions available. First, the original C code implementing the robust flow methods described in Black and Anandan '96:\n</p>\n<p style=\"margin-left: 80px; \">\n    <a href=\"http://www.cs.brown.edu/~black/regression.html\">Area-based optical flow: robust affine regression.&nbsp;</a><br>\n   <a href=\"http://www.cs.brown.edu/~black/ignc.html\">Dense optical flow: robust regularization.</a>\n</p>\n<p style=\"margin-left: 40px; \">\n  Reference:\n</p>\n<p style=\"margin-left: 80px; \">\n   <b>The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</b>,<br>\n    Black, M. J. and Anandan, P.,<br>\n   <i>Computer Vision and Image Understanding, CVIU</i>, 63(1), pp. 75-104, Jan. 1996.<br>\n   <a href=\"http://www.cs.brown.edu/~black/Papers/cviu.63.1.1996.html\">(pdf),</a>&nbsp;<a href=\"http://www.sciencedirect.com/science?_ob=MImg&amp;_imagekey=B6WCX-45N4RPR-28-1&amp;_cdi=6750&amp;_user=489286&amp;_orig=search&amp;_coverDate=01/31/1996&amp;_sk=999369998&amp;view=c&amp;wchp=dGLbVzb-zSkWb&amp;md5=a6197e0c7956f5741c975ef9e81b0fb1&amp;ie=/sdarticle.pdf\" target=\"_blank\">(pdf from publisher)</a>\n</p>\n\n<br>\n<h4>Robust Principal Component Analysis (PCA)</h4>\n<p style=\"margin-left: 40px; \">\n Software is from the ICCV'2001 paper with Fernando De la Torre.\n</p>\n<p style=\"margin-left: 80px; \">\n  De la Torre, F. and Black, M. J.,&nbsp;<b>Robust principal component analysis for computer vision,</b>&nbsp;to appear:&nbsp;<i>Int. Conf. on Computer Vision, ICCV-2001,</i>&nbsp;Vancouver, BC.&nbsp;<a href=\"http://www.cs.brown.edu/~black/Papers/iccv01fdl-copyright.html\">(postscript, 1.0MB)(pdf, 0.36MB),</a>&nbsp;<a href=\"http://www.cs.brown.edu/~black/rpca.html\">(abstract)</a>\n</p>\n<p style=\"margin-left: 40px; \">\n  <a href=\"http://www.salleurl.edu/~ftorre/papers/rpca2.html\">Software, demos, and data.</a>\n</p>\n\n<br>\n<h4>Human motion tracking</h4>\n<p style=\"margin-left: 40px; \">\n The code below provides a simple Matlab implementation of the Bayesian 3D person tracking system described in ECCV'00 and ICCV'01. It is too slow to be used to track the entire body but can be used to track various limbs and provides a basis for people who want to understand the methods better and extend them.\n</p>\n<p style=\"margin-left: 80px; \">\n  <b>Learning image statistics for Bayesian tracking,</b><br>\n Sidenbladh, H. and Black, M. J.,<br>\n  <i>Int. Conf. on Computer Vision, ICCV-2001,</i>&nbsp;Vancouver, BC, Vol. II, pp. 709-716.<br>\n  <a href=\"http://www.cs.brown.edu/~black/Papers/iccv01hvg-copyright.html\">(postscript, 2.8MB)(pdf, 0.38MB),</a>&nbsp;<a href=\"http://www.cs.brown.edu/~black/hedvigICCV01.html\">(abstract)</a>\n</p>\n<p style=\"margin-left: 80px; \">\n  </p><p style=\"margin-left: 40px; \">\n   <b>Stochastic tracking of 3D human figures using 2D image motion,</b><br>\n   Sidenbladh, H., Black, M. J., and Fleet, D.J.,<br>\n    <i>European Conference on Computer Vision</i>, D. Vernon (Ed.), Springer Verlag, LNCS 1843, Dublin, Ireland, pp. 702-718 June 2000.<br>\n   <a href=\"http://www.cs.brown.edu/~black/Papers/eccv00-copyright.html\">(postscript)(pdf),&nbsp;</a><a href=\"http://www.cs.brown.edu/~black/eccv00.html\">(abstract)</a></p>\n<p></p>\n<p style=\"margin-left: 40px; \">\n  <a href=\"http://www.cs.brown.edu/~black/Software/SidenbladhMatlab.tar.gz\">Software</a>. (Note: if you uncompress and untar this on a PC using Winzip, the path names may be lost which will cause Matlab to fail when you load the .mat files.&nbsp; Instead uncompress/untar using gunzip and tar.)\n  </p><p></p>\n                <hr>\n              </div>\n              <div id=\"affiliations\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><div>\n <a href=\"http://www.cin.uni-tuebingen.de/\" target=\"_top\">Werner Reichardt Center for Integrative Neuroscience,</a> Eberhard Karls Universit\u00e4t T\u00fcbingen, Member since 2011.</div>\n<div>\n &nbsp;</div>\n<div>\n <a href=\"http://www.bccn-tuebingen.de/\" target=\"_top\">Bernstein Center for Computational Neuroscience</a>, T\u00fcbingen, since Jan. 2011.</div>\n<p>\n <a href=\"http://brainscience.brown.edu\" target=\"_top\">Brown Institute for Brain Science</a>&nbsp;(BIBS), Member</p>\n<p>\n  &nbsp;</p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"contact\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><p>\n  How to reach me:</p>\n<ul>\n  <li>\n    email:&nbsp;<a href=\"mailto:black@tue.mpg.de\">black@tue.mpg.de</a></li>\n <li>\n    Skype: michael_j_black</li>\n <li>\n    Phone:&nbsp;+49 7071 601-1801</li>\n  <li>\n    FAX:&nbsp;+49&nbsp;7071 601-1802</li>\n</ul>\n<p>\n Mailing address</p>\n<p style=\"margin-left: 40px; \">\n  Michael J. Black<br>\n  Max Planck Institute for Intelligent Systems<br>\n  Spemannstrasse 41<br>\n 72076 T\u00fcbingen<br>\n Germany</p>\n<p>\n  For more information including our address and directions, see the department <a href=\"../contact\">CONTACT</a> page.</p>\n<p>\n I receive more email than I can read, let alone respond to. I apologize if you do not get a response. If you do not hear from me, consider the following:</p>\n<ul>\n <li>\n    If you need something that is time sensitive (letter of reference, paper review, etc.), please contact or cc my assistant,&nbsp;<a href=\"mailto:melanie.feldhofer@is.mpg.de\">Melanie Feldhofer</a>&nbsp; (melanie.feldhofer@is.mpg.de)</li>\n <li>\n    If you have asked me to do something (like review a paper or grant proposal), and I haven't responded saying that I can do it, then I have not agreed to do it.&nbsp;</li>\n  <li>\n    <melanie.feldhofer@is.mpg.de></melanie.feldhofer@is.mpg.de>If you are seeking a job or want to be a PhD student,&nbsp;visit the <a href=\"../jobs\">CAREER</a>&nbsp;page</li>\n <li>\n    Applications for jobs or graduate school should be sent to&nbsp;<a href=\"mailto:ps-apply@tuebingen.mpg.de\">ps-apply@tuebingen.mpg.de</a></li>\n</ul>\n<p>\n My assistant reads mail sent to me at black@is.mpg.de. &nbsp;If you have something particularly private, you can email me at <a href=\"mailto:black@cs.brown.edu\">black@cs.brown.edu</a> and only I will read it.</p>\n<p>\n &nbsp;</p>\n<p></p>\n                <hr>\n              </div>\n              <div id=\"talks\" class=\"tab-pane custom-tab-pane fade in t-tags default-link-ul \">\n                <p></p><h3>Overview talks</h3>\n\n<p><strong>Estimating 3D people interacting with 3D scenes</strong><br>\nVarious workshops at ICCV&nbsp;2019 (<a href=\"http://files.is.tue.mpg.de/black/talks/ICCVworkshops2019web.pptx\">pptx</a>, 981MB)</p>\n\n<p><strong>Expressive human models for communication and interaction,</strong><br>\nVarious workshops at CVPR 2019 (<a href=\"http://files.is.tue.mpg.de/black/talks/CVPRworkshops2019web.pptx\">pptx</a>, 546MB)</p>\n\n<p><strong><strong>Estimating Human Motion: Past, Present, and Future. </strong></strong><em>(with full bibliography)</em><br>\n40 Years DAGM - Invited Talks, GCPR 2018 <strong><a href=\"http://files.is.tue.mpg.de/black/talks/GCPR2018webSmall.pdf\">(pdf)</a></strong></p>\n\n<p><strong><strong>What is optical flow for? On prediction, persistence and structure.</strong></strong><br>\nWorkshop on What is Optical Flow For? ECCV, Munich, Sept. 2018.<br>\n(<a href=\"http://files.is.tue.mpg.de/black/talks/WhatIsFlowForPublic.pptx\">ppt</a>&nbsp;76MB)</p>\n\n<p><strong><strong>The Future and Generative Models: A Case Study of Human Bodies in Motion.</strong></strong><br>\n2-hour course given at the Int. Computer Vision Summer School, July 2016.<br>\n(<a href=\"http://files.is.tue.mpg.de/black/talks/BodiesICVSS2016final.pptx\">ppt</a>&nbsp;1.5GB)</p>\n\n<p><strong><strong>On building digital humans.</strong></strong><br>\nAn overview of our work on 3D body shape, based on a series of talks during 2015.<br>\n(<a href=\"http://files.is.tue.mpg.de/black/talks/DigitalHumansPublic.pptx\">ppt</a>&nbsp;1GB)</p>\n\n<p><strong>&nbsp;</strong></p>\n\n<h3><strong>Keynotes</strong></h3>\n\n<div>\n<div>\n<div>\n<div>\n<div>\n<div>\n<div><strong><strong>On building digital humans</strong>,</strong></div>\n\n<div><em>Animation Studies Summer School</em>, T\u00fcbiungen, 2015.</div>\n\n<div>&nbsp;</div>\n</div>\n\n<div><strong><strong>How and why to learn a 3D model of the human body,</strong></strong></div>\n\n<div><em>12th IEEE Int. Conf. on Advanced Video and&nbsp;Signal-based Surveillance, AVSS</em>, Karlsruhe, Germany, Aug. 2015.</div>\n\n<div>&nbsp;</div>\n</div>\n<strong><strong>4D capture, modeling, and animation of human soft-tissue motion,</strong></strong></div>\n\n<div><em>Kinovis Inaugural Workshop</em>, INRIA Rhone-Alps,&nbsp;Grenoble, 2015.</div>\n\n<div><strong>&nbsp;</strong></div>\n</div>\n</div>\n\n<div>\n<div><strong><strong>Visions of motor control: From motion capture to the&nbsp;cortical control of movement</strong>,</strong></div>\n\n<div><em>Int. Conf. on Robotics and Automation (ICRA)</em>,&nbsp;Karlsruhe, Germany, May 2013.</div>\n</div>\n</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Modernizing Muybridge: From 3D models of the body to decoding the brain</strong>,</strong></div>\n\n<div>Keynote, <em>Svenska S\u00e4llskapet f\u00f6r Automatiserad Bildanalys (Swedish Society for Automated Image Analysis, SSBA)</em>, KTH, Stockholm,&nbsp;March 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>On modeling bodies and brains: From 3D models to decoding the brain</strong>,</strong></div>\n\n<div>Keynote, <em>Vision, Modeling and Visualization Workshop</em>, October 4-6, 2011, Berlin.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Human activity understanding: Observing the body and the brain</strong></strong></div>\n\n<div>Keynote, <em>International Workshop on Human Activity Understanding from 3D Data,</em> Colorado Spring, June 24, 2011.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<h3><strong>Invited Conference, Workshop, and Summer School&nbsp;Talks</strong></h3>\n\n<div>\n<div>\n<div>\n<div><strong><strong>Learning to be a Digital Human</strong></strong></div>\n\n<div><em>9th Int. Workshop on Human Behavior Understanding</em>. at ECCV, Sept. 2018.</div>\n\n<div>(<a href=\"http://files.is.tue.mpg.de/black/talks/HBU2018web.pptx\">pptx</a>)</div>\n\n<div><strong>&nbsp;</strong></div>\n<strong><strong>On Building Digital Humans</strong></strong></div>\n\n<div><em>Shape Analysis and Learning by Geometry and Machine </em> Inst. for Pure and Applied Mathematics (IPAM), UCLA, Feb. 2016.</div>\n\n<div>(<a href=\"http://files.is.tue.mpg.de/black/talks/BodiesIPAM2016web.pptx\">pptx</a>)</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Estimating 3D human pose and shape using differentiable rendering</strong></strong></div>\n\n<div><em>Inverse Rendering Workshop, ICCV,</em> Santiago Chile, Dec. 2015.</div>\n\n<div>(<a href=\"http://files.is.tue.mpg.de/black/talks/DigitalHumansInverseRenderingWeb.pptx\">pptx</a>)</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>On building digital humans</strong>,</strong></div>\n\n<div><em>Computational Vision Summer School (CVSS),</em>&nbsp;Black Forest, Germany, 2015,</div>\n\n<div><em>Machine Learning Summer School</em>, T\u00fcbingen, July 2015.</div>\n\n<div>\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>The Mathematics of Body Shape -- The Secret Lives of&nbsp;Triangles in Hollywood</strong>,</strong></div>\n\n<div><em>Science Notes, WAHRnehmung</em>,&nbsp;T\u00fcbingen,&nbsp;May 7, 2015.<br>\n(<a href=\"https://youtu.be/EGS35JogE_0\">youtube</a>)</div>\n\n<div>&nbsp;</div>\n\n<div>\n<div><strong><strong>How to build a&nbsp;digital human,</strong></strong></div>\n\n<div><em>FMX,&nbsp;Conference on Animation, Effects, Games and Transmedia</em>,&nbsp;Stuttgart, Germany, May 05-08, &nbsp;2015.</div>\n\n<div>\n<div><strong>&nbsp;</strong></div>\n\n<div>\n<div><strong><strong>Video segmentation: What should the answer be?,</strong></strong></div>\n\n<div><em>First Int.&nbsp;Workshop on Video Segmentation,</em> with&nbsp;ECCV'14, Z\u00fcrich, Sept.&nbsp;2014.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Grassmann averages for scalable robust PCA</strong></strong></div>\n\n<div><em>4th Int.&nbsp;Workshop on Computer Vision</em>, Alghero, Italy, May 2014.</div>\n\n<div>Optical flow: The \"good parts\" version,</div>\n\n<div><em>ETH/MPI Research Network on Learning Systems, Summer School</em>,&nbsp;Z\u00fcrich, June 2014.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Optical flow: The \"good parts\" version</strong>,</strong></div>\n\n<div><em>Machine Learning Summer School (MLSS)</em>, T\u00fcbiungen, 2013.</div>\n\n<div>(<a href=\"http://youtu.be/tIwpDuqJqcE\">youtube</a>)</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>MPI-Sintel: From animation to evaluation of optical flow</strong>,</strong></div>\n\n<div><em>ECCV 2012 Workshop on&nbsp;Unsolved Problems in Optical Flow and Stereo Estimation</em>, Oct. 12,&nbsp;Florence Italy.</div>\n\n<div><a href=\"http://files.is.tue.mpg.de/black/talks/sintel-eccv2012-workshop.pdf\">[pdf]</a></div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Modernizing Muybridge: From 3D models of the human body to&nbsp;decoding the brain,</strong></strong></div>\n\n<div><em>Sensory Coding &amp; Natural Environment,</em> IST Austrial, Sept. 2012.</div>\n\n<div>&nbsp;</div>\n\n<div><strong><strong>A naturalistic film for optical flow evaluation</strong>,</strong></div>\n\n<div><em>At the intersection of Vision, Graphics, Learning and Sensing - Representations and Applications Workshop</em>, Cambridge, May 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Thinking about movement: Decoding the brain to restore lost function</strong>,</strong></div>\n\n<div><em>Inaugural Symposium: New Perspectives in Integrative Neuroscience</em>, Hertie Institute, T\u00fcbingen, May 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>On modeling and estimating human body shape</strong>,</strong></div>\n\n<div><em>Rank Prize Symposium on Machine Learning and Computer Vision</em>,&nbsp;</div>\n\n<div>Grasmere, UK, March 26-29, 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Modernizing Muybridge: From 3D models of the body to decoding the brain</strong>,</strong></div>\n\n<div><em>Bernstein Cluster C Symposium,&nbsp;Bayesian Inference: From Spikes to Behaviour,</em> T\u00fcbingen, &nbsp;December 9-10, 2011.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>From Muybridge to a brain-computer interface: A computational investigation of animal movement</strong>,</strong></div>\n\n<div><em>Technion Computer Engineering (TCE) Inaugural Conference</em>, Haifa, Israel, June 1-5, 2011.</div>\n\n<div>&nbsp;</div>\n</div>\n\n<h3><strong>Invited Talks: Colloquia and Seminars</strong></h3>\n\n<div><strong><strong>The Mathematics of Body Shape,</strong></strong></div>\n\n<div><em>CIN-MPI Body Perception seminar,</em> T\u00fcbingen, July 2012.</div>\n\n<div>&nbsp;</div>\n\n<div><strong><strong>Modernizing Muybridge: From 3D models of the body to decoding the brain,</strong></strong></div>\n\n<div><em>Gatsby Computational Neuroscience Unit,</em> Univ. College, London, Jan. 2012.</div>\n\n<div>&nbsp;</div>\n\n<div><strong><strong>Modernizing Muybridge: From 3D models of the body to decoding the brain,</strong></strong></div>\n\n<div><em>Oxford University</em>, &nbsp;Robotics Research Group Seminar, Jan. 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Modeling bodies and brains: From computer vision to neural prostheses,</strong></strong></div>\n\n<div><em>Wilhelm Schickhard Institute for Computer Sciences, Eberhard Karls Universit</em>y, T\u00fcbingen, Germany, July 2011.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<h4><strong>Other Talks</strong></h4>\n\n<div>\n<div><strong><strong>From Scans to Avatars. Using &nbsp;Multi-Viewpoint, High Precision 3D Surface Imaging to create </strong></strong>Realistic Deformable Models of the Body,</div>\n\n<div>Jointly with Chris Lane, CEO 3dMD LLC.</div>\n\n<div><em>3rd International Conference and Exhibition on&nbsp;3D Body Scanning Technologies,&nbsp;</em>Lugano, Switzerland, 16-17 October 2012.</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Computing Optical Flow,</strong></strong></div>\n\n<div><em>Computational Vision Summer School 2012</em>, Freudenstadt-Lauterbad (Black Forest), June-July 2012</div>\n\n<div><strong>&nbsp;</strong></div>\n\n<div><strong><strong>Seeing machines,</strong></strong></div>\n\n<div><em>Paul-Peter Ewald Kolloquium</em>, MPI for Ingelligent Systems,</div>\n\n<div>Stuttgart, July 1, 2011.</div>\n\n<div><strong>&nbsp;</strong></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div>\n<p></p>\n                <hr>\n              </div>\n\n              <!-- VIDEOS -->\n\n              <!-- RESEARCH PROJECTS -->\n\n              \n              <!-- PUBLICATIONS -->\n              <div id=\"publications\" class=\"tab-pane custom-tab-pane fade in t-tags \">\n                <div id=\"publications_ajax_pagination_container\">\n                  <div class=\"row\">\n\t<div class=\"col-md-12\">\n\t\t<span class=\"results-number custom-results-number pull-left\">\n\t\t\t324 results\n\n\t\t\t<!-- IW-425 -->\n\t\t\t<!-- <small class=\"all-bib-export-button-text\">()</small> -->\n\t\t\t<small class=\"all-bib-export-button-text\">(<a target=\"_blank\" href=\"/publications/get_bibtexfile_all?action=show&amp;controller=employees&amp;employee_id=18&amp;export_type=bibtex&amp;id=black\">View BibTeX file of all listed publications</a>)</small>\n\n\t\t\t</span>\n\t\t\t<span class=\"results-number pull-right\">\n\t\t\t\t<div class=\"row \">\n\t\t\t\t\t<div class=\"col-md-12 viewSwitchButtonGroup\">\n\t\t\t\t\t\t<div class=\"btn-group\">\n\t\t\t\t\t\t\t<button id=\"detailViewButton\" type=\"button\" class=\"btn btn-default btn-sm custom-toggle-active\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-th-list\"></i>\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t\t<button id=\"listViewButton\" type=\"button\" class=\"btn btn-default btn-sm\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-align-justify\"></i>\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t</span>\n\t\t</div>\n\t</div>\n\n\t<div id=\"publicationViewContainer\" data-department=\"\">\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12 \">\n\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2020</span></h4>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr-top\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/vibe-cvpr-2020\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"{VIBE}: Video Inference for Human Body Pose and Shape Estimation\" src=\"/uploads/publication/image/22866/thumb_xl_vibe.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/vibe-cvpr-2020\">VIBE: Video Inference for Human Body Pose and Shape Estimation</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision and Pattern Recognition (CVPR)</em>, June 2020 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22866\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22866\" href=\"#abstractContent22866\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22866\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tHuman motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methodsfail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \u201cVideo Inference for Body Pose and Shape Estimation\u201d (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1912.05656\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/mkocabas/VIBE\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22866/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/vibe-cvpr-2020&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methodsfail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \u201cVideo Inference for Body Pose and Shape Estimation\u201d (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE: https://ps.is.tuebingen.mpg.de/person/black/vibe-cvpr-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/vibe-cvpr-2020&amp;amp;title=Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methodsfail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \u201cVideo Inference for Body Pose and Shape Estimation\u201d (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE &amp;amp;summary={VIBE}: Video Inference for Human Body Pose and Shape Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methodsfail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \u201cVideo Inference for Body Pose and Shape Estimation\u201d (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE %20https://ps.is.tuebingen.mpg.de/person/black/vibe-cvpr-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Human motion is fundamental to understanding behavior. Despite progress on single-image 3D pose and shape estimation, existing video-based state-of-the-art methodsfail to produce accurate and natural motion sequences due to a lack of ground-truth 3D motion data for training. To address this problem, we propose \u201cVideo Inference for Body Pose and Shape Estimation\u201d (VIBE), which makes use of an existing large-scale motion capture dataset (AMASS) together with unpaired, in-the-wild, 2D keypoint annotations. Our key novelty is an adversarial learning framework that leverages AMASS to discriminate between real human motions and those produced by our temporal pose and shape\nregression networks. We define a temporal network architecture and show that adversarial training, at the sequence level, produces kinematically plausible motion sequences without in-the-wild ground-truth 3D labels. We perform extensive experimentation to analyze the importance of motion and demonstrate the effectiveness of VIBE on challenging 3D pose estimation datasets, achieving state-of-the-art performance. Code and pretrained models are available at https://github.com/mkocabas/VIBE &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/vibe-cvpr-2020&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"col-md-12 publication-container-list \">\n\t\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2020</span></h4>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-20\"></div>\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mkocabas\">Kocabas, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nathanasiou\">Athanasiou, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/vibe-cvpr-2020\">VIBE: Video Inference for Human Body Pose and Shape Estimation</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Computer Vision and Pattern Recognition (CVPR)</em>, June 2020 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1912.05656\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/mkocabas/VIBE\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22866/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/ruegg-aaai-2020\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Chained Representation Cycling: Learning to Estimate 3D Human Pose and Shape by Cycling Between Representations\" src=\"/uploads/publication/image/22790/thumb_xl_aaai.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ruegg-aaai-2020\">Chained Representation Cycling: Learning to Estimate 3D Human Pose and Shape by Cycling Between Representations</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/classner\">Lassner, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Schindler, K.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</em>, Febuary 2020 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22790\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22790\" href=\"#abstractContent22790\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22790\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/AAAI-RueeggN.199.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22790/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/ruegg-aaai-2020&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems.: https://ps.is.tuebingen.mpg.de/person/black/ruegg-aaai-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/ruegg-aaai-2020&amp;amp;title=The goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems. &amp;amp;summary=Chained Representation Cycling: Learning to Estimate 3D Human Pose and Shape by Cycling Between Representations&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems. %20https://ps.is.tuebingen.mpg.de/person/black/ruegg-aaai-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/ruegg-aaai-2020&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nrueegg\">Rueegg, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/classner\">Lassner, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Schindler, K.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/ruegg-aaai-2020\">Chained Representation Cycling: Learning to Estimate 3D Human Pose and Shape by Cycling Between Representations</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</em>, Febuary 2020 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/AAAI-RueeggN.199.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22790/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/multihumanflow\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning Multi-Human Optical Flow\" src=\"/uploads/publication/image/22775/thumb_xl_MultiHumanOFlow_thumb.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/multihumanflow\">Learning Multi-Human Optical Flow</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dhoffmann\">Hoffmann, D. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>International Journal of Computer Vision (IJCV)</em>, January 2020 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22775\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22775\" href=\"#abstractContent22775\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22775\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single-and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/1910.11667.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/s11263-019-01279-w\"><i class=\"fa fa-file-o\"></i>  Publisher Version</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/552/poster_MultiHumanOFlow_v1.2.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://humanflow.is.tue.mpg.de \"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/s11263-019-01279-w\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22775/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/multihumanflow&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single-and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research.: https://ps.is.tuebingen.mpg.de/person/black/multihumanflow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/multihumanflow&amp;amp;title=The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single-and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research. &amp;amp;summary=Learning Multi-Human Optical Flow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single-and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research. %20https://ps.is.tuebingen.mpg.de/person/black/multihumanflow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The optical flow of humans is well known to be useful for the analysis of human action. Recent optical flow methods focus on training deep networks to approach the problem. However, the training data used by them does not cover the domain of human motion. Therefore, we develop a dataset of multi-human optical flow and train optical flow networks on this dataset. We use a 3D model of the human body and motion capture data to synthesize realistic flow fields in both single-and multi-person images. We then train optical flow networks to estimate human flow fields from pairs of images. We demonstrate that our trained networks are more accurate than a wide range of top methods on held-out test data and that they can generalize well to real image sequences. The code, trained models and the dataset are available for research. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/multihumanflow&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dhoffmann\">Hoffmann, D. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/multihumanflow\">Learning Multi-Human Optical Flow</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>International Journal of Computer Vision (IJCV)</em>, January 2020 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/pdf/1910.11667.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://link.springer.com/article/10.1007/s11263-019-01279-w\"><i class=\"fa fa-file-o\"></i>  Publisher Version</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/552/poster_MultiHumanOFlow_v1.2.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://humanflow.is.tue.mpg.de \"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/s11263-019-01279-w\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22775/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/gma-ehd-2020\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"General Movement Assessment from videos of computed {3D} infant body models is equally effective compared to conventional {RGB} Video rating\" src=\"/uploads/publication/image/22833/thumb_xl_gma.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/gma-ehd-2020\">General Movement Assessment from videos of computed 3D infant body models is equally effective compared to conventional RGB Video rating</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSchroeder, S., Hesse, N., Weinberger, R., Tacke, U., Gerstl, L., Hilgendorff, A., Heinen, F., Arens, M., Bodensteiner, C., Dijkstra, L. J., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Hadders-Algra, M.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Early Human Development</em>, 2020 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22833\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22833\" href=\"#abstractContent22833\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22833\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tBackground: General Movement Assessment (GMA) is a powerful tool to predict Cerebral Palsy (CP). Yet, GMA requires substantial training hampering its implementation in clinical routine. This inspired a world-wide quest for automated GMA.\nAim: To test whether a low-cost, marker-less system for three-dimensional motion capture from RGB depth sequences using a whole body infant model may serve as the basis for automated GMA.\nStudy design: Clinical case study at an academic neurodevelopmental outpatient clinic.\nSubjects: Twenty-nine high-risk infants were recruited and assessed at their clinical follow-up at 2-4 month corrected age (CA). Their neurodevelopmental outcome was assessed regularly up to 12-31 months CA.\nOutcome measures: GMA according to Hadders-Algra by a masked GMA-expert of conventional and computed 3D body model (\u201cSMIL motion\u201d) videos of the same GMs. Agreement between both GMAs was assessed, and sensitivity and specificity of both methods to predict CP at \u226512 months CA.\nResults: The agreement of the two GMA ratings was substantial, with \u03ba=0.66 for the classification of definitely abnormal (DA) GMs and an ICC of 0.887 (95% CI 0.762;0.947) for a more detailed GM-scoring. Five children were diagnosed with CP (four bilateral, one unilateral CP). The GMs of the child with unilateral CP were twice rated as mildly abnormal. DA-ratings of both videos predicted bilateral CP well: sensitivity 75% and 100%, specificity 88% and 92% for conventional and SMIL motion videos, respectively.\nConclusions: Our computed infant 3D full body model is an attractive starting point for automated GMA in infants at risk of CP.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22833/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/gma-ehd-2020&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Background: General Movement Assessment (GMA) is a powerful tool to predict Cerebral Palsy (CP). Yet, GMA requires substantial training hampering its implementation in clinical routine. This inspired a world-wide quest for automated GMA.\nAim: To test whether a low-cost, marker-less system for three-dimensional motion capture from RGB depth sequences using a whole body infant model may serve as the basis for automated GMA.\nStudy design: Clinical case study at an academic neurodevelopmental outpatient clinic.\nSubjects: Twenty-nine high-risk infants were recruited and assessed at their clinical follow-up at 2-4 month corrected age (CA). Their neurodevelopmental outcome was assessed regularly up to 12-31 months CA.\nOutcome measures: GMA according to Hadders-Algra by a masked GMA-expert of conventional and computed 3D body model (\u201cSMIL motion\u201d) videos of the same GMs. Agreement between both GMAs was assessed, and sensitivity and specificity of both methods to predict CP at \u226512 months CA.\nResults: The agreement of the two GMA ratings was substantial, with \u03ba=0.66 for the classification of definitely abnormal (DA) GMs and an ICC of 0.887 (95% CI 0.762;0.947) for a more detailed GM-scoring. Five children were diagnosed with CP (four bilateral, one unilateral CP). The GMs of the child with unilateral CP were twice rated as mildly abnormal. DA-ratings of both videos predicted bilateral CP well: sensitivity 75% and 100%, specificity 88% and 92% for conventional and SMIL motion videos, respectively.\nConclusions: Our computed infant 3D full body model is an attractive starting point for automated GMA in infants at risk of CP.: https://ps.is.tuebingen.mpg.de/person/black/gma-ehd-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/gma-ehd-2020&amp;amp;title=Background: General Movement Assessment (GMA) is a powerful tool to predict Cerebral Palsy (CP). Yet, GMA requires substantial training hampering its implementation in clinical routine. This inspired a world-wide quest for automated GMA.\nAim: To test whether a low-cost, marker-less system for three-dimensional motion capture from RGB depth sequences using a whole body infant model may serve as the basis for automated GMA.\nStudy design: Clinical case study at an academic neurodevelopmental outpatient clinic.\nSubjects: Twenty-nine high-risk infants were recruited and assessed at their clinical follow-up at 2-4 month corrected age (CA). Their neurodevelopmental outcome was assessed regularly up to 12-31 months CA.\nOutcome measures: GMA according to Hadders-Algra by a masked GMA-expert of conventional and computed 3D body model (\u201cSMIL motion\u201d) videos of the same GMs. Agreement between both GMAs was assessed, and sensitivity and specificity of both methods to predict CP at \u226512 months CA.\nResults: The agreement of the two GMA ratings was substantial, with \u03ba=0.66 for the classification of definitely abnormal (DA) GMs and an ICC of 0.887 (95% CI 0.762;0.947) for a more detailed GM-scoring. Five children were diagnosed with CP (four bilateral, one unilateral CP). The GMs of the child with unilateral CP were twice rated as mildly abnormal. DA-ratings of both videos predicted bilateral CP well: sensitivity 75% and 100%, specificity 88% and 92% for conventional and SMIL motion videos, respectively.\nConclusions: Our computed infant 3D full body model is an attractive starting point for automated GMA in infants at risk of CP. &amp;amp;summary=General Movement Assessment from videos of computed {3D} infant body models is equally effective compared to conventional {RGB} Video rating&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Background: General Movement Assessment (GMA) is a powerful tool to predict Cerebral Palsy (CP). Yet, GMA requires substantial training hampering its implementation in clinical routine. This inspired a world-wide quest for automated GMA.\nAim: To test whether a low-cost, marker-less system for three-dimensional motion capture from RGB depth sequences using a whole body infant model may serve as the basis for automated GMA.\nStudy design: Clinical case study at an academic neurodevelopmental outpatient clinic.\nSubjects: Twenty-nine high-risk infants were recruited and assessed at their clinical follow-up at 2-4 month corrected age (CA). Their neurodevelopmental outcome was assessed regularly up to 12-31 months CA.\nOutcome measures: GMA according to Hadders-Algra by a masked GMA-expert of conventional and computed 3D body model (\u201cSMIL motion\u201d) videos of the same GMs. Agreement between both GMAs was assessed, and sensitivity and specificity of both methods to predict CP at \u226512 months CA.\nResults: The agreement of the two GMA ratings was substantial, with \u03ba=0.66 for the classification of definitely abnormal (DA) GMs and an ICC of 0.887 (95% CI 0.762;0.947) for a more detailed GM-scoring. Five children were diagnosed with CP (four bilateral, one unilateral CP). The GMs of the child with unilateral CP were twice rated as mildly abnormal. DA-ratings of both videos predicted bilateral CP well: sensitivity 75% and 100%, specificity 88% and 92% for conventional and SMIL motion videos, respectively.\nConclusions: Our computed infant 3D full body model is an attractive starting point for automated GMA in infants at risk of CP. %20https://ps.is.tuebingen.mpg.de/person/black/gma-ehd-2020&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Background: General Movement Assessment (GMA) is a powerful tool to predict Cerebral Palsy (CP). Yet, GMA requires substantial training hampering its implementation in clinical routine. This inspired a world-wide quest for automated GMA.\nAim: To test whether a low-cost, marker-less system for three-dimensional motion capture from RGB depth sequences using a whole body infant model may serve as the basis for automated GMA.\nStudy design: Clinical case study at an academic neurodevelopmental outpatient clinic.\nSubjects: Twenty-nine high-risk infants were recruited and assessed at their clinical follow-up at 2-4 month corrected age (CA). Their neurodevelopmental outcome was assessed regularly up to 12-31 months CA.\nOutcome measures: GMA according to Hadders-Algra by a masked GMA-expert of conventional and computed 3D body model (\u201cSMIL motion\u201d) videos of the same GMs. Agreement between both GMAs was assessed, and sensitivity and specificity of both methods to predict CP at \u226512 months CA.\nResults: The agreement of the two GMA ratings was substantial, with \u03ba=0.66 for the classification of definitely abnormal (DA) GMs and an ICC of 0.887 (95% CI 0.762;0.947) for a more detailed GM-scoring. Five children were diagnosed with CP (four bilateral, one unilateral CP). The GMs of the child with unilateral CP were twice rated as mildly abnormal. DA-ratings of both videos predicted bilateral CP well: sensitivity 75% and 100%, specificity 88% and 92% for conventional and SMIL motion videos, respectively.\nConclusions: Our computed infant 3D full body model is an attractive starting point for automated GMA in infants at risk of CP. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/gma-ehd-2020&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tSchroeder, S., Hesse, N., Weinberger, R., Tacke, U., Gerstl, L., Hilgendorff, A., Heinen, F., Arens, M., Bodensteiner, C., Dijkstra, L. J., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Hadders-Algra, M.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/gma-ehd-2020\">General Movement Assessment from videos of computed 3D infant body models is equally effective compared to conventional RGB Video rating</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>Early Human Development</em>, 2020 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22833/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12 publication-year-container-list-margin-top\">\n\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2019</span></h4>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr-top\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/flowattack\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Attacking Optical Flow\" src=\"/uploads/publication/image/22759/thumb_xl_teaser_singlecol.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/flowattack\">Attacking Optical Flow</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jjanai\">Janai, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, November 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22759\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22759\" href=\"#abstractContent22759\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22759\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tDeep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\" https://youtu.be/5nQ7loiPmdA\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://flowattack.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/546/0496.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/547/0496-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Supplementary Material</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://flowattack.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22759/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/flowattack&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.: https://ps.is.tuebingen.mpg.de/person/black/flowattack&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/flowattack&amp;amp;title=Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes. &amp;amp;summary=Attacking Optical Flow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes. %20https://ps.is.tuebingen.mpg.de/person/black/flowattack&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/flowattack&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"col-md-12 publication-container-list publication-year-container-list-margin-top\">\n\t\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2019</span></h4>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-20\"></div>\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jjanai\">Janai, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/flowattack\">Attacking Optical Flow</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, November 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\" https://youtu.be/5nQ7loiPmdA\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://flowattack.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/546/0496.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/547/0496-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Supplementary Material</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://flowattack.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22759/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/foster-neuroimage-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Decoding subcategories of human bodies from both body- and face-responsive cortical regions\" src=\"/uploads/publication/image/22677/thumb_xl_Celia.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/foster-neuroimage-2019\">Decoding subcategories of human bodies from both body- and face-responsive cortical regions</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/cfoster\">Foster, C.</a></span>, Zhao, M., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Mohler, B. J., Bartels, A., B\u00fclthoff, I.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>NeuroImage</em>, 202(15):116085, November 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22677\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22677\" href=\"#abstractContent22677\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22677\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tOur visual system can easily categorize objects (e.g. faces vs. bodies) and further differentiate them into subcategories (e.g. male vs. female). This ability is particularly important for objects of social significance, such as human faces and bodies. While many studies have demonstrated category selectivity to faces and bodies in the brain, how subcategories of faces and bodies are represented remains unclear. Here, we investigated how the brain encodes two prominent subcategories shared by both faces and bodies, sex and weight, and whether neural responses to these subcategories rely on low-level visual, high-level visual or semantic similarity. We recorded brain activity with fMRI while participants viewed faces and bodies that varied in sex, weight, and image size. The results showed that the sex of bodies can be decoded from both body- and face-responsive brain areas, with the former exhibiting more consistent size-invariant decoding than the latter. Body weight could also be decoded in face-responsive areas and in distributed body-responsive areas, and this decoding was also invariant to image size. The weight of faces could be decoded from the fusiform body area (FBA), and weight could be decoded across face and body stimuli in the extrastriate body area (EBA) and a distributed body-responsive area. The sex of well-controlled faces (e.g. excluding hairstyles) could not be decoded from face- or body-responsive regions. These results demonstrate that both face- and body-responsive brain regions encode information that can distinguish the sex and weight of bodies. Moreover, the neural patterns corresponding to sex and weight were invariant to image size and could sometimes generalize across face and body stimuli, suggesting that such subcategorical information is encoded with a high-level visual or semantic code.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S1053811919306731\"><i class=\"fa fa-file-o\"></i>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S1053811919306731/pdfft?md5=0034c2d7527a87959ccb38583c1a8744&amp;pid=1-s2.0-S1053811919306731-main.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1016/j.neuroimage.2019.116085\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22677/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/foster-neuroimage-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Our visual system can easily categorize objects (e.g. faces vs. bodies) and further differentiate them into subcategories (e.g. male vs. female). This ability is particularly important for objects of social significance, such as human faces and bodies. While many studies have demonstrated category selectivity to faces and bodies in the brain, how subcategories of faces and bodies are represented remains unclear. Here, we investigated how the brain encodes two prominent subcategories shared by both faces and bodies, sex and weight, and whether neural responses to these subcategories rely on low-level visual, high-level visual or semantic similarity. We recorded brain activity with fMRI while participants viewed faces and bodies that varied in sex, weight, and image size. The results showed that the sex of bodies can be decoded from both body- and face-responsive brain areas, with the former exhibiting more consistent size-invariant decoding than the latter. Body weight could also be decoded in face-responsive areas and in distributed body-responsive areas, and this decoding was also invariant to image size. The weight of faces could be decoded from the fusiform body area (FBA), and weight could be decoded across face and body stimuli in the extrastriate body area (EBA) and a distributed body-responsive area. The sex of well-controlled faces (e.g. excluding hairstyles) could not be decoded from face- or body-responsive regions. These results demonstrate that both face- and body-responsive brain regions encode information that can distinguish the sex and weight of bodies. Moreover, the neural patterns corresponding to sex and weight were invariant to image size and could sometimes generalize across face and body stimuli, suggesting that such subcategorical information is encoded with a high-level visual or semantic code.: https://ps.is.tuebingen.mpg.de/person/black/foster-neuroimage-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/foster-neuroimage-2019&amp;amp;title=Our visual system can easily categorize objects (e.g. faces vs. bodies) and further differentiate them into subcategories (e.g. male vs. female). This ability is particularly important for objects of social significance, such as human faces and bodies. While many studies have demonstrated category selectivity to faces and bodies in the brain, how subcategories of faces and bodies are represented remains unclear. Here, we investigated how the brain encodes two prominent subcategories shared by both faces and bodies, sex and weight, and whether neural responses to these subcategories rely on low-level visual, high-level visual or semantic similarity. We recorded brain activity with fMRI while participants viewed faces and bodies that varied in sex, weight, and image size. The results showed that the sex of bodies can be decoded from both body- and face-responsive brain areas, with the former exhibiting more consistent size-invariant decoding than the latter. Body weight could also be decoded in face-responsive areas and in distributed body-responsive areas, and this decoding was also invariant to image size. The weight of faces could be decoded from the fusiform body area (FBA), and weight could be decoded across face and body stimuli in the extrastriate body area (EBA) and a distributed body-responsive area. The sex of well-controlled faces (e.g. excluding hairstyles) could not be decoded from face- or body-responsive regions. These results demonstrate that both face- and body-responsive brain regions encode information that can distinguish the sex and weight of bodies. Moreover, the neural patterns corresponding to sex and weight were invariant to image size and could sometimes generalize across face and body stimuli, suggesting that such subcategorical information is encoded with a high-level visual or semantic code. &amp;amp;summary=Decoding subcategories of human bodies from both body- and face-responsive cortical regions&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Our visual system can easily categorize objects (e.g. faces vs. bodies) and further differentiate them into subcategories (e.g. male vs. female). This ability is particularly important for objects of social significance, such as human faces and bodies. While many studies have demonstrated category selectivity to faces and bodies in the brain, how subcategories of faces and bodies are represented remains unclear. Here, we investigated how the brain encodes two prominent subcategories shared by both faces and bodies, sex and weight, and whether neural responses to these subcategories rely on low-level visual, high-level visual or semantic similarity. We recorded brain activity with fMRI while participants viewed faces and bodies that varied in sex, weight, and image size. The results showed that the sex of bodies can be decoded from both body- and face-responsive brain areas, with the former exhibiting more consistent size-invariant decoding than the latter. Body weight could also be decoded in face-responsive areas and in distributed body-responsive areas, and this decoding was also invariant to image size. The weight of faces could be decoded from the fusiform body area (FBA), and weight could be decoded across face and body stimuli in the extrastriate body area (EBA) and a distributed body-responsive area. The sex of well-controlled faces (e.g. excluding hairstyles) could not be decoded from face- or body-responsive regions. These results demonstrate that both face- and body-responsive brain regions encode information that can distinguish the sex and weight of bodies. Moreover, the neural patterns corresponding to sex and weight were invariant to image size and could sometimes generalize across face and body stimuli, suggesting that such subcategorical information is encoded with a high-level visual or semantic code. %20https://ps.is.tuebingen.mpg.de/person/black/foster-neuroimage-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Our visual system can easily categorize objects (e.g. faces vs. bodies) and further differentiate them into subcategories (e.g. male vs. female). This ability is particularly important for objects of social significance, such as human faces and bodies. While many studies have demonstrated category selectivity to faces and bodies in the brain, how subcategories of faces and bodies are represented remains unclear. Here, we investigated how the brain encodes two prominent subcategories shared by both faces and bodies, sex and weight, and whether neural responses to these subcategories rely on low-level visual, high-level visual or semantic similarity. We recorded brain activity with fMRI while participants viewed faces and bodies that varied in sex, weight, and image size. The results showed that the sex of bodies can be decoded from both body- and face-responsive brain areas, with the former exhibiting more consistent size-invariant decoding than the latter. Body weight could also be decoded in face-responsive areas and in distributed body-responsive areas, and this decoding was also invariant to image size. The weight of faces could be decoded from the fusiform body area (FBA), and weight could be decoded across face and body stimuli in the extrastriate body area (EBA) and a distributed body-responsive area. The sex of well-controlled faces (e.g. excluding hairstyles) could not be decoded from face- or body-responsive regions. These results demonstrate that both face- and body-responsive brain regions encode information that can distinguish the sex and weight of bodies. Moreover, the neural patterns corresponding to sex and weight were invariant to image size and could sometimes generalize across face and body stimuli, suggesting that such subcategorical information is encoded with a high-level visual or semantic code. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/foster-neuroimage-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/cfoster\">Foster, C.</a></span>, Zhao, M., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Mohler, B. J., Bartels, A., B\u00fclthoff, I.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/foster-neuroimage-2019\">Decoding subcategories of human bodies from both body- and face-responsive cortical regions</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>NeuroImage</em>, 202(15):116085, November 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S1053811919306731\"><i class=\"fa fa-file-o\"></i>  paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.sciencedirect.com/science/article/pii/S1053811919306731/pdfft?md5=0034c2d7527a87959ccb38583c1a8744&amp;pid=1-s2.0-S1053811919306731-main.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1016/j.neuroimage.2019.116085\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22677/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/aircap2019aerialswarms\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"AirCap -- Aerial Outdoor Motion Capture\" src=\"/uploads/publication/image/22698/thumb_xl_cover_walking_seq.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/aircap2019aerialswarms\">AirCap \u2013 Aerial Outdoor Motion Capture</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, Lawless, G., <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/imartinovic\">Martinovic, I.</a></span>, B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019), Workshop on Aerial Swarms</em>, November 2019 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22698\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22698\" href=\"#abstractContent22698\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22698\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThis paper presents an overview of the Grassroots project Aerial Outdoor Motion Capture (AirCap) running at the Max Planck Institute for Intelligent Systems. AirCap's goal is to achieve markerless, unconstrained, human motion capture (mocap) in unknown and unstructured outdoor environments. To that end, we have developed an autonomous flying motion capture system using a team of aerial vehicles (MAVs) with only on-board, monocular RGB cameras. We have conducted several real robot experiments involving up to 3 aerial vehicles autonomously tracking and following a person in several challenging scenarios using our approach of active cooperative perception developed in AirCap. Using the images captured by these robots during the experiments, we have demonstrated a successful offline body pose and shape estimation with sufficiently high accuracy. Overall, we have demonstrated the first fully autonomous flying motion capture system involving multiple robots for outdoor scenarios.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/557/IROS_2019.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Talk slides</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: 3D Motion Capture\" class=\"btn btn-default btn-xs\" href=\"/research_projects/aircap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: Perception-Based Control\" class=\"btn btn-default btn-xs\" href=\"/research_projects/autonomous-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22698/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/aircap2019aerialswarms&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - This paper presents an overview of the Grassroots project Aerial Outdoor Motion Capture (AirCap) running at the Max Planck Institute for Intelligent Systems. AirCap&amp;#39;s goal is to achieve markerless, unconstrained, human motion capture (mocap) in unknown and unstructured outdoor environments. To that end, we have developed an autonomous flying motion capture system using a team of aerial vehicles (MAVs) with only on-board, monocular RGB cameras. We have conducted several real robot experiments involving up to 3 aerial vehicles autonomously tracking and following a person in several challenging scenarios using our approach of active cooperative perception developed in AirCap. Using the images captured by these robots during the experiments, we have demonstrated a successful offline body pose and shape estimation with sufficiently high accuracy. Overall, we have demonstrated the first fully autonomous flying motion capture system involving multiple robots for outdoor scenarios.: https://ps.is.tuebingen.mpg.de/person/black/aircap2019aerialswarms&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/aircap2019aerialswarms&amp;amp;title=This paper presents an overview of the Grassroots project Aerial Outdoor Motion Capture (AirCap) running at the Max Planck Institute for Intelligent Systems. AirCap&amp;#39;s goal is to achieve markerless, unconstrained, human motion capture (mocap) in unknown and unstructured outdoor environments. To that end, we have developed an autonomous flying motion capture system using a team of aerial vehicles (MAVs) with only on-board, monocular RGB cameras. We have conducted several real robot experiments involving up to 3 aerial vehicles autonomously tracking and following a person in several challenging scenarios using our approach of active cooperative perception developed in AirCap. Using the images captured by these robots during the experiments, we have demonstrated a successful offline body pose and shape estimation with sufficiently high accuracy. Overall, we have demonstrated the first fully autonomous flying motion capture system involving multiple robots for outdoor scenarios. &amp;amp;summary=AirCap -- Aerial Outdoor Motion Capture&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=This paper presents an overview of the Grassroots project Aerial Outdoor Motion Capture (AirCap) running at the Max Planck Institute for Intelligent Systems. AirCap&amp;#39;s goal is to achieve markerless, unconstrained, human motion capture (mocap) in unknown and unstructured outdoor environments. To that end, we have developed an autonomous flying motion capture system using a team of aerial vehicles (MAVs) with only on-board, monocular RGB cameras. We have conducted several real robot experiments involving up to 3 aerial vehicles autonomously tracking and following a person in several challenging scenarios using our approach of active cooperative perception developed in AirCap. Using the images captured by these robots during the experiments, we have demonstrated a successful offline body pose and shape estimation with sufficiently high accuracy. Overall, we have demonstrated the first fully autonomous flying motion capture system involving multiple robots for outdoor scenarios. %20https://ps.is.tuebingen.mpg.de/person/black/aircap2019aerialswarms&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=This paper presents an overview of the Grassroots project Aerial Outdoor Motion Capture (AirCap) running at the Max Planck Institute for Intelligent Systems. AirCap&amp;#39;s goal is to achieve markerless, unconstrained, human motion capture (mocap) in unknown and unstructured outdoor environments. To that end, we have developed an autonomous flying motion capture system using a team of aerial vehicles (MAVs) with only on-board, monocular RGB cameras. We have conducted several real robot experiments involving up to 3 aerial vehicles autonomously tracking and following a person in several challenging scenarios using our approach of active cooperative perception developed in AirCap. Using the images captured by these robots during the experiments, we have demonstrated a successful offline body pose and shape estimation with sufficiently high accuracy. Overall, we have demonstrated the first fully autonomous flying motion capture system involving multiple robots for outdoor scenarios. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/aircap2019aerialswarms&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, Lawless, G., <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/imartinovic\">Martinovic, I.</a></span>, B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/aircap2019aerialswarms\">AirCap \u2013 Aerial Outdoor Motion Capture</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2019), Workshop on Aerial Swarms</em>, November 2019 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/557/IROS_2019.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Talk slides</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: 3D Motion Capture\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/aircap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: Perception-Based Control\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/autonomous-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22698/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/spin-iccv-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning to Reconstruct {3D} Human Pose and Shape via Model-fitting in the Loop\" src=\"/uploads/publication/image/22716/thumb_xl_spin3.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/spin-iccv-2019\">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tKolotouros, N., <span class=\"default-link-ul\"><a href=\"/person/gpavlakos\">Pavlakos, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Daniilidis, K.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22716\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22716\" href=\"#abstractContent22716\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22716\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tModel-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1909.12828\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/nkolot/SPIN\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.seas.upenn.edu/~nkolot/projects/spin/\"><i class=\"fa fa-file-o\"></i>  project</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22716/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/spin-iccv-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins.: https://ps.is.tuebingen.mpg.de/person/black/spin-iccv-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/spin-iccv-2019&amp;amp;title=Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. &amp;amp;summary=Learning to Reconstruct {3D} Human Pose and Shape via Model-fitting in the Loop&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. %20https://ps.is.tuebingen.mpg.de/person/black/spin-iccv-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/spin-iccv-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tKolotouros, N., <span class=\"default-link-ul\"><a href=\"/person/gpavlakos\">Pavlakos, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Daniilidis, K.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/spin-iccv-2019\">Learning to Reconstruct 3D Human Pose and Shape via Model-fitting in the Loop</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1909.12828\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/nkolot/SPIN\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.seas.upenn.edu/~nkolot/projects/spin/\"><i class=\"fa fa-file-o\"></i>  project</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22716/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/prox-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Resolving {3D} Human Pose Ambiguities with {3D} Scene Constraints\" src=\"/uploads/publication/image/22653/thumb_xl_website_teaser.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/prox-2019\">Resolving 3D Human Pose Ambiguities with 3D Scene Constraints</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>,  pages: 2282-2292, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22653\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22653\" href=\"#abstractContent22653\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22653\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tTo understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The interpenetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion-capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/530/ICCV_2019___PROX.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/550/ICCV_2019___PROX_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://prox.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22653/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/prox-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The interpenetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion-capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.: https://ps.is.tuebingen.mpg.de/person/black/prox-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/prox-2019&amp;amp;title=To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The interpenetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion-capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de. &amp;amp;summary=Resolving {3D} Human Pose Ambiguities with {3D} Scene Constraints&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The interpenetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion-capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de. %20https://ps.is.tuebingen.mpg.de/person/black/prox-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The interpenetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion-capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/prox-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mhassan\">Hassan, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/prox-2019\">Resolving 3D Human Pose Ambiguities with 3D Scene Constraints</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>,  pages: 2282-2292, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/530/ICCV_2019___PROX.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/550/ICCV_2019___PROX_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://prox.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22653/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/jie-graph-decomposition-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"End-to-end Learning for Graph Decomposition\" src=\"/uploads/publication/image/22471/thumb_xl_end-to-end-learning-for-graph-decomposition.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/jie-graph-decomposition-2018\">End-to-end Learning for Graph Decomposition</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tSong, J., Andres, B., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22471\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22471\" href=\"#abstractContent22471\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22471\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tDeep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/534/2661.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  PDF</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22471/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/jie-graph-decomposition-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation.: https://ps.is.tuebingen.mpg.de/person/black/jie-graph-decomposition-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/jie-graph-decomposition-2018&amp;amp;title=Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation. &amp;amp;summary=End-to-end Learning for Graph Decomposition&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation. %20https://ps.is.tuebingen.mpg.de/person/black/jie-graph-decomposition-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Deep neural networks provide powerful tools for pattern recognition, while classical graph algorithms are widely used to solve combinatorial problems. In computer vision, many tasks combine elements of both pattern recognition and graph reasoning. In this paper, we study how to connect deep networks with graph decomposition into an end-to-end trainable framework. More specifically, the minimum cost multicut problem is first converted to an unconstrained binary cubic formulation where cycle consistency constraints are incorporated into the objective function. The new optimization problem can be viewed as a Conditional Random Field (CRF) in which the random variables are associated with the binary edge labels. Cycle constraints are introduced into the CRF as high-order potentials. A standard Convolutional Neural Network (CNN) provides the front-end features for the fully differentiable CRF. The parameters of both parts are optimized in an end-to-end manner. The efficacy of the proposed learning algorithm is demonstrated via experiments on clustering MNIST images and on the challenging task of real-world multi-people pose estimation. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/jie-graph-decomposition-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tSong, J., Andres, B., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/jie-graph-decomposition-2018\">End-to-end Learning for Graph Decomposition</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/534/2661.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  PDF</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22471/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/3d-animal-pose-shape\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images &quot;In the Wild&quot;\" src=\"/uploads/publication/image/22661/thumb_xl_ps_web.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/3d-animal-pose-shape\">Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images \"In the Wild\"</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, Berger-Wolf, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22661\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22661\" href=\"#abstractContent22661\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22661\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy's zebras from a collection of images. The Grevy's zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other.  To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. \nLearning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision.\nMoreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. \nWe show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. \nCode and data are available at https://github.com/silviazuffi/smalst\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/silviazuffi/smalst\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/533/6034_after_pdfexpress.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/535/6034_supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supmat</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/548/iccv19_smalst_talk_compressed.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  iccv19 presentation</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Animal Shape\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-animal-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22661/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/3d-animal-pose-shape&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy&amp;#39;s zebras from a collection of images. The Grevy&amp;#39;s zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other.  To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. \nLearning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision.\nMoreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. \nWe show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. \nCode and data are available at https://github.com/silviazuffi/smalst: https://ps.is.tuebingen.mpg.de/person/black/3d-animal-pose-shape&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/3d-animal-pose-shape&amp;amp;title=We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy&amp;#39;s zebras from a collection of images. The Grevy&amp;#39;s zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other.  To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. \nLearning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision.\nMoreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. \nWe show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. \nCode and data are available at https://github.com/silviazuffi/smalst &amp;amp;summary=Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images &amp;quot;In the Wild&amp;quot;&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy&amp;#39;s zebras from a collection of images. The Grevy&amp;#39;s zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other.  To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. \nLearning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision.\nMoreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. \nWe show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. \nCode and data are available at https://github.com/silviazuffi/smalst %20https://ps.is.tuebingen.mpg.de/person/black/3d-animal-pose-shape&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=We present the first method to perform automatic 3D pose, shape and texture capture of animals from images acquired in-the-wild. In particular, we focus on the problem of capturing 3D information about Grevy&amp;#39;s zebras from a collection of images. The Grevy&amp;#39;s zebra is one of the most endangered species in Africa, with only a few thousand individuals left. Capturing the shape and pose of these animals can provide biologists and conservationists with information about animal health and behavior. In contrast to research on human pose, shape and texture estimation, training data for endangered species is limited, the animals are in complex natural scenes with occlusion, they are naturally camouflaged, travel in herds, and look similar to each other.  To overcome these challenges, we integrate the recent SMAL animal model into a network-based regression pipeline, which we train end-to-end on synthetically generated images with pose, shape, and background variation. Going beyond state-of-the-art methods for human shape and pose estimation, our method learns a shape space for zebras during training. \nLearning such a shape space from images using only a photometric loss is novel, and the approach can be used to learn shape in other settings with limited 3D supervision.\nMoreover, we couple 3D pose and shape prediction with the task of texture synthesis, obtaining a full texture map of the animal from a single image. \nWe show that the predicted texture map allows a novel per-instance unsupervised optimization over the network features. This method, SMALST (SMAL with learned Shape and Texture) goes beyond previous work, which assumed manual keypoints and/or segmentation, to regress directly from pixels to 3D animal shape, pose and texture. \nCode and data are available at https://github.com/silviazuffi/smalst &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/3d-animal-pose-shape&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, Berger-Wolf, T., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/3d-animal-pose-shape\">Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images \"In the Wild\"</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/silviazuffi/smalst\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/533/6034_after_pdfexpress.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/535/6034_supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supmat</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/548/iccv19_smalst_talk_compressed.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  iccv19 presentation</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Animal Shape\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/capturing-animal-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22661/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/nitin_iccv_19\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles\" src=\"/uploads/publication/image/22648/thumb_xl_ICCV_2_cover_crop_small.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/nitin_iccv_19\">Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/renficiaud\">Enficiaud, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, Martinovi\u0107, I., <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22648\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22648\" href=\"#abstractContent22648\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22648\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tCapturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/Aircap_Pose_Estimator\"><i class=\"fa fa-github-square\"></i>  Code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://aircapdata.is.tue.mpg.de/aircap-pose-estimator\"><i class=\"fa fa-file-o\"></i>  Data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/0w_yK20sVRU\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/544/5116.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper Manuscript</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: 3D Motion Capture\" class=\"btn btn-default btn-xs\" href=\"/research_projects/aircap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22648/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/nitin_iccv_19&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles.: https://ps.is.tuebingen.mpg.de/person/black/nitin_iccv_19&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/nitin_iccv_19&amp;amp;title=Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles. &amp;amp;summary=Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles. %20https://ps.is.tuebingen.mpg.de/person/black/nitin_iccv_19&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/nitin_iccv_19&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nsaini\">Saini, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/renficiaud\">Enficiaud, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, Martinovi\u0107, I., <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/nitin_iccv_19\">Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>International Conference on Computer Vision</em>, October 2019 <small class=\"text-muted\">(inproceedings)</small> <span class=\"label label-light\">Accepted</span>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/robot-perception-group/Aircap_Pose_Estimator\"><i class=\"fa fa-github-square\"></i>  Code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://aircapdata.is.tue.mpg.de/aircap-pose-estimator\"><i class=\"fa fa-file-o\"></i>  Data</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/0w_yK20sVRU\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/544/5116.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper Manuscript</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: 3D Motion Capture\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/aircap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22648/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/rahul_ral_2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Active Perception based Formation Control for Multiple Aerial Vehicles\" src=\"/uploads/publication/image/22644/thumb_xl_autonomous_mocap_cover_image_new.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/rahul_ral_2019\">Active Perception based Formation Control for Multiple Aerial Vehicles</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, Karlapalem, K., B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, <em>Robotics and Automation Letters</em>, 4(4):4491-4498, IEEE, October 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22644\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22644\" href=\"#abstractContent22644\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22644\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe present a novel robotic front-end for autonomous aerial motion-capture (mocap) in outdoor environments. In previous work, we presented an approach for cooperative detection and tracking (CDT) of a subject using multiple micro-aerial vehicles (MAVs). However, it did not ensure optimal view-point configurations of the MAVs to minimize the uncertainty in the person's cooperatively tracked 3D position estimate. In this article, we introduce an active approach for CDT. In contrast to cooperatively tracking only the 3D positions of the person, the MAVs can actively compute optimal local motion plans, resulting in optimal view-point configurations, which minimize the uncertainty in the tracked estimate. We achieve this by decoupling the goal of active tracking into a quadratic objective and non-convex constraints corresponding to angular configurations of the MAVs w.r.t. the person. We derive this decoupling using Gaussian observation model assumptions within the CDT algorithm. We preserve convexity in optimization by embedding all the non-convex constraints, including those for dynamic obstacle avoidance, as external control inputs in the MPC dynamics. Multiple real robot experiments and comparisons involving 3 MAVs in several challenging scenarios are presented.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8784232\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2019.2932570\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: Perception-Based Control\" class=\"btn btn-default btn-xs\" href=\"/research_projects/autonomous-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22644/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/rahul_ral_2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We present a novel robotic front-end for autonomous aerial motion-capture (mocap) in outdoor environments. In previous work, we presented an approach for cooperative detection and tracking (CDT) of a subject using multiple micro-aerial vehicles (MAVs). However, it did not ensure optimal view-point configurations of the MAVs to minimize the uncertainty in the person&amp;#39;s cooperatively tracked 3D position estimate. In this article, we introduce an active approach for CDT. In contrast to cooperatively tracking only the 3D positions of the person, the MAVs can actively compute optimal local motion plans, resulting in optimal view-point configurations, which minimize the uncertainty in the tracked estimate. We achieve this by decoupling the goal of active tracking into a quadratic objective and non-convex constraints corresponding to angular configurations of the MAVs w.r.t. the person. We derive this decoupling using Gaussian observation model assumptions within the CDT algorithm. We preserve convexity in optimization by embedding all the non-convex constraints, including those for dynamic obstacle avoidance, as external control inputs in the MPC dynamics. Multiple real robot experiments and comparisons involving 3 MAVs in several challenging scenarios are presented.: https://ps.is.tuebingen.mpg.de/person/black/rahul_ral_2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/rahul_ral_2019&amp;amp;title=We present a novel robotic front-end for autonomous aerial motion-capture (mocap) in outdoor environments. In previous work, we presented an approach for cooperative detection and tracking (CDT) of a subject using multiple micro-aerial vehicles (MAVs). However, it did not ensure optimal view-point configurations of the MAVs to minimize the uncertainty in the person&amp;#39;s cooperatively tracked 3D position estimate. In this article, we introduce an active approach for CDT. In contrast to cooperatively tracking only the 3D positions of the person, the MAVs can actively compute optimal local motion plans, resulting in optimal view-point configurations, which minimize the uncertainty in the tracked estimate. We achieve this by decoupling the goal of active tracking into a quadratic objective and non-convex constraints corresponding to angular configurations of the MAVs w.r.t. the person. We derive this decoupling using Gaussian observation model assumptions within the CDT algorithm. We preserve convexity in optimization by embedding all the non-convex constraints, including those for dynamic obstacle avoidance, as external control inputs in the MPC dynamics. Multiple real robot experiments and comparisons involving 3 MAVs in several challenging scenarios are presented. &amp;amp;summary=Active Perception based Formation Control for Multiple Aerial Vehicles&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=We present a novel robotic front-end for autonomous aerial motion-capture (mocap) in outdoor environments. In previous work, we presented an approach for cooperative detection and tracking (CDT) of a subject using multiple micro-aerial vehicles (MAVs). However, it did not ensure optimal view-point configurations of the MAVs to minimize the uncertainty in the person&amp;#39;s cooperatively tracked 3D position estimate. In this article, we introduce an active approach for CDT. In contrast to cooperatively tracking only the 3D positions of the person, the MAVs can actively compute optimal local motion plans, resulting in optimal view-point configurations, which minimize the uncertainty in the tracked estimate. We achieve this by decoupling the goal of active tracking into a quadratic objective and non-convex constraints corresponding to angular configurations of the MAVs w.r.t. the person. We derive this decoupling using Gaussian observation model assumptions within the CDT algorithm. We preserve convexity in optimization by embedding all the non-convex constraints, including those for dynamic obstacle avoidance, as external control inputs in the MPC dynamics. Multiple real robot experiments and comparisons involving 3 MAVs in several challenging scenarios are presented. %20https://ps.is.tuebingen.mpg.de/person/black/rahul_ral_2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=We present a novel robotic front-end for autonomous aerial motion-capture (mocap) in outdoor environments. In previous work, we presented an approach for cooperative detection and tracking (CDT) of a subject using multiple micro-aerial vehicles (MAVs). However, it did not ensure optimal view-point configurations of the MAVs to minimize the uncertainty in the person&amp;#39;s cooperatively tracked 3D position estimate. In this article, we introduce an active approach for CDT. In contrast to cooperatively tracking only the 3D positions of the person, the MAVs can actively compute optimal local motion plans, resulting in optimal view-point configurations, which minimize the uncertainty in the tracked estimate. We achieve this by decoupling the goal of active tracking into a quadratic objective and non-convex constraints corresponding to angular configurations of the MAVs w.r.t. the person. We derive this decoupling using Gaussian observation model assumptions within the CDT algorithm. We preserve convexity in optimization by embedding all the non-convex constraints, including those for dynamic obstacle avoidance, as external control inputs in the MPC dynamics. Multiple real robot experiments and comparisons involving 3 MAVs in several challenging scenarios are presented. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/rahul_ral_2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, Karlapalem, K., B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/rahul_ral_2019\">Active Perception based Formation Control for Multiple Aerial Vehicles</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, <em>Robotics and Automation Letters</em>, 4(4):4491-4498, IEEE, October 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8784232\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2019.2932570\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"AirCap: Perception-Based Control\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/autonomous-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22644/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/amass-iccv-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"{AMASS}: Archive of Motion Capture as Surface Shapes\" src=\"/uploads/publication/image/22663/thumb_xl_amass.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/amass-iccv-2019\">AMASS: Archive of Motion Capture as Surface Shapes</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nghorbani\">Ghorbani, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ntroje\">Troje, N. F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>International Conference on Computer Vision</em>,  pages: 5442-5451, October 2019 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22663\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22663\" href=\"#abstractContent22663\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22663\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tLarge datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [26], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker-sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyper-parameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections,\nhaving more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and is available for research at https://amass.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/nghorbani/amass\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass-sup.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1904.03278\"><i class=\"fa fa-file-o\"></i>  arxiv</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://amass.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  project website</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/cceRrlnTCEs\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass_iccv_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/549/6302_AMASS.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  AMASS_Poster</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22663/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/amass-iccv-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [26], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker-sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyper-parameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections,\nhaving more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and is available for research at https://amass.is.tue.mpg.de/.: https://ps.is.tuebingen.mpg.de/person/black/amass-iccv-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/amass-iccv-2019&amp;amp;title=Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [26], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker-sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyper-parameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections,\nhaving more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and is available for research at https://amass.is.tue.mpg.de/. &amp;amp;summary={AMASS}: Archive of Motion Capture as Surface Shapes&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [26], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker-sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyper-parameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections,\nhaving more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and is available for research at https://amass.is.tue.mpg.de/. %20https://ps.is.tuebingen.mpg.de/person/black/amass-iccv-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [26], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker-sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyper-parameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections,\nhaving more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and is available for research at https://amass.is.tue.mpg.de/. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/amass-iccv-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nghorbani\">Ghorbani, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ntroje\">Troje, N. F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/amass-iccv-2019\">AMASS: Archive of Motion Capture as Surface Shapes</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>International Conference on Computer Vision</em>,  pages: 5442-5451, October 2019 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/nghorbani/amass\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass-sup.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1904.03278\"><i class=\"fa fa-file-o\"></i>  arxiv</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://amass.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  project website</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/cceRrlnTCEs\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/amass_iccv_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/549/6302_AMASS.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  AMASS_Poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22663/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/loper2015mosh\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Method for providing a three dimensional body model\" src=\"/uploads/publication/image/18594/thumb_xl_MoSh_heroes_icon.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/loper2015mosh\">Method for providing a three dimensional body model</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tSeptember 2019, U.S.~Patent 10,417,818 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion18594\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion18594\" href=\"#abstractContent18594\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent18594\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tA method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://mosh.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  MoSh Project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/moshpatent.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/18594/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/loper2015mosh&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - A method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations.: https://ps.is.tuebingen.mpg.de/person/black/loper2015mosh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/loper2015mosh&amp;amp;title=A method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations. &amp;amp;summary=Method for providing a three dimensional body model&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=A method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations. %20https://ps.is.tuebingen.mpg.de/person/black/loper2015mosh&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=A method for providing a three-dimensional body model which may be applied for an animation, based on a moving body, wherein the method comprises providing a parametric three-dimensional body model, which allows shape and pose variations; applying a standard set of body markers; optimizing the set of body markers by generating an additional set of body markers and applying the same for providing 3D coordinate marker signals for capturing shape and pose of the body and dynamics of soft tissue; and automatically providing an animation by processing the 3D coordinate marker signals in order to provide a personalized three-dimensional body model, based on estimated shape and an estimated pose of the body by means of predicted marker locations. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/loper2015mosh&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/loper2015mosh\">Method for providing a three dimensional body model</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tSeptember 2019, U.S.~Patent 10,417,818 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://mosh.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  MoSh Project</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/moshpatent.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/18594/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/hoffmann-gcpr-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning to Train with Synthetic Humans\" src=\"/uploads/publication/image/22642/thumb_xl_lala2.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hoffmann-gcpr-2019\">Learning to Train with Synthetic Humans</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dhoffmann\">Hoffmann, D. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, September 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22642\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22642\" href=\"#abstractContent22642\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22642\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tNeural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans, as well as a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that this student-teacher framework outperforms all our baselines.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/526/GCPR19_LTSH_paper.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/527/GCPR19_LTSH_supmat.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/536/poster_LTSH.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ltsh.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22642/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/hoffmann-gcpr-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Neural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans, as well as a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that this student-teacher framework outperforms all our baselines.: https://ps.is.tuebingen.mpg.de/person/black/hoffmann-gcpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/hoffmann-gcpr-2019&amp;amp;title=Neural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans, as well as a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that this student-teacher framework outperforms all our baselines. &amp;amp;summary=Learning to Train with Synthetic Humans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Neural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans, as well as a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that this student-teacher framework outperforms all our baselines. %20https://ps.is.tuebingen.mpg.de/person/black/hoffmann-gcpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Neural networks need big annotated datasets for training. However, manual annotation can be too expensive or even unfeasible for certain tasks, like multi-person 2D pose estimation with severe occlusions. A remedy for this is synthetic data with perfect ground truth. Here we explore two variations of synthetic data for this challenging problem; a dataset with purely synthetic humans, as well as a real dataset augmented with synthetic humans. We then study which approach better generalizes to real data, as well as the influence of virtual humans in the training loss. We observe that not all synthetic samples are equally informative for training, while the informative samples are different for each training stage. To exploit this observation, we employ an adversarial student-teacher framework; the teacher improves the student by providing the hardest samples for its current state as a challenge. Experiments show that this student-teacher framework outperforms all our baselines. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/hoffmann-gcpr-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dhoffmann\">Hoffmann, D. T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/hoffmann-gcpr-2019\">Learning to Train with Synthetic Humans</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, September 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/526/GCPR19_LTSH_paper.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/527/GCPR19_LTSH_supmat.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/536/poster_LTSH.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ltsh.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22642/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/thaler-sap-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"The Influence of Visual Perspective on Body Size Estimation in Immersive Virtual Reality\" src=\"/uploads/publication/image/22666/thumb_xl_sap.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/thaler-sap-2019\">The Influence of Visual Perspective on Body Size Estimation in Immersive Virtual Reality</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, Stefanucci, J. K., Creem-Regehr, S. H., <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>ACM Symposium on Applied Perception</em>, September 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22666\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22666\" href=\"#abstractContent22666\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22666\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe creation of realistic self-avatars that users identify with is important for many virtual reality applications. However, current approaches for creating biometrically plausible avatars that represent a particular individual require expertise and are time-consuming. We investigated the visual perception of an avatar\u2019s body dimensions by asking males and females to estimate their own body weight and shape on a virtual body using a virtual reality avatar creation tool. In a method of adjustment task, the virtual body was presented in an HTC Vive head-mounted display either co-located with (first-person perspective) or facing (third-person perspective) the participants. Participants adjusted the body weight and dimensions of various body parts to match their own body shape and size. Both males and females underestimated their weight by 10-20% in the virtual body, but the estimates of the other body dimensions were relatively accurate and within a range of \u00b16%. There was a stronger influence of visual perspective on the estimates for males, but this effect was dependent on the amount of control over the shape of the virtual body, indicating that the results might be caused by where in the body the weight changes expressed themselves. These results suggest that this avatar creation tool could be used to allow participants to make a relatively accurate self-avatar in terms of adjusting body part dimensions, but not weight, and that the influence of visual perspective and amount of control needed\nover the body shape are likely gender-specific.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/sap19-17.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22666/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/thaler-sap-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The creation of realistic self-avatars that users identify with is important for many virtual reality applications. However, current approaches for creating biometrically plausible avatars that represent a particular individual require expertise and are time-consuming. We investigated the visual perception of an avatar\u2019s body dimensions by asking males and females to estimate their own body weight and shape on a virtual body using a virtual reality avatar creation tool. In a method of adjustment task, the virtual body was presented in an HTC Vive head-mounted display either co-located with (first-person perspective) or facing (third-person perspective) the participants. Participants adjusted the body weight and dimensions of various body parts to match their own body shape and size. Both males and females underestimated their weight by 10-20% in the virtual body, but the estimates of the other body dimensions were relatively accurate and within a range of \u00b16%. There was a stronger influence of visual perspective on the estimates for males, but this effect was dependent on the amount of control over the shape of the virtual body, indicating that the results might be caused by where in the body the weight changes expressed themselves. These results suggest that this avatar creation tool could be used to allow participants to make a relatively accurate self-avatar in terms of adjusting body part dimensions, but not weight, and that the influence of visual perspective and amount of control needed\nover the body shape are likely gender-specific.: https://ps.is.tuebingen.mpg.de/person/black/thaler-sap-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/thaler-sap-2019&amp;amp;title=The creation of realistic self-avatars that users identify with is important for many virtual reality applications. However, current approaches for creating biometrically plausible avatars that represent a particular individual require expertise and are time-consuming. We investigated the visual perception of an avatar\u2019s body dimensions by asking males and females to estimate their own body weight and shape on a virtual body using a virtual reality avatar creation tool. In a method of adjustment task, the virtual body was presented in an HTC Vive head-mounted display either co-located with (first-person perspective) or facing (third-person perspective) the participants. Participants adjusted the body weight and dimensions of various body parts to match their own body shape and size. Both males and females underestimated their weight by 10-20% in the virtual body, but the estimates of the other body dimensions were relatively accurate and within a range of \u00b16%. There was a stronger influence of visual perspective on the estimates for males, but this effect was dependent on the amount of control over the shape of the virtual body, indicating that the results might be caused by where in the body the weight changes expressed themselves. These results suggest that this avatar creation tool could be used to allow participants to make a relatively accurate self-avatar in terms of adjusting body part dimensions, but not weight, and that the influence of visual perspective and amount of control needed\nover the body shape are likely gender-specific. &amp;amp;summary=The Influence of Visual Perspective on Body Size Estimation in Immersive Virtual Reality&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The creation of realistic self-avatars that users identify with is important for many virtual reality applications. However, current approaches for creating biometrically plausible avatars that represent a particular individual require expertise and are time-consuming. We investigated the visual perception of an avatar\u2019s body dimensions by asking males and females to estimate their own body weight and shape on a virtual body using a virtual reality avatar creation tool. In a method of adjustment task, the virtual body was presented in an HTC Vive head-mounted display either co-located with (first-person perspective) or facing (third-person perspective) the participants. Participants adjusted the body weight and dimensions of various body parts to match their own body shape and size. Both males and females underestimated their weight by 10-20% in the virtual body, but the estimates of the other body dimensions were relatively accurate and within a range of \u00b16%. There was a stronger influence of visual perspective on the estimates for males, but this effect was dependent on the amount of control over the shape of the virtual body, indicating that the results might be caused by where in the body the weight changes expressed themselves. These results suggest that this avatar creation tool could be used to allow participants to make a relatively accurate self-avatar in terms of adjusting body part dimensions, but not weight, and that the influence of visual perspective and amount of control needed\nover the body shape are likely gender-specific. %20https://ps.is.tuebingen.mpg.de/person/black/thaler-sap-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The creation of realistic self-avatars that users identify with is important for many virtual reality applications. However, current approaches for creating biometrically plausible avatars that represent a particular individual require expertise and are time-consuming. We investigated the visual perception of an avatar\u2019s body dimensions by asking males and females to estimate their own body weight and shape on a virtual body using a virtual reality avatar creation tool. In a method of adjustment task, the virtual body was presented in an HTC Vive head-mounted display either co-located with (first-person perspective) or facing (third-person perspective) the participants. Participants adjusted the body weight and dimensions of various body parts to match their own body shape and size. Both males and females underestimated their weight by 10-20% in the virtual body, but the estimates of the other body dimensions were relatively accurate and within a range of \u00b16%. There was a stronger influence of visual perspective on the estimates for males, but this effect was dependent on the amount of control over the shape of the virtual body, indicating that the results might be caused by where in the body the weight changes expressed themselves. These results suggest that this avatar creation tool could be used to allow participants to make a relatively accurate self-avatar in terms of adjusting body part dimensions, but not weight, and that the influence of visual perspective and amount of control needed\nover the body shape are likely gender-specific. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/thaler-sap-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, Stefanucci, J. K., Creem-Regehr, S. H., <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/thaler-sap-2019\">The Influence of Visual Perspective on Body Size Estimation in Immersive Virtual Reality</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>ACM Symposium on Applied Perception</em>, September 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/sap19-17.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22666/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/adversarial-collaboration\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation\" src=\"/uploads/publication/image/20171/thumb_xl_teaser_results.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/adversarial-collaboration\">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vjampani\">Jampani, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lballes\">Balles, L.</a></span>, Kim, K., <span class=\"default-link-ul\"><a href=\"/person/dsun\">Sun, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jwulff\">Wulff, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20171\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20171\" href=\"#abstractContent20171\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20171\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/508/competitive_collaboration.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://github.com/anuragranj/cc\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Optical Flow\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Scene Models for Optical Flow\" class=\"btn btn-default btn-xs\" href=\"/research_projects/scene-models-for-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20171/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/adversarial-collaboration&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems.: https://ps.is.tuebingen.mpg.de/person/black/adversarial-collaboration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/adversarial-collaboration&amp;amp;title=We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems. &amp;amp;summary=Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems. %20https://ps.is.tuebingen.mpg.de/person/black/adversarial-collaboration&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=We address the unsupervised learning of several interconnected problems in low-level vision: single view depth prediction, camera motion estimation, optical flow, and segmentation of a video into the static scene and moving regions. Our key insight is that these four fundamental vision problems are coupled through geometric constraints. Consequently, learning to solve them together simplifies the problem because the solutions can reinforce each other. We go beyond previous work by exploiting geometry more explicitly and segmenting the scene into static and moving regions. To that end, we introduce Competitive Collaboration, a framework that facilitates the coordinated training of multiple specialized neural networks to solve complex problems. Competitive Collaboration works much like expectation-maximization, but with neural networks that act as both competitors to explain pixels that correspond to static or moving regions, and as collaborators through a moderator that assigns pixels to be either static or independently moving. Our novel method integrates all these problems in a common framework and simultaneously reasons about the segmentation of the scene into moving objects and the static background, the camera motion, depth of the static scene structure, and the optical flow of moving objects. Our model is trained without any supervision and achieves state-of-the-art performance among joint unsupervised methods on all sub-problems. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/adversarial-collaboration&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vjampani\">Jampani, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/lballes\">Balles, L.</a></span>, Kim, K., <span class=\"default-link-ul\"><a href=\"/person/dsun\">Sun, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jwulff\">Wulff, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/adversarial-collaboration\">Competitive Collaboration: Joint Unsupervised Learning of Depth, Camera Motion, Optical Flow and Motion Segmentation</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/508/competitive_collaboration.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://github.com/anuragranj/cc\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Optical Flow\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Scene Models for Optical Flow\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/scene-models-for-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20171/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/hasson-cvpr-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning Joint Reconstruction of Hands and Manipulated Objects\" src=\"/uploads/publication/image/22546/thumb_xl_obman_new.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hasson-cvpr-2019\">Learning Joint Reconstruction of Hands and Manipulated Objects</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tHasson, Y., <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Kalevatykh, I., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Laptev, I., <span class=\"default-link-ul\"><a href=\"/person/cschmid2\">Schmid, C.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 11807-11816, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22546\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22546\" href=\"#abstractContent22546\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22546\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tEstimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/499/obman.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/500/obman-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/522/obman_poster_cvpr.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.di.ens.fr/willow/research/obman\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Hands in Action\" class=\"btn btn-default btn-xs\" href=\"/research_projects/hands-in-action\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning from Synthetic Data\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-from-synthetic-data\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22546/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/hasson-cvpr-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.: https://ps.is.tuebingen.mpg.de/person/black/hasson-cvpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/hasson-cvpr-2019&amp;amp;title=Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data. &amp;amp;summary=Learning Joint Reconstruction of Hands and Manipulated Objects&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data. %20https://ps.is.tuebingen.mpg.de/person/black/hasson-cvpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made significant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to significant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object configurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/hasson-cvpr-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tHasson, Y., <span class=\"default-link-ul\"><a href=\"/person/gvarol\">Varol, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Kalevatykh, I., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Laptev, I., <span class=\"default-link-ul\"><a href=\"/person/cschmid2\">Schmid, C.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/hasson-cvpr-2019\">Learning Joint Reconstruction of Hands and Manipulated Objects</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 11807-11816, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/499/obman.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/500/obman-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/522/obman_poster_cvpr.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.di.ens.fr/willow/research/obman\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Hands in Action\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/hands-in-action\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning from Synthetic Data\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-from-synthetic-data\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22546/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/ringnet-cvpr-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision\" src=\"/uploads/publication/image/22553/thumb_xl_ringnet.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ringnet-cvpr-2019\">Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ssanyal\">Sanyal, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 7763-7772, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22553\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22553\" href=\"#abstractContent22553\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22553\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual\u2019s face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces \u201cnot quite in-the-wild\u201d (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/soubhiksanyal/RingNet\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/509/paper_camera_ready.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf preprint</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ringnet.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22553/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/ringnet-cvpr-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual\u2019s face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces \u201cnot quite in-the-wild\u201d (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes.: https://ps.is.tuebingen.mpg.de/person/black/ringnet-cvpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/ringnet-cvpr-2019&amp;amp;title=The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual\u2019s face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces \u201cnot quite in-the-wild\u201d (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes. &amp;amp;summary=Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual\u2019s face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces \u201cnot quite in-the-wild\u201d (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes. %20https://ps.is.tuebingen.mpg.de/person/black/ringnet-cvpr-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The estimation of 3D face shape from a single image must be robust to variations in lighting, head pose, expression, facial hair, makeup, and occlusions. Robustness requires a large training set of in-the-wild images, which by construction, lack ground truth 3D shape. To train a network without any 2D-to-3D supervision, we present RingNet, which learns to compute 3D face shape from a single image. Our key observation is that an individual\u2019s face shape is constant across images, regardless of expression, pose, lighting, etc. RingNet leverages multiple images of a person and automatically detected 2D face features. It uses a novel loss that encourages the face shape to be similar when the identity is the same and different for different people. We achieve invariance to expression by representing the face using the FLAME model. Once trained, our method takes a single image and outputs the parameters of FLAME, which can be readily animated. Additionally we create a new database of faces \u201cnot quite in-the-wild\u201d (NoW) with 3D head scans and high-resolution images of the subjects in a wide variety of conditions. We evaluate publicly available methods and find that RingNet is more accurate than methods that use 3D supervision. The dataset, model, and results are available for research purposes. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/ringnet-cvpr-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/ssanyal\">Sanyal, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/hfeng\">Feng, H.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/ringnet-cvpr-2019\">Learning to Regress 3D Face Shape and Expression from an Image without 3D Supervision</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 7763-7772, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/soubhiksanyal/RingNet\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/509/paper_camera_ready.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf preprint</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ringnet.is.tue.mpg.de/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22553/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/smplex-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Expressive Body Capture: 3D Hands, Face, and Body from a Single Image\" src=\"/uploads/publication/image/22547/thumb_xl_smplx_teaser_watermark.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/smplex-2019\">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/gpavlakos\">Pavlakos, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nghorbani\">Ghorbani, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aosman\">Osman, A. A. A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 10975-10985, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22547\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22547\" href=\"#abstractContent22547\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22547\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\t\nTo facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/XyXIEmapWkw\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/vchoutas/smplify-x\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/497/SMPL-X.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/498/SMPL-X-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/517/smplx_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://smpl-x.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Expressive Body Models\" class=\"btn btn-default btn-xs\" href=\"/research_projects/expressive-body-models\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22547/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/smplex-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - \nTo facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de.: https://ps.is.tuebingen.mpg.de/person/black/smplex-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/smplex-2019&amp;amp;title=\nTo facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de. &amp;amp;summary=Expressive Body Capture: 3D Hands, Face, and Body from a Single Image&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=\nTo facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de. %20https://ps.is.tuebingen.mpg.de/person/black/smplex-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=\nTo facilitate the analysis of human actions, interactions and emotions, we compute a 3D model of human body pose, hand pose, and facial expression from a single monocular image. To achieve this, we use thousands of 3D scans to train a new, unified, 3D model of the human body, SMPL-X, that extends SMPL with fully articulated hands and an expressive face. Learning to regress the parameters of SMPL-X directly from images is challenging without paired images and 3D ground truth. Consequently, we follow the approach of SMPLify, which estimates 2D features and then optimizes model parameters to fit the features. We improve on SMPLify in several significant ways: (1) we detect 2D features corresponding to the face, hands, and feet and fit the full SMPL-X model to these; (2) we train a new neural network pose prior using a large MoCap dataset; (3) we define a new interpenetration penalty that is both fast and accurate; (4) we automatically detect gender and the appropriate body models (male, female, or neutral); (5) our PyTorch implementation achieves a speedup of more than 8x over Chumpy. We use the new method, SMPLify-X, to fit SMPL-X to both controlled images and images in the wild. We evaluate 3D accuracy on a new curated dataset comprising 100 images with pseudo ground-truth. This is a step towards automatic expressive human capture from monocular RGB data. The models, code, and data are available for research purposes at https://smpl-x.is.tue.mpg.de. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/smplex-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/gpavlakos\">Pavlakos, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vchoutas\">Choutas, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nghorbani\">Ghorbani, N.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aosman\">Osman, A. A. A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/smplex-2019\">Expressive Body Capture: 3D Hands, Face, and Body from a Single Image</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 10975-10985, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/XyXIEmapWkw\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/vchoutas/smplify-x\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/497/SMPL-X.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/498/SMPL-X-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppl</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/517/smplx_poster.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  poster</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://smpl-x.is.tue.mpg.de\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Expressive Body Models\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/expressive-body-models\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22547/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/voca2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Capture, Learning, and Synthesis of 3D Speaking Styles\" src=\"/uploads/publication/image/22550/thumb_xl_voca.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/voca2019\">Capture, Learning, and Synthesis of 3D Speaking Styles</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dcudeiro\">Cudeiro, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/claidlaw\">Laidlaw, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22550\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22550\" href=\"#abstractContent22550\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22550\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAudio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\u2014even speech in languages other than English\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TimoBolkart/voca\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://voca.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/XceCxf_GyW4\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/510/paper_final.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22550/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/voca2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\u2014even speech in languages other than English\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de.: https://ps.is.tuebingen.mpg.de/person/black/voca2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/voca2019&amp;amp;title=Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\u2014even speech in languages other than English\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de. &amp;amp;summary=Capture, Learning, and Synthesis of 3D Speaking Styles&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\u2014even speech in languages other than English\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de. %20https://ps.is.tuebingen.mpg.de/person/black/voca2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\u2014even speech in languages other than English\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/voca2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dcudeiro\">Cudeiro, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/claidlaw\">Laidlaw, C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/voca2019\">Capture, Learning, and Synthesis of 3D Speaking Styles</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</em>, June 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TimoBolkart/voca\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://voca.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/XceCxf_GyW4\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/510/paper_final.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22550/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/hesse-pami-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning and Tracking the {3D} Body Shape of Freely Moving Infants from {RGB-D} sequences\" src=\"/uploads/publication/image/22626/thumb_xl_HessePAMI.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hesse-pami-2019\">Learning and Tracking the 3D Body Shape of Freely Moving Infants from RGB-D sequences</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tHesse, N., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Arens, M., Hofmann, U., Schroeder, S.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22626\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22626\" href=\"#abstractContent22626\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22626\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tStatistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8732396\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8732396\"><i class=\"fa fa-file-o\"></i>  Journal</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/TPAMI.2019.2917908\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22626/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/hesse-pami-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA.: https://ps.is.tuebingen.mpg.de/person/black/hesse-pami-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/hesse-pami-2019&amp;amp;title=Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA. &amp;amp;summary=Learning and Tracking the {3D} Body Shape of Freely Moving Infants from {RGB-D} sequences&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA. %20https://ps.is.tuebingen.mpg.de/person/black/hesse-pami-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Statistical models of the human body surface are generally learned from thousands of high-quality 3D scans in predefined poses to cover the wide variety of human body shapes and articulations. Acquisition of such data requires expensive equipment, calibration procedures, and is limited to cooperative subjects who can understand and follow instructions, such as adults. We present a method for learning a statistical 3D Skinned Multi-Infant Linear body model (SMIL) from incomplete, low-quality RGB-D sequences of freely moving infants. Quantitative experiments show that SMIL faithfully represents the RGB-D data and properly factorizes the shape and pose of the infants. To demonstrate the applicability of SMIL, we fit the model to RGB-D sequences of freely moving infants and show, with a case study, that our method captures enough motion detail for General Movements Assessment (GMA), a method used in clinical practice for early detection of neurodevelopmental disorders in infants. SMIL provides a new tool for analyzing infant shape and movement and is a step towards an automated system for GMA. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/hesse-pami-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tHesse, N., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, Arens, M., Hofmann, U., Schroeder, S.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/hesse-pami-2019\">Learning and Tracking the 3D Body Shape of Freely Moving Infants from RGB-D sequences</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8732396\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8732396\"><i class=\"fa fa-file-o\"></i>  Journal</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/TPAMI.2019.2917908\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22626/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/kenny-tap-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\" Perceptual Effects of Inconsistency in Human Animations\" src=\"/uploads/publication/image/22524/thumb_xl_kenny.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/kenny-tap-2019\"> Perceptual Effects of Inconsistency in Human Animations</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tKenny, S., <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, Honda, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ntroje\">Troje, N. F.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>ACM Trans. Appl. Percept.</em>, 16(1):2:1-2:18, Febuary 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22524\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22524\" href=\"#abstractContent22524\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22524\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person\u2019s movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer\u2019s individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dl.acm.org/citation.cfm?doid=3310277.3301411\"><i class=\"fa fa-file-o\"></i>  publisher</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dl.acm.org/ft_gateway.cfm?id=3301411&amp;ftid=2040160&amp;dwn=1&amp;CFID=110540843&amp;CFTOKEN=60a33bf0f64bbc85-61F40CE3-9030-DDBB-64357339747392D9\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3301411\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22524/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/kenny-tap-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person\u2019s movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer\u2019s individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion.: https://ps.is.tuebingen.mpg.de/person/black/kenny-tap-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/kenny-tap-2019&amp;amp;title=The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person\u2019s movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer\u2019s individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion. &amp;amp;summary= Perceptual Effects of Inconsistency in Human Animations&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person\u2019s movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer\u2019s individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion. %20https://ps.is.tuebingen.mpg.de/person/black/kenny-tap-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The individual shape of the human body, including the geometry of its articulated structure and the distribution of weight over that structure, influences the kinematics of a person\u2019s movements. How sensitive is the visual system to inconsistencies between shape and motion introduced by retargeting motion from one person onto the shape of another? We used optical motion capture to record five pairs of male performers with large differences in body weight, while they pushed, lifted, and threw objects. From these data, we estimated both the kinematics of the actions as well as the performer\u2019s individual body shape. To obtain consistent and inconsistent stimuli, we created animated avatars by combining the shape and motion estimates from either a single performer or from different performers. Using these stimuli we conducted three experiments in an immersive virtual reality environment. First, a group of participants detected which of two stimuli was inconsistent. Performance was very low, and results were only marginally significant. Next, a second group of participants rated perceived attractiveness, eeriness, and humanness of consistent and inconsistent stimuli, but these judgements of animation characteristics were not affected by consistency of the stimuli. Finally, a third group of participants rated properties of the objects rather than of the performers. Here, we found strong influences of shape-motion inconsistency on perceived weight and thrown distance of objects. This suggests that the visual system relies on its knowledge of shape and motion and that these components are assimilated into an altered perception of the action outcome. We propose that the visual system attempts to resist inconsistent interpretations of human animations. Actions involving object manipulations present an opportunity for the visual system to reinterpret the introduced inconsistencies as a change in the dynamics of an object rather than as an unexpected combination of body shape and body motion. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/kenny-tap-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tKenny, S., <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, Honda, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ntroje\">Troje, N. F.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/kenny-tap-2019\"> Perceptual Effects of Inconsistency in Human Animations</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>ACM Trans. Appl. Percept.</em>, 16(1):2:1-2:18, Febuary 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dl.acm.org/citation.cfm?doid=3310277.3301411\"><i class=\"fa fa-file-o\"></i>  publisher</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://dl.acm.org/ft_gateway.cfm?id=3301411&amp;ftid=2040160&amp;dwn=1&amp;CFID=110540843&amp;CFTOKEN=60a33bf0f64bbc85-61F40CE3-9030-DDBB-64357339747392D9\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1145/3301411\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22524/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/ps-sab-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Perceiving Systems (2016-2018)\" src=\"/uploads/publication/image/22641/thumb_xl_webteaser.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ps-sab-2019\">Perceiving Systems (2016-2018)</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\n\n\t\t\t\t\t\t\tScientific Advisory Board Report, 2019 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/PS-SAB-2019.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22641/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/ps-sab-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.tuebingen.mpg.de/person/black/ps-sab-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/ps-sab-2019&amp;amp;title= &amp;amp;summary=Perceiving Systems (2016-2018)&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url= %20https://ps.is.tuebingen.mpg.de/person/black/ps-sab-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/ps-sab-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/ps-sab-2019\">Perceiving Systems (2016-2018)</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tScientific Advisory Board Report, 2019 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/PS-SAB-2019.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22641/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/vcaliper-2019\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from {3D} Measurements\" src=\"/uploads/publication/image/22511/thumb_xl_virtualcaliper.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/vcaliper-2019\">The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, Hesse, N., B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Transactions on Visualization and Computer Graphics</em>, 25, pages: 1887,1897, IEEE, 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22511\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22511\" href=\"#abstractContent22511\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22511\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tCreating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://virtualcaliper.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8648222\"><i class=\"fa fa-file-o\"></i>  IEEE Open Access</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8648222\"><i class=\"fa fa-file-o\"></i>  IEEE Open Access PDF</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/TVCG.2019.2898748\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22511/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/vcaliper-2019&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned.: https://ps.is.tuebingen.mpg.de/person/black/vcaliper-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/vcaliper-2019&amp;amp;title=Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned. &amp;amp;summary=The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from {3D} Measurements&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned. %20https://ps.is.tuebingen.mpg.de/person/black/vcaliper-2019&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating \u201cThe Virtual Caliper\u201d, which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/vcaliper-2019&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jtesch\">Tesch, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/nmahmood\">Mahmood, N.</a></span>, Hesse, N., B\u00fclthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/vcaliper-2019\">The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>IEEE Transactions on Visualization and Computer Graphics</em>, 25, pages: 1887,1897, IEEE, 2019 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://virtualcaliper.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8648222\"><i class=\"fa fa-file-o\"></i>  IEEE Open Access</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8648222\"><i class=\"fa fa-file-o\"></i>  IEEE Open Access PDF</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/TVCG.2019.2898748\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22511/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/ghosh2019resisting\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders\" src=\"/uploads/publication/image/20771/thumb_xl_Model.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ghosh2019resisting\">Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh, P.</a></span>, Losalka, A., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Proc. AAAI</em>, 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20771\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20771\" href=\"#abstractContent20771\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20771\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tSusceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples\" and ``fooling samples\", have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1806.00081\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Deep Representations of 3D\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-high-dimensional-deep-representations\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20771/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/ghosh2019resisting&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples&amp;quot; and ``fooling samples&amp;quot;, have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization.: https://ps.is.tuebingen.mpg.de/person/black/ghosh2019resisting&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/ghosh2019resisting&amp;amp;title=Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples&amp;quot; and ``fooling samples&amp;quot;, have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization. &amp;amp;summary=Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples&amp;quot; and ``fooling samples&amp;quot;, have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization. %20https://ps.is.tuebingen.mpg.de/person/black/ghosh2019resisting&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Susceptibility of deep neural networks to adversarial attacks poses a major theoretical and practical challenge. All efforts to harden classifiers against such attacks have seen limited success till now. Two distinct categories of samples against which deep neural networks are vulnerable, ``adversarial samples&amp;quot; and ``fooling samples&amp;quot;, have been tackled separately so far due to the difficulty posed when considered together. In this work, we show how one can defend against them both under a unified framework. Our model has the form of a variational autoencoder with a Gaussian mixture prior on the latent variable, such that each mixture component corresponds to a single class. We show how selective classification can be performed using this model, thereby causing the adversarial objective to entail a conflict. The proposed method leads to the rejection of adversarial samples instead of misclassification, while maintaining high precision and recall on test data. It also inherently provides a way of learning a selective classifier in a semi-supervised scenario, which can similarly resist adversarial attacks. We further show how one can reclassify the detected adversarial samples by iterative optimization. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/ghosh2019resisting&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh, P.</a></span>, Losalka, A., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/ghosh2019resisting\">Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Proc. AAAI</em>, 2019 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1806.00081\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Deep Representations of 3D\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-high-dimensional-deep-representations\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20771/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/ghoshetal19\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"From Variational to Deterministic Autoencoders\" src=\"/uploads/publication/image/22551/thumb_xl_RAE.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/ghoshetal19\">From Variational to Deterministic Autoencoders</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh*, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/msajjadi\">Sajjadi*, M. S. M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/avergari\">Vergari, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/bs\">Sch\u00f6lkopf, B.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t2019, *equal contribution <small class=\"text-muted\">(conference)</small> <span class=\"label label-light\">Submitted</span>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22551\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22551\" href=\"#abstractContent22551\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22551\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tVariational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative\nmodels. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1903.12436\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22551/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/ghoshetal19&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Variational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative\nmodels. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets.: https://ps.is.tuebingen.mpg.de/person/black/ghoshetal19&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/ghoshetal19&amp;amp;title=Variational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative\nmodels. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets. &amp;amp;summary=From Variational to Deterministic Autoencoders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Variational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative\nmodels. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets. %20https://ps.is.tuebingen.mpg.de/person/black/ghoshetal19&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Variational Autoencoders (VAEs) provide a theoretically-backed framework for deep generative\nmodels. However, they often produce \u201cblurry\u201d images, which is linked to their training objective. Sampling in the most popular implementation, the Gaussian VAE, can be interpreted as simply injecting noise to the input of a deterministic decoder. In practice, this simply enforces a smooth latent space structure. We challenge the adoption of the full VAE framework on this specific point in favor of a simpler, deterministic one. Specifically, we investigate how substituting stochasticity with other explicit and implicit regularization schemes can lead to a meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism for sampling new data points, we propose to employ an efficient ex-post density estimation step that can be readily adopted both for the proposed deterministic autoencoders as well as to improve sample quality of existing VAEs. We show in a rigorous empirical study that regularized deterministic autoencoding achieves state-of-the-art sample quality on the common MNIST, CIFAR-10 and CelebA datasets. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/ghoshetal19&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/pghosh\">Ghosh*, P.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/msajjadi\">Sajjadi*, M. S. M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/avergari\">Vergari, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/bs\">Sch\u00f6lkopf, B.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/ghoshetal19\">From Variational to Deterministic Autoencoders</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t2019, *equal contribution <small class=\"text-muted\">(conference)</small> <span class=\"label label-light\">Submitted</span>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1903.12436\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22551/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12 publication-year-container-list-margin-top\">\n\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2018</span></h4>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr-top\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/matang-accv-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Customized Multi-Person Tracker\" src=\"/uploads/publication/image/20764/thumb_xl_imgidx-00326.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/matang-accv-2018\">Customized Multi-Person Tracker</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tMa, L., <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Van Gool, L.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Computer Vision \u2013  ACCV 2018</em>, Springer International Publishing, December 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/469/0509.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  PDF</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Groups and Crowds\" class=\"btn btn-default btn-xs\" href=\"/research_projects/groups-and-crowds\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20764/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/matang-accv-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.tuebingen.mpg.de/person/black/matang-accv-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/matang-accv-2018&amp;amp;title= &amp;amp;summary=Customized Multi-Person Tracker&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url= %20https://ps.is.tuebingen.mpg.de/person/black/matang-accv-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/matang-accv-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"col-md-12 publication-container-list publication-year-container-list-margin-top\">\n\t\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2018</span></h4>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-20\"></div>\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tMa, L., <span class=\"default-link-ul\"><a href=\"/person/stang\">Tang, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Van Gool, L.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/matang-accv-2018\">Customized Multi-Person Tracker</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Computer Vision \u2013  ACCV 2018</em>, Springer International Publishing, December 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/469/0509.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  PDF</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Groups and Crowds\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/groups-and-crowds\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20764/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/dip_sa18\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time\" src=\"/uploads/publication/image/20319/thumb_xl_dip_final.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/dip_sa18\">Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yhuang2\">Huang, Y.</a></span>, Kaufmann, M., Aksan, E., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)</em>, 37, pages: 185:1-185:15, ACM, November 2018, Two first authors contributed equally <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20319\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20319\" href=\"#abstractContent20319\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20319\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user's body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes. \n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eth-ait/dip18\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tuebingen.mpg.de/assets/dip.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf preprint</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tuebingen.mpg.de/assets/dip18_errata.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  errata</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/embed/p1fmpOWA504   \"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1145/3272127.3275108\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"IMU-based Human Motion Capture Systems\" class=\"btn btn-default btn-xs\" href=\"/research_projects/imu-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20319/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/dip_sa18&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user&amp;#39;s body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes. : https://ps.is.tuebingen.mpg.de/person/black/dip_sa18&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/dip_sa18&amp;amp;title=We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user&amp;#39;s body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.  &amp;amp;summary=Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user&amp;#39;s body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.  %20https://ps.is.tuebingen.mpg.de/person/black/dip_sa18&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=We demonstrate a novel deep neural network capable of reconstructing human full body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on the user&amp;#39;s body. In doing so, we address several difficult challenges. First, the problem is severely under-constrained as multiple pose parameters produce the same IMU orientations. Second, capturing IMU data in conjunction with ground-truth poses is expensive and difficult to do in many target application scenarios (e.g., outdoors). Third, modeling temporal dependencies through non-linear optimization has proven effective in prior work but makes real-time prediction infeasible. To address this important limitation, we learn the temporal pose priors using deep learning. To learn from sufficient data, we synthesize IMU data from motion capture datasets. A bi-directional RNN architecture leverages past and future information that is available at training time. At test time, we deploy the network in a sliding window fashion, retaining real time capabilities. To evaluate our method, we recorded DIP-IMU, a dataset consisting of 10 subjects wearing 17 IMUs for validation in 64 sequences with 330,000 time instants; this constitutes the largest IMU dataset publicly available. We quantitatively evaluate our approach on multiple datasets and show results from a real-time implementation. DIP-IMU and the code are available for research purposes.  &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/dip_sa18&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/yhuang2\">Huang, Y.</a></span>, Kaufmann, M., Aksan, E., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Hilliges, O., <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/dip_sa18\">Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse Inertial Measurements in Real Time</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>ACM Transactions on Graphics, (Proc. SIGGRAPH Asia)</em>, 37, pages: 185:1-185:15, ACM, November 2018, Two first authors contributed equally <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  data</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/eth-ait/dip18\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tuebingen.mpg.de/assets/dip.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf preprint</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dip.is.tuebingen.mpg.de/assets/dip18_errata.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  errata</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.youtube.com/embed/p1fmpOWA504   \"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1145/3272127.3275108\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"IMU-based Human Motion Capture Systems\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/imu-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20319/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/sevilla-gcpr-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"On the Integration of Optical Flow and Action Recognition\" src=\"/uploads/publication/image/20323/thumb_xl_SevillaGCPR.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/sevilla-gcpr-2018\">On the Integration of Optical Flow and Action Recognition</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/lsevilla\">Sevilla-Lara, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yliao\">Liao, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fguney\">G\u00fcney, F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vjampani\">Jampani, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, LNCS 11269, pages: 281-297, Springer, Cham, October 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20323\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20323\" href=\"#abstractContent20323\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20323\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMost of the top performing action recognition methods use optical flow as a \"black box\" input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1712.08416\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-12939-2_20\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20323/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/sevilla-gcpr-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Most of the top performing action recognition methods use optical flow as a &amp;quot;black box&amp;quot; input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities.: https://ps.is.tuebingen.mpg.de/person/black/sevilla-gcpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/sevilla-gcpr-2018&amp;amp;title=Most of the top performing action recognition methods use optical flow as a &amp;quot;black box&amp;quot; input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities. &amp;amp;summary=On the Integration of Optical Flow and Action Recognition&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Most of the top performing action recognition methods use optical flow as a &amp;quot;black box&amp;quot; input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities. %20https://ps.is.tuebingen.mpg.de/person/black/sevilla-gcpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Most of the top performing action recognition methods use optical flow as a &amp;quot;black box&amp;quot; input. Here we take a deeper look at the combination of flow and action recognition, and investigate why optical flow is helpful, what makes a flow method good for action recognition, and how we can make it better. In particular, we investigate the impact of different flow algorithms and input transformations to better understand how these affect a state-of-the-art action recognition method. Furthermore, we fine tune two neural-network flow methods end-to-end on the most widely used action recognition dataset (UCF101). Based on these experiments, we make the following five observations: 1) optical flow is useful for action recognition because it is invariant to appearance, 2) optical flow methods are optimized to minimize end-point-error (EPE), but the EPE of current methods is not well correlated with action recognition performance, 3) for the flow methods tested, accuracy at boundaries and at small displacements is most correlated with action recognition performance, 4) training optical flow to minimize classification error instead of minimizing EPE improves recognition performance, and 5) optical flow learned for the task of action recognition differs from traditional optical flow especially inside the human body and at the boundary of the body. These observations may encourage optical flow researchers to look beyond EPE as a goal and guide action recognition researchers to seek better motion cues, leading to a tighter integration of the optical flow and action recognition communities. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/sevilla-gcpr-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/lsevilla\">Sevilla-Lara, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/yliao\">Liao, Y.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fguney\">G\u00fcney, F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/vjampani\">Jampani, V.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/sevilla-gcpr-2018\">On the Integration of Optical Flow and Action Recognition</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, LNCS 11269, pages: 281-297, Springer, Cham, October 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1712.08416\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-12939-2_20\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20323/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/uavs-drones-mocap\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles\" src=\"/uploads/publication/image/20178/thumb_xl_cover.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/uavs-drones-mocap\">Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/glawless\">Lawless, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/imartinovic\">Martinovic, I.</a></span>, Buelthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, <em>Robotics and Automation Letters</em>, 3(4):3193-3200, IEEE, October 2018, Also accepted and presented in the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20178\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20178\" href=\"#abstractContent20178\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20178\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMulti-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/458/price_et_al_RAL_18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Published Version</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8394622/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2018.2850224\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20178/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/uavs-drones-mocap&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Multi-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community.: https://ps.is.tuebingen.mpg.de/person/black/uavs-drones-mocap&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/uavs-drones-mocap&amp;amp;title=Multi-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community. &amp;amp;summary=Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Multi-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community. %20https://ps.is.tuebingen.mpg.de/person/black/uavs-drones-mocap&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Multi-camera tracking of humans and animals in outdoor environments is a relevant and challenging problem. Our approach to it involves a team of cooperating micro aerial vehicles (MAVs) with on-board cameras only. DNNs often fail at objects with small scale or far away from the camera, which are typical characteristics of a scenario with aerial robots. Thus, the core problem addressed in this paper is how to achieve on-board, online, continuous and accurate vision-based detections using DNNs for visual person tracking through MAVs. Our solution leverages cooperation among multiple MAVs and active selection of most informative regions of image. We demonstrate the efficiency of our approach through simulations with up to 16 robots and real robot experiments involving two aerial robots tracking a person, while maintaining an active perception-driven formation. ROS-based source code is provided for the benefit of the community. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/uavs-drones-mocap&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/eprice\">Price, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/glawless\">Lawless, G.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/rludwig\">Ludwig, R.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/imartinovic\">Martinovic, I.</a></span>, Buelthoff, H. H., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/uavs-drones-mocap\">Deep Neural Network-based Cooperative Visual Tracking through Multiple Micro Aerial Vehicles</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>IEEE Robotics and Automation Letters</em>, <em>Robotics and Automation Letters</em>, 3(4):3193-3200, IEEE, October 2018, Also accepted and presented in the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/458/price_et_al_RAL_18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Published Version</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8394622/\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/LRA.2018.2850224\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20178/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/wulff-gcpr-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation\" src=\"/uploads/publication/image/20322/thumb_xl_Interpolation.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/wulff-gcpr-2018\">Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/jwulff\">Wulff, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, LNCS 11269, pages: 567-582, Springer, Cham, October 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20322\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20322\" href=\"#abstractContent20322\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20322\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe di\u000efficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then \ffine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation\nas a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi\fne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi\felds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/WulffGCPR18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1809.08317\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-12939-2_39\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Optical Flow\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20322/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/wulff-gcpr-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The di\u000efficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then \ffine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation\nas a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi\fne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi\felds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow.: https://ps.is.tuebingen.mpg.de/person/black/wulff-gcpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/wulff-gcpr-2018&amp;amp;title=The di\u000efficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then \ffine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation\nas a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi\fne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi\felds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow. &amp;amp;summary=Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The di\u000efficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then \ffine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation\nas a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi\fne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi\felds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow. %20https://ps.is.tuebingen.mpg.de/person/black/wulff-gcpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The di\u000efficulty of annotating training data is a major obstacle to using CNNs for low-level tasks in video. Synthetic data often does not generalize to real videos, while unsupervised methods require heuristic n losses. Proxy tasks can overcome these issues, and start by training a network for a task for which annotation is easier or which can be trained unsupervised. The trained network is then \ffine-tuned for the original task using small amounts of ground truth data. Here, we investigate frame interpolation\nas a proxy task for optical flow. Using real movies, we train a CNN unsupervised for temporal interpolation. Such a network implicitly estimates motion, but cannot handle untextured regions. By fi\fne-tuning on small amounts of ground truth flow, the network can learn to fill in homogeneous regions and compute full optical flow fi\felds. Using this unsupervised pre-training, our network outperforms similar architectures that were trained supervised using synthetic optical flow. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/wulff-gcpr-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/jwulff\">Wulff, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/wulff-gcpr-2018\">Temporal Interpolation as an Unsupervised Pretraining Task for Optical Flow Estimation</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>German Conference on Pattern Recognition (GCPR)</em>, LNCS 11269, pages: 567-582, Springer, Cham, October 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/WulffGCPR18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1809.08317\"><i class=\"fa fa-file-o\"></i>  arXiv</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-12939-2_39\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Optical Flow\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-optical-flow\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20322/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/coma\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Generating {3D} Faces using Convolutional Mesh Autoencoders\" src=\"/uploads/publication/image/20239/thumb_xl_coma_faces.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/coma\">Generating 3D Faces using Convolutional Mesh Autoencoders</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ssanyal\">Sanyal, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11207, pages: 725-741, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20239\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20239\" href=\"#abstractContent20239\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20239\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tLearned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/coma\"><i class=\"fa fa-github-square\"></i>  Code (tensorflow)</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/pixelite1201/pytorch_coma\"><i class=\"fa fa-github-square\"></i>  Code (pytorch)</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://coma.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/439/1285.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/440/1285-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supplementary</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01219-9_43\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Deep Representations of 3D\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-high-dimensional-deep-representations\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20239/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/coma&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/.: https://ps.is.tuebingen.mpg.de/person/black/coma&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/coma&amp;amp;title=Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/. &amp;amp;summary=Generating {3D} Faces using Convolutional Mesh Autoencoders&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/. %20https://ps.is.tuebingen.mpg.de/person/black/coma&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Learned 3D representations of human faces are useful for computer vision problems such as 3D face tracking and reconstruction from images, as well as graphics applications such as character generation and animation. Traditional models learn a latent representation of a face using linear subspaces or higher-order tensor generalizations. Due to this linearity, they can not capture extreme deformations and non-linear expressions. To address this, we introduce a versatile model that learns a non-linear representation of a face using spectral convolutions on a mesh surface. We introduce mesh sampling operations that enable a hierarchical mesh representation that captures non-linear variations in shape and expression at multiple scales within the model. In a variational setting, our model samples diverse realistic 3D faces from a multivariate Gaussian distribution. Our training data consists of 20,466 meshes of extreme expressions captured over 12 different subjects. Despite limited training data, our trained model outperforms state-of-the-art face models with 50% lower reconstruction error, while using 75% fewer parameters. We also show that, replacing the expression space of an existing state-of-the-art face model with our autoencoder, achieves a lower reconstruction error. Our data, model and code are available at http://coma.is.tue.mpg.de/. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/coma&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ssanyal\">Sanyal, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/coma\">Generating 3D Faces using Convolutional Mesh Autoencoders</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11207, pages: 725-741, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/coma\"><i class=\"fa fa-github-square\"></i>  Code (tensorflow)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/pixelite1201/pytorch_coma\"><i class=\"fa fa-github-square\"></i>  Code (pytorch)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://coma.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  Project Page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/439/1285.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/440/1285-supp.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supplementary</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01219-9_43\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning Deep Representations of 3D\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-high-dimensional-deep-representations\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20239/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/humanflow\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning Human Optical Flow\" src=\"/uploads/publication/image/20212/thumb_xl_persondetect__copy_.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/humanflow\">Learning Human Optical Flow</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em> 29th British Machine Vision Conference</em>, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20212\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20212\" href=\"#abstractContent20212\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20212\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/IFZbsDt9jMw\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/humanflow\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/433/bmvc_crc.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/humanflow\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning from Synthetic Data\" class=\"btn btn-default btn-xs\" href=\"/research_projects/learning-from-synthetic-data\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Optical Flow and Human Action\" class=\"btn btn-default btn-xs\" href=\"/research_projects/optical-flow-and-human-action\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20212/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/humanflow&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research.: https://ps.is.tuebingen.mpg.de/person/black/humanflow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/humanflow&amp;amp;title=The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research. &amp;amp;summary=Learning Human Optical Flow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research. %20https://ps.is.tuebingen.mpg.de/person/black/humanflow&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The optical flow of humans is well known to be useful for the analysis of human action. Given this, we devise an optical flow algorithm specifically for human motion and show that it is superior to generic flow methods. Designing a method by hand is impractical, so we develop a new training database of image sequences with ground truth optical flow. For this we use a 3D model of the human body and motion capture data to synthesize realistic flow fields. We then train a convolutional neural network to estimate human flow fields from pairs of images. Since many applications in human motion analysis depend on speed, and we anticipate mobile applications, we base our method on SpyNet with several modifications. We demonstrate that our trained network is more accurate than a wide range of top methods on held-out test data and that it generalizes well to real image sequences. When combined with a person detector/tracker, the approach provides a full solution to the problem of 2D human flow estimation. Both the code and the dataset are available for research. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/humanflow&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/humanflow\">Learning Human Optical Flow</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em> 29th British Machine Vision Conference</em>, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/IFZbsDt9jMw\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/humanflow\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/433/bmvc_crc.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/anuragranj/humanflow\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Learning from Synthetic Data\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/learning-from-synthetic-data\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Optical Flow and Human Action\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/optical-flow-and-human-action\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20212/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/janai2018eccv\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Unsupervised Learning of Multi-Frame Optical Flow with Occlusions \" src=\"/uploads/publication/image/20263/thumb_xl_joeleccv18.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/janai2018eccv\">Unsupervised Learning of Multi-Frame Optical Flow with Occlusions </a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/jjanai\">Janai, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fguney\">G\u00fcney, F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11220, pages: 713-731, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cvlibs.net/publications/Janai2018ECCV.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cvlibs.net/publications/Janai2018ECCV_supplementary.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppmat</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.youtube.com/watch?v=z7TZsAFDMMk\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/JJanai/back2future\"><i class=\"fa fa-github-square\"></i>  Project Page</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01270-0_42\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Unsupervised Learning of Flow with Occlusions\" class=\"btn btn-default btn-xs\" href=\"/research_projects/back2future\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20263/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/janai2018eccv&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - : https://ps.is.tuebingen.mpg.de/person/black/janai2018eccv&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/janai2018eccv&amp;amp;title= &amp;amp;summary=Unsupervised Learning of Multi-Frame Optical Flow with Occlusions &quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url= %20https://ps.is.tuebingen.mpg.de/person/black/janai2018eccv&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject= &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/janai2018eccv&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/jjanai\">Janai, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/fguney\">G\u00fcney, F.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aranjan\">Ranjan, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/ageiger\">Geiger, A.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/janai2018eccv\">Unsupervised Learning of Multi-Frame Optical Flow with Occlusions </a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11220, pages: 713-731, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cvlibs.net/publications/Janai2018ECCV.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cvlibs.net/publications/Janai2018ECCV_supplementary.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  suppmat</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.youtube.com/watch?v=z7TZsAFDMMk\"><i class=\"fa fa-file-video-o\"></i>  Video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/JJanai/back2future\"><i class=\"fa fa-github-square\"></i>  Project Page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01270-0_42\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Unsupervised Learning of Flow with Occlusions\" class=\"btn btn-default btn-xs\" href=\"https://avg.is.tuebingen.mpg.de/research_projects/back2future\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20263/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/hesse-micai-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning an Infant Body Model from {RGB-D} Data for Accurate Full Body Motion Analysis\" src=\"/uploads/publication/image/20182/thumb_xl_sample3_merge_black.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/hesse-micai-2018\">Learning an Infant Body Model from RGB-D Data for Accurate Full Body Motion Analysis</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tHesse, N., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Bodensteiner, C., Arens, M., Hofmann, U. G., Tacke, U., Hadders-Algra, M., Weinberger, R., Muller-Felber, W., Schroeder, A. S.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>Int. Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20182\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20182\" href=\"#abstractContent20182\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20182\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tInfant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/miccai18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://s.fhg.de/smil\"><i class=\"fa fa-file-o\"></i>  Project page</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/aahF1xGurmM\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1810.07538\"><i class=\"fa fa-file-o\"></i>  extended arXiv version</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-030-00928-1_89\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Medical Diagnosis\" class=\"btn btn-default btn-xs\" href=\"/research_projects/medical-diagnosis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20182/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/hesse-micai-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Infant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA.: https://ps.is.tuebingen.mpg.de/person/black/hesse-micai-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/hesse-micai-2018&amp;amp;title=Infant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA. &amp;amp;summary=Learning an Infant Body Model from {RGB-D} Data for Accurate Full Body Motion Analysis&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Infant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA. %20https://ps.is.tuebingen.mpg.de/person/black/hesse-micai-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Infant motion analysis enables early detection of neurodevelopmental disorders like cerebral palsy (CP). Diagnosis, however, is challenging, requiring expert human judgement. An automated solution would be beneficial but requires the accurate capture of 3D full-body movements. To that end, we develop a non-intrusive, low-cost, lightweight acquisition system that captures the shape and motion of infants. Going beyond work on modeling adult body shape, we learn a 3D Skinned Multi-Infant Linear body model (SMIL) from noisy, low-quality, and incomplete RGB-D data. We demonstrate the capture of shape and motion with 37 infants in a clinical environment. Quantitative experiments show that SMIL faithfully represents the data and properly factorizes the shape and pose of the infants. With a case study based on general movement assessment (GMA), we demonstrate that SMIL captures enough information to allow medical assessment. SMIL provides a new tool and a step towards a fully automatic system for GMA. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/hesse-micai-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tHesse, N., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Bodensteiner, C., Arens, M., Hofmann, U. G., Tacke, U., Hadders-Algra, M., Weinberger, R., Muller-Felber, W., Schroeder, A. S.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/hesse-micai-2018\">Learning an Infant Body Model from RGB-D Data for Accurate Full Body Motion Analysis</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>Int. Conf. on Medical Image Computing and Computer Assisted Intervention (MICCAI)</em>, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/miccai18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://s.fhg.de/smil\"><i class=\"fa fa-file-o\"></i>  Project page</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/aahF1xGurmM\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://arxiv.org/abs/1810.07538\"><i class=\"fa fa-file-o\"></i>  extended arXiv version</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1007/978-3-030-00928-1_89\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Medical Diagnosis\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/medical-diagnosis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20182/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/vip-eccv-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Recovering Accurate {3D} Human Pose in The Wild Using {IMUs} and a Moving Camera\" src=\"/uploads/publication/image/20269/thumb_xl_vip.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/vip-eccv-2018\">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tMarcard, T. V., Henschel, R., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Rosenhahn, B., <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11214, pages: 614-631, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20269\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20269\" href=\"#abstractContent20269\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20269\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate\n3D pose in challenging sequences, including walking in the city, going up-stairs, having co\u000bffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/VIP.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/VIPsup.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  SupMat</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://virtualhumans.mpi-inf.mpg.de/3DPW\"><i class=\"fa fa-file-o\"></i>  data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://virtualhumans.mpi-inf.mpg.de/3DPW/\"><i class=\"fa fa-file-o\"></i>  project</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01249-6_37\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"IMU-based Human Motion Capture Systems\" class=\"btn btn-default btn-xs\" href=\"/research_projects/imu-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20269/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/vip-eccv-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate\n3D pose in challenging sequences, including walking in the city, going up-stairs, having co\u000bffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW.: https://ps.is.tuebingen.mpg.de/person/black/vip-eccv-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/vip-eccv-2018&amp;amp;title=In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate\n3D pose in challenging sequences, including walking in the city, going up-stairs, having co\u000bffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW. &amp;amp;summary=Recovering Accurate {3D} Human Pose in The Wild Using {IMUs} and a Moving Camera&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate\n3D pose in challenging sequences, including walking in the city, going up-stairs, having co\u000bffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW. %20https://ps.is.tuebingen.mpg.de/person/black/vip-eccv-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMU-equipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW ), a new dataset consisting of more than 51; 000 frames with accurate\n3D pose in challenging sequences, including walking in the city, going up-stairs, having co\u000bffee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/vip-eccv-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tMarcard, T. V., Henschel, R., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Rosenhahn, B., <span class=\"default-link-ul\"><a href=\"/person/gpons\">Pons-Moll, G.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/vip-eccv-2018\">Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>European Conference on Computer Vision (ECCV)</em>, Lecture Notes in Computer Science, vol 11214, pages: 614-631, Springer, Cham, September 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/VIP.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/VIPsup.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  SupMat</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://virtualhumans.mpi-inf.mpg.de/3DPW\"><i class=\"fa fa-file-o\"></i>  data</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://virtualhumans.mpi-inf.mpg.de/3DPW/\"><i class=\"fa fa-file-o\"></i>  project</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.1007/978-3-030-01249-6_37\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"IMU-based Human Motion Capture Systems\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/imu-mocap\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20269/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/thaler-frontiers-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Visual Perception and Evaluation of Photo-Realistic Self-Avatars From {3D} Body Scans in Males and Females\" src=\"/uploads/publication/image/20769/thumb_xl_fict-05-00018-g003.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/thaler-frontiers-2018\">Visual Perception and Evaluation of Photo-Realistic Self-Avatars From 3D Body Scans in Males and Females</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, Piryankova, I., Stefanucci, J. K., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, de la Rosa, S., <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Frontiers in ICT</em>, 5, pages: 1-14, September 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20769\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20769\" href=\"#abstractContent20769\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20769\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant's body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participant\u2019s height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar's weight corresponded to their own weight. Participants also adjusted the avatar's weight to their desired weight and evaluated the avatar's appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.frontiersin.org/articles/10.3389/fict.2018.00018/full\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.3389/fict.2018.00018\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20769/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/thaler-frontiers-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant&amp;#39;s body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participant\u2019s height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar&amp;#39;s weight corresponded to their own weight. Participants also adjusted the avatar&amp;#39;s weight to their desired weight and evaluated the avatar&amp;#39;s appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars.: https://ps.is.tuebingen.mpg.de/person/black/thaler-frontiers-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/thaler-frontiers-2018&amp;amp;title=The creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant&amp;#39;s body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participant\u2019s height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar&amp;#39;s weight corresponded to their own weight. Participants also adjusted the avatar&amp;#39;s weight to their desired weight and evaluated the avatar&amp;#39;s appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars. &amp;amp;summary=Visual Perception and Evaluation of Photo-Realistic Self-Avatars From {3D} Body Scans in Males and Females&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant&amp;#39;s body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participant\u2019s height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar&amp;#39;s weight corresponded to their own weight. Participants also adjusted the avatar&amp;#39;s weight to their desired weight and evaluated the avatar&amp;#39;s appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars. %20https://ps.is.tuebingen.mpg.de/person/black/thaler-frontiers-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The creation or streaming of photo-realistic self-avatars is important for virtual reality applications that aim for perception and action to replicate real world experience. The appearance and recognition of a digital self-avatar may be especially important for applications related to telepresence, embodied virtual reality, or immersive games. We investigated gender differences in the use of visual cues (shape, texture) of a self-avatar for estimating body weight and evaluating avatar appearance. A full-body scanner was used to capture each participant&amp;#39;s body geometry and color information and a set of 3D virtual avatars with realistic weight variations was created based on a statistical body model. Additionally, a second set of avatars was created with an average underlying body shape matched to each participant\u2019s height and weight. In four sets of psychophysical experiments, the influence of visual cues on the accuracy of body weight estimation and the sensitivity to weight changes was assessed by manipulating body shape (own, average) and texture (own photo-realistic, checkerboard). The avatars were presented on a large-screen display, and participants responded to whether the avatar&amp;#39;s weight corresponded to their own weight. Participants also adjusted the avatar&amp;#39;s weight to their desired weight and evaluated the avatar&amp;#39;s appearance with regard to similarity to their own body, uncanniness, and their willingness to accept it as a digital representation of the self. The results of the psychophysical experiments revealed no gender difference in the accuracy of estimating body weight in avatars. However, males accepted a larger weight range of the avatars as corresponding to their own. In terms of the ideal body weight, females but not males desired a thinner body. With regard to the evaluation of avatar appearance, the questionnaire responses suggest that own photo-realistic texture was more important to males for higher similarity ratings, while own body shape seemed to be more important to females. These results argue for gender-specific considerations when creating self-avatars. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/thaler-frontiers-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, Piryankova, I., Stefanucci, J. K., <span class=\"default-link-ul\"><a href=\"/person/spujades\">Pujades, S.</a></span>, de la Rosa, S., <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/thaler-frontiers-2018\">Visual Perception and Evaluation of Photo-Realistic Self-Avatars From 3D Body Scans in Males and Females</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>Frontiers in ICT</em>, 5, pages: 1-14, September 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://www.frontiersin.org/articles/10.3389/fict.2018.00018/full\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/https://doi.org/10.3389/fict.2018.00018\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20769/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/gueorguiev18-hbtea-modelling\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction\" src=\"/uploads/publication/image/20316/thumb_xl_teaser_PS_HI.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/gueorguiev18-hbtea-modelling\">Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dgueorguiev\">Gueorguiev, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Pacchierotti, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tExtended abstract presented at the Hand, Brain and Technology conference (HBT), Ascona, Switzerland, August 2018 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20316\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20316\" href=\"#abstractContent20316\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20316\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tLittle is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the finger\u2019s deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skin\u2019s compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific user\u2019s finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Hand Deformations During Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/modeling-hand-deformations-during-contact\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20316/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hbtea-modelling&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Little is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the finger\u2019s deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skin\u2019s compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific user\u2019s finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues.: https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hbtea-modelling&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hbtea-modelling&amp;amp;title=Little is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the finger\u2019s deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skin\u2019s compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific user\u2019s finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues. &amp;amp;summary=Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Little is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the finger\u2019s deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skin\u2019s compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific user\u2019s finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues. %20https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hbtea-modelling&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Little is known about the shape and properties of the human finger during haptic interaction, even though these are essential parameters for controlling wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning (3D over time) and modelling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution while simultaneously recording the interfacial forces at the contact. Preliminary results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion and proximal/distal bending, deformations that cannot be captured by imaging of the contact area alone. Therefore, we are currently capturing a dataset that will enable us to create a statistical model of the finger\u2019s deformations and predict the contact forces induced by tactile interaction with objects. This technique could improve current methods for tactile rendering in wearable haptic devices, which rely on general physical modelling of the skin\u2019s compliance, by developing an accurate model of the variations in finger properties across the human population. The availability of such a model will also enable a more realistic simulation of virtual finger behaviour in virtual reality (VR) environments, as well as the ability to accurately model a specific user\u2019s finger from lower resolution data. It may also be relevant for inferring the physical properties of the underlying tissue from observing the surface mesh deformations, as previously shown for body tissues. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hbtea-modelling&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dgueorguiev\">Gueorguiev, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Pacchierotti, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/gueorguiev18-hbtea-modelling\">Statistical Modelling of Fingertip Deformations and Contact Forces during Tactile Interaction</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tExtended abstract presented at the Hand, Brain and Technology conference (HBT), Ascona, Switzerland, August 2018 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Hand Deformations During Contact\" class=\"btn btn-default btn-xs\" href=\"https://hi.is.mpg.de/research_projects/modeling-hand-deformations-during-contact\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20316/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/decmpc18\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Decentralized {MPC} based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios\" src=\"/uploads/publication/image/20186/thumb_xl_Aircap_CA_3.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/decmpc18\">Decentralized MPC based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, Rajappa, S., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Karlapalem, K., <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)</em>,  pages: 1-8, IEEE, August 2018 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20186\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20186\" href=\"#abstractContent20186\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20186\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tIn this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/459/tallamraju_et_al_SSRR_18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Published Version</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8468655\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/SSRR.2018.8468655\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20186/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/decmpc18&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - In this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm.: https://ps.is.tuebingen.mpg.de/person/black/decmpc18&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/decmpc18&amp;amp;title=In this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm. &amp;amp;summary=Decentralized {MPC} based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=In this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm. %20https://ps.is.tuebingen.mpg.de/person/black/decmpc18&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=In this work, we consider the problem of decentralized multi-robot target tracking and obstacle avoidance in dynamic environments. Each robot executes a local motion planning algorithm which is based on model predictive control (MPC). The planner is designed as a quadratic program, subject to constraints on robot dynamics and obstacle avoidance. Repulsive potential field functions are employed to avoid obstacles. The novelty of our approach lies in embedding these non-linear potential field functions as constraints within a convex optimization framework. Our method convexifies nonconvex constraints and dependencies, by replacing them as pre-computed external input forces in robot dynamics. The proposed algorithm additionally incorporates different methods to avoid field local minima problems associated with using potential field functions in planning. The motion planner does not enforce predefined trajectories or any formation geometry on the robots and is a comprehensive solution for cooperative obstacle avoidance in the context of multi-robot target tracking. We perform simulation studies for different scenarios to showcase the convergence and efficacy of the proposed algorithm. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/decmpc18&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/rtallamraju\">Tallamraju, R.</a></span>, Rajappa, S., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Karlapalem, K., <span class=\"default-link-ul\"><a href=\"/person/aahmad\">Ahmad, A.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/decmpc18\">Decentralized MPC based Obstacle Avoidance for Multi-Robot Target Tracking Scenarios</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)</em>,  pages: 1-8, IEEE, August 2018 <small class=\"text-muted\">(conference)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/459/tallamraju_et_al_SSRR_18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  Published Version</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://ieeexplore.ieee.org/document/8468655\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1109/SSRR.2018.8468655\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20186/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/alborno-sca-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Robust Physics-based Motion Retargeting with Realistic Body Shapes\" src=\"/uploads/publication/image/20185/thumb_xl_mazen.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/alborno-sca-2018\">Robust Physics-based Motion Retargeting with Realistic Body Shapes</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\tBorno, M. A., <span class=\"default-link-ul\"><a href=\"/person/lrighetti\">Righetti, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Delp, S. L., Fiume, E., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Computer Graphics Forum</em>, 37, pages: 6:1-12, July 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20185\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20185\" href=\"#abstractContent20185\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20185\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tMotion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each character\u2019s anatomy and their actions are robust to perturbations.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/sca18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cs.toronto.edu/~mazen/rsca18.mp4\"><i class=\"fa fa-file-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Human Movement\" class=\"btn btn-default btn-xs\" href=\"/research_projects/modeling-human-movement\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Physics of Body Shape and Motion\" class=\"btn btn-default btn-xs\" href=\"/research_projects/physics-shape-motion\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20185/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/alborno-sca-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each character\u2019s anatomy and their actions are robust to perturbations.: https://ps.is.tuebingen.mpg.de/person/black/alborno-sca-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/alborno-sca-2018&amp;amp;title=Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each character\u2019s anatomy and their actions are robust to perturbations. &amp;amp;summary=Robust Physics-based Motion Retargeting with Realistic Body Shapes&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each character\u2019s anatomy and their actions are robust to perturbations. %20https://ps.is.tuebingen.mpg.de/person/black/alborno-sca-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically-based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of motions from a single mocap sequence by simply varying the characters. In an interactive environment, successful retargeting requires adapting the motion to unexpected external forces. We achieve robustness to such forces using a novel LQR-tree formulation. We show that the simulated motions look appropriate to each character\u2019s anatomy and their actions are robust to perturbations. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/alborno-sca-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\tBorno, M. A., <span class=\"default-link-ul\"><a href=\"/person/lrighetti\">Righetti, L.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Delp, S. L., Fiume, E., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/alborno-sca-2018\">Robust Physics-based Motion Retargeting with Realistic Body Shapes</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>Computer Graphics Forum</em>, 37, pages: 6:1-12, July 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/sca18.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.cs.toronto.edu/~mazen/rsca18.mp4\"><i class=\"fa fa-file-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Human Movement\" class=\"btn btn-default btn-xs\" href=\"https://mg.is.tuebingen.mpg.de/research_projects/modeling-human-movement\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Physics of Body Shape and Motion\" class=\"btn btn-default btn-xs\" href=\"https://mg.is.tuebingen.mpg.de/research_projects/physics-shape-motion\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20185/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/black-pct-2009\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Method and Apparatus for Estimating Body Shape\" src=\"/uploads/publication/image/22/thumb_xl_patent2009.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/black-pct-2009\">Method and Apparatus for Estimating Body Shape</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Balan, A., <span class=\"default-link-ul\"><a href=\"/person/aweiss\">Weiss, A.</a></span>, Sigal, L., <span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, St Clair, T.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tJune 2018, U.S.~Patent 10,002,460 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion22\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion22\" href=\"#abstractContent22\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent22\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tA system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://patents.google.com/patent/US10002460B2/en?oq=10%2c002%2c460\"><i class=\"fa fa-file-o\"></i>  Google Patents</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Bodies from RGB-D\" class=\"btn btn-default btn-xs\" href=\"/research_projects/bodies-from-rgbd\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/black-pct-2009&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping.: https://ps.is.tuebingen.mpg.de/person/black/black-pct-2009&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/black-pct-2009&amp;amp;title=A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping. &amp;amp;summary=Method and Apparatus for Estimating Body Shape&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping. %20https://ps.is.tuebingen.mpg.de/person/black/black-pct-2009&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=A system and method of estimating the body shape of an individual from input data such as images or range maps. The body may appear in one or more poses captured at different times and a consistent body shape is computed for all poses. The body may appear in minimal tight-fitting clothing or in normal clothing wherein the described method produces an estimate of the body shape under the clothing. Clothed or bare regions of the body are detected via image classification and the fitting method is adapted to treat each region differently. Body shapes are represented parametrically and are matched to other bodies based on shape similarity and other features. Standard measurements are extracted using parametric or non-parametric functions of body shape. The system components support many applications in body scanning, advertising, social networking, collaborative filtering and Internet clothing shopping. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/black-pct-2009&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Balan, A., <span class=\"default-link-ul\"><a href=\"/person/aweiss\">Weiss, A.</a></span>, Sigal, L., <span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, St Clair, T.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/black-pct-2009\">Method and Apparatus for Estimating Body Shape</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tJune 2018, U.S.~Patent 10,002,460 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://patents.google.com/patent/US10002460B2/en?oq=10%2c002%2c460\"><i class=\"fa fa-file-o\"></i>  Google Patents</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Bodies from RGB-D\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/bodies-from-rgbd\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/22/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/molbert-pm-2017\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted\" src=\"/uploads/publication/image/19710/thumb_xl_ANimage2mask3.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/molbert-pm-2017\">Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">M\u00f6lbert, S. C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Zipfel, S., Karnath, H., Giel, K. E.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>Psychological Medicine</em>, 48(4):642-653, March 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion19710\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion19710\" href=\"#abstractContent19710\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent19710\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tBackground: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN.  Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of \u00b120% of the participants' weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity.\nResults: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body\nwas similar to Experiment 1.\nConclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://doi.org/10.1017/S0033291717002008\"><i class=\"fa fa-file-o\"></i>  doi</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tinyurl.com/yc56sd2t\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1017/S0033291717002008\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Anorexia and Body Shape\" class=\"btn btn-default btn-xs\" href=\"/research_projects/anorexia-and-body-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19710/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/molbert-pm-2017&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Background: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN.  Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of \u00b120% of the participants&amp;#39; weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity.\nResults: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body\nwas similar to Experiment 1.\nConclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight.: https://ps.is.tuebingen.mpg.de/person/black/molbert-pm-2017&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/molbert-pm-2017&amp;amp;title=Background: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN.  Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of \u00b120% of the participants&amp;#39; weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity.\nResults: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body\nwas similar to Experiment 1.\nConclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight. &amp;amp;summary=Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Background: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN.  Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of \u00b120% of the participants&amp;#39; weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity.\nResults: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body\nwas similar to Experiment 1.\nConclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight. %20https://ps.is.tuebingen.mpg.de/person/black/molbert-pm-2017&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Background: Body image disturbance (BID) is a core symptom of anorexia nervosa (AN), but as yet distinctive features of BID are unknown. The present study aimed at disentangling perceptual and attitudinal components of BID in AN.  Methods: We investigated n=24 women with AN and n=24 controls. Based on a 3D body scan, we created realistic virtual 3D bodies (avatars) for each participant that were varied through a range of \u00b120% of the participants&amp;#39; weights. Avatars were presented in a virtual reality mirror scenario. Using different psychophysical tasks, participants identified and adjusted their actual and their desired body weight. To test for general perceptual biases in estimating body weight, a second experiment investigated perception of weight and shape matched avatars with another identity.\nResults: Women with AN and controls underestimated their weight, with a trend that women with AN underestimated more. The average desired body of controls had normal weight while the average desired weight of women with AN corresponded to extreme AN (DSM-5). Correlation analyses revealed that desired body weight, but not accuracy of weight estimation, was associated with eating disorder symptoms. In the second experiment, both groups estimated accurately while the most attractive body\nwas similar to Experiment 1.\nConclusions: Our results contradict the widespread assumption that patients with AN overestimate their body weight due to visual distortions. Rather, they illustrate that BID might be driven by distorted attitudes with regard to the desired body. Clinical interventions should aim at helping patients with AN to change their desired weight. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/molbert-pm-2017&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/smoelbert\">M\u00f6lbert, S. C.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Zipfel, S., Karnath, H., Giel, K. E.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/molbert-pm-2017\">Assessing body image in anorexia nervosa using biometric self-avatars in virtual reality: Attitudinal components rather than visual body size estimation are distorted</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>Psychological Medicine</em>, 48(4):642-653, March 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://doi.org/10.1017/S0033291717002008\"><i class=\"fa fa-file-o\"></i>  doi</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://tinyurl.com/yc56sd2t\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/10.1017/S0033291717002008\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Anorexia and Body Shape\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/anorexia-and-body-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19710/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/gueorguiev18-hswip-model\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Towards a Statistical Model of Fingertip Contact Deformations from 4{D} Data\" src=\"/uploads/publication/image/20315/thumb_xl_teaser_PS_HI.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/gueorguiev18-hswip-model\">Towards a Statistical Model of Fingertip Contact Deformations from 4D Data</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dgueorguiev\">Gueorguiev, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Pacchierotti, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tWork-in-progress paper (3 pages) presented at the IEEE Haptics Symposium, San Francisco, USA, March 2018 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion20315\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion20315\" href=\"#abstractContent20315\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent20315\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tLittle is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30\u25e6, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the finger\u2019s behavior during haptic interaction.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://2018proc.hapticssymposium.org/hapticscomp18/hapticscomp18wip-p1022-p.pdf\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Hand Deformations During Contact\" class=\"btn btn-default btn-xs\" href=\"/research_projects/modeling-hand-deformations-during-contact\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20315/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hswip-model&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Little is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30\u25e6, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the finger\u2019s behavior during haptic interaction.: https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hswip-model&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hswip-model&amp;amp;title=Little is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30\u25e6, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the finger\u2019s behavior during haptic interaction. &amp;amp;summary=Towards a Statistical Model of Fingertip Contact Deformations from 4{D} Data&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Little is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30\u25e6, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the finger\u2019s behavior during haptic interaction. %20https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hswip-model&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Little is known about the shape and properties of the human finger during haptic interaction even though this knowledge is essential to control wearable finger devices and deliver realistic tactile feedback. This study explores a framework for four-dimensional scanning and modeling of finger-surface interactions, aiming to capture the motion and deformations of the entire finger with high resolution. The results show that when the fingertip is actively pressing a rigid surface, it undergoes lateral expansion of about 0.2 cm and proximal/distal bending of about 30\u25e6, deformations that cannot be captured by imaging of the contact area alone. This project constitutes a first step towards an accurate statistical model of the finger\u2019s behavior during haptic interaction. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/gueorguiev18-hswip-model&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/dgueorguiev\">Gueorguiev, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dtzionas\">Tzionas, D.</a></span>, Pacchierotti, C., <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/kjk\">Kuchenbecker, K. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/gueorguiev18-hswip-model\">Towards a Statistical Model of Fingertip Contact Deformations from 4D Data</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tWork-in-progress paper (3 pages) presented at the IEEE Haptics Symposium, San Francisco, USA, March 2018 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://2018proc.hapticssymposium.org/hapticscomp18/hapticscomp18wip-p1022-p.pdf\"><i class=\"fa fa-external-link\"></i>  link (url)</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Modeling Hand Deformations During Contact\" class=\"btn btn-default btn-xs\" href=\"https://hi.is.mpg.de/research_projects/modeling-hand-deformations-during-contact\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/20315/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/thaler-plos-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Body size estimation of self and others in females varying in {BMI}\" src=\"/uploads/publication/image/19927/thumb_xl_plos1.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/thaler-plos-2018\">Body size estimation of self and others in females varying in BMI</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, Geuss, M. N., <span class=\"default-link-ul\"><a href=\"/person/smoelbert\">M\u00f6lbert, S. C.</a></span>, Giel, K. E., <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>PLoS ONE</em>, 13(2), Febuary 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion19927\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion19927\" href=\"#abstractContent19927\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent19927\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tPrevious literature suggests that a disturbed ability to accurately identify own body size may\ncontribute to overweight. Here, we investigated the influence of personal body size, indexed\nby body mass index (BMI), on body size estimation in a non-clinical population of females\nvarying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal\ninfluences by manipulating whether participants believed the body stimuli (personalized\navatars with realistic weight variations) represented their own body or that of another\nperson. Our results show that the accuracy of own body size estimation is predicted by personal\nBMI, such that participants with lower BMI underestimated their body size and participants\nwith higher BMI overestimated their body size. Further, participants with higher BMI\nwere less likely to notice the same percentage of weight gain than participants with lower\nBMI. Importantly, these results were only apparent when participants were judging a virtual\nbody that was their own identity (Experiment 1), but not when they estimated the size of a\nbody with another identity and the same underlying body shape (Experiment 2a). The different\ninfluences of BMI on accuracy of body size estimation and sensitivity to weight change\nfor self and other identity suggests that effects of BMI on visual body size estimation are\nself-specific and not generalizable to other bodies.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192152\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192152\">DOI</a>\n\t\t\t\t\t\t</span>\t\t\t\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Anorexia and Body Shape\" class=\"btn btn-default btn-xs\" href=\"/research_projects/anorexia-and-body-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19927/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/thaler-plos-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Previous literature suggests that a disturbed ability to accurately identify own body size may\ncontribute to overweight. Here, we investigated the influence of personal body size, indexed\nby body mass index (BMI), on body size estimation in a non-clinical population of females\nvarying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal\ninfluences by manipulating whether participants believed the body stimuli (personalized\navatars with realistic weight variations) represented their own body or that of another\nperson. Our results show that the accuracy of own body size estimation is predicted by personal\nBMI, such that participants with lower BMI underestimated their body size and participants\nwith higher BMI overestimated their body size. Further, participants with higher BMI\nwere less likely to notice the same percentage of weight gain than participants with lower\nBMI. Importantly, these results were only apparent when participants were judging a virtual\nbody that was their own identity (Experiment 1), but not when they estimated the size of a\nbody with another identity and the same underlying body shape (Experiment 2a). The different\ninfluences of BMI on accuracy of body size estimation and sensitivity to weight change\nfor self and other identity suggests that effects of BMI on visual body size estimation are\nself-specific and not generalizable to other bodies.: https://ps.is.tuebingen.mpg.de/person/black/thaler-plos-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/thaler-plos-2018&amp;amp;title=Previous literature suggests that a disturbed ability to accurately identify own body size may\ncontribute to overweight. Here, we investigated the influence of personal body size, indexed\nby body mass index (BMI), on body size estimation in a non-clinical population of females\nvarying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal\ninfluences by manipulating whether participants believed the body stimuli (personalized\navatars with realistic weight variations) represented their own body or that of another\nperson. Our results show that the accuracy of own body size estimation is predicted by personal\nBMI, such that participants with lower BMI underestimated their body size and participants\nwith higher BMI overestimated their body size. Further, participants with higher BMI\nwere less likely to notice the same percentage of weight gain than participants with lower\nBMI. Importantly, these results were only apparent when participants were judging a virtual\nbody that was their own identity (Experiment 1), but not when they estimated the size of a\nbody with another identity and the same underlying body shape (Experiment 2a). The different\ninfluences of BMI on accuracy of body size estimation and sensitivity to weight change\nfor self and other identity suggests that effects of BMI on visual body size estimation are\nself-specific and not generalizable to other bodies. &amp;amp;summary=Body size estimation of self and others in females varying in {BMI}&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Previous literature suggests that a disturbed ability to accurately identify own body size may\ncontribute to overweight. Here, we investigated the influence of personal body size, indexed\nby body mass index (BMI), on body size estimation in a non-clinical population of females\nvarying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal\ninfluences by manipulating whether participants believed the body stimuli (personalized\navatars with realistic weight variations) represented their own body or that of another\nperson. Our results show that the accuracy of own body size estimation is predicted by personal\nBMI, such that participants with lower BMI underestimated their body size and participants\nwith higher BMI overestimated their body size. Further, participants with higher BMI\nwere less likely to notice the same percentage of weight gain than participants with lower\nBMI. Importantly, these results were only apparent when participants were judging a virtual\nbody that was their own identity (Experiment 1), but not when they estimated the size of a\nbody with another identity and the same underlying body shape (Experiment 2a). The different\ninfluences of BMI on accuracy of body size estimation and sensitivity to weight change\nfor self and other identity suggests that effects of BMI on visual body size estimation are\nself-specific and not generalizable to other bodies. %20https://ps.is.tuebingen.mpg.de/person/black/thaler-plos-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Previous literature suggests that a disturbed ability to accurately identify own body size may\ncontribute to overweight. Here, we investigated the influence of personal body size, indexed\nby body mass index (BMI), on body size estimation in a non-clinical population of females\nvarying in BMI. We attempted to disentangle general biases in body size estimates and attitudinal\ninfluences by manipulating whether participants believed the body stimuli (personalized\navatars with realistic weight variations) represented their own body or that of another\nperson. Our results show that the accuracy of own body size estimation is predicted by personal\nBMI, such that participants with lower BMI underestimated their body size and participants\nwith higher BMI overestimated their body size. Further, participants with higher BMI\nwere less likely to notice the same percentage of weight gain than participants with lower\nBMI. Importantly, these results were only apparent when participants were judging a virtual\nbody that was their own identity (Experiment 1), but not when they estimated the size of a\nbody with another identity and the same underlying body shape (Experiment 2a). The different\ninfluences of BMI on accuracy of body size estimation and sensitivity to weight change\nfor self and other identity suggests that effects of BMI on visual body size estimation are\nself-specific and not generalizable to other bodies. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/thaler-plos-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/athaler\">Thaler, A.</a></span>, Geuss, M. N., <span class=\"default-link-ul\"><a href=\"/person/smoelbert\">M\u00f6lbert, S. C.</a></span>, Giel, K. E., <span class=\"default-link-ul\"><a href=\"/person/sstreuber\">Streuber, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mohler\">Mohler, B. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/thaler-plos-2018\">Body size estimation of self and others in females varying in BMI</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>PLoS ONE</em>, 13(2), Febuary 2018 <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192152\"><i class=\"fa fa-file-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://dx.doi.org/http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192152\">DOI</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Anorexia and Body Shape\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/anorexia-and-body-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19927/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/coreg-patent-2012\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Co-Registration -- Simultaneous Alignment and Modeling of Articulated {3D} Shapes\" src=\"/uploads/publication/image/200/thumb_xl_coregpatentfig.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/coreg-patent-2012\">Co-Registration \u2013 Simultaneous Alignment and Modeling of Articulated 3D Shapes</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dhirshberg\">Hirshberg, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/erachlin\">Rachlin, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aweiss\">Weiss, A.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tFebuary 2018, U.S.~Patent 9,898,848 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion200\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion200\" href=\"#abstractContent200\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent200\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tPresent application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S).\n\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.freepatentsonline.com/9898848.html\"><i class=\"fa fa-file-o\"></i>  text</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/200/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/coreg-patent-2012&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Present application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S).\n: https://ps.is.tuebingen.mpg.de/person/black/coreg-patent-2012&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/coreg-patent-2012&amp;amp;title=Present application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S).\n &amp;amp;summary=Co-Registration -- Simultaneous Alignment and Modeling of Articulated {3D} Shapes&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Present application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S).\n %20https://ps.is.tuebingen.mpg.de/person/black/coreg-patent-2012&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Present application refers to a method, a model generation unit and a computer program (product) for generating trained models (M) of moving persons, based on physically measured person scan data (S). The approach is based on a common template (T) for the respective person and on the measured person scan data (S) in different shapes and different poses. Scan data are measured with a 3D laser scanner. A generic personal model is used for co-registering a set of person scan data (S) aligning the template (T) to the set of person scans (S) while simultaneously training the generic personal model to become a trained person model (M) by constraining the generic person model to be scan-specific, person-specific and pose-specific and providing the trained model (M), based on the co registering of the measured object scan data (S).\n &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/coreg-patent-2012&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/black\">Black, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/dhirshberg\">Hirshberg, D.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/mloper\">Loper, M.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/erachlin\">Rachlin, E.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/aweiss\">Weiss, A.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/coreg-patent-2012\">Co-Registration \u2013 Simultaneous Alignment and Modeling of Articulated 3D Shapes</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tFebuary 2018, U.S.~Patent 9,898,848 <small class=\"text-muted\">(misc)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://www.freepatentsonline.com/9898848.html\"><i class=\"fa fa-file-o\"></i>  text</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/200/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/kanazawa-cvpr-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"End-to-end Recovery of Human Shape and Pose\" src=\"/uploads/publication/image/19984/thumb_xl_HMRteaser.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/kanazawa-cvpr-2018\">End-to-end Recovery of Human Shape and Pose</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Jacobs, D. W., Malik, J.\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 7122-7131, IEEE Computer Society, 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion19984\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion19984\" href=\"#abstractContent19984\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent19984\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tWe describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/kanazawaCVPR2018.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/akanazawa/hmr\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://akanazawa.github.io/hmr/\"><i class=\"fa fa-github-square\"></i>  project</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/bmMV9aJKa-c\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"3D Body Shape and Pose from Images\" class=\"btn btn-default btn-xs\" href=\"/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19984/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/kanazawa-cvpr-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.: https://ps.is.tuebingen.mpg.de/person/black/kanazawa-cvpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/kanazawa-cvpr-2018&amp;amp;title=We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation. &amp;amp;summary=End-to-end Recovery of Human Shape and Pose&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation. %20https://ps.is.tuebingen.mpg.de/person/black/kanazawa-cvpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/kanazawa-cvpr-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Jacobs, D. W., Malik, J.  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/kanazawa-cvpr-2018\">End-to-end Recovery of Human Shape and Pose</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>,  pages: 7122-7131, IEEE Computer Society, 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/kanazawaCVPR2018.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/akanazawa/hmr\"><i class=\"fa fa-github-square\"></i>  code</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://akanazawa.github.io/hmr/\"><i class=\"fa fa-github-square\"></i>  project</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/bmMV9aJKa-c\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"3D Body Shape and Pose from Images\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/3d-pose-and-shape-from-images\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19984/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/zuffi-cvpr-2018\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Lions and Tigers and Bears: Capturing Non-Rigid, {3D}, Articulated Shape from Images\" src=\"/uploads/publication/image/19983/thumb_xl_SMALRteaser.jpg\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/zuffi-cvpr-2018\">Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\tIn <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, IEEE Computer Society, 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion19983\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion19983\" href=\"#abstractContent19983\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent19983\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tAnimals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames.\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://smalr.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  code/data</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://sketchfab.com/SMALR\"><i class=\"fa fa-file-o\"></i>  3D models</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Animal Shape\" class=\"btn btn-default btn-xs\" href=\"/research_projects/capturing-animal-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19983/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/zuffi-cvpr-2018&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames.: https://ps.is.tuebingen.mpg.de/person/black/zuffi-cvpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/zuffi-cvpr-2018&amp;amp;title=Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames. &amp;amp;summary=Lions and Tigers and Bears: Capturing Non-Rigid, {3D}, Articulated Shape from Images&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames. %20https://ps.is.tuebingen.mpg.de/person/black/zuffi-cvpr-2018&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=Animals are widespread in nature and the analysis of their shape and motion is important in many fields and industries. Modeling 3D animal shape, however, is difficult because the 3D scanning methods used to capture human shape are not applicable to wild animals or natural settings. Consequently, we propose a method to capture the detailed 3D shape of animals from images alone. The articulated and deformable nature of animals makes this problem extremely challenging, particularly in unconstrained environments with moving and uncalibrated cameras. To make this possible, we use a strong prior model of articulated animal shape that we fit to the image data. We then deform the animal shape in a canonical reference pose such that it matches image evidence when articulated and projected into multiple images. Our method extracts significantly more 3D shape detail than previous methods and is able to model new species, including the shape of an extinct animal, using only a few video frames. Additionally, the projected 3D shapes are accurate enough to facilitate the extraction of a realistic texture map from multiple frames. &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/zuffi-cvpr-2018&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/szuffi\">Zuffi, S.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/akanazawa\">Kanazawa, A.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/zuffi-cvpr-2018\">Lions and Tigers and Bears: Capturing Non-Rigid, 3D, Articulated Shape from Images</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\tIn <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, IEEE Computer Society, 2018 <small class=\"text-muted\">(inproceedings)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://files.is.tue.mpg.de/black/papers/zuffiCVPR2018.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  pdf</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://smalr.is.tue.mpg.de/\"><i class=\"fa fa-file-o\"></i>  code/data</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://sketchfab.com/SMALR\"><i class=\"fa fa-file-o\"></i>  3D models</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Capturing Animal Shape\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/capturing-animal-shape\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19983/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\n\n\n\t\t<!-- DETAIL VIEW -->\n\t\t<div id=\"detailViewPublicationContainer\" class=\"row detailViewPublicationContainer employeeDetailRow\">\n\n\t\t\t\t<div class=\"col-md-12 publication-year-container-list-margin-top\">\n\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2017</span></h4>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t<hr class=\"devider devider-dotted employee-seperator-hr-top\">\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-3\">\n\t\t\t\t\t<!-- IW-123 -->\n\t\t\t\t\t <!-- REMOVED IW-123 -->\n\t\t\t\t\t <!-- ADDED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- REMOVED IW-123 -->\n\t\t\t\t\t\n\t\t\t\t\t<!-- ADDED IW-123 -->\n\n\t\t\t\t\t<a href=\"/publications/flame-2017\"><img class=\"img-responsive img-bordered t-img-bordered\" alt=\"Learning a model of facial shape and expression from {4D} scans\" src=\"/uploads/publication/image/19823/thumb_xl_FLAMEwebteaserwide.png\"></a>\n\n\n\t\t\t\t</div>\n\t\t\t\t<div class=\"col-md-9\">\n\t\t\t\t\t<h5 class=\"publicationGridTitle\">\n\t\t\t\t\t\t<a href=\"/publications/flame-2017\">Learning a model of facial shape and expression from 4D scans</a> <i class=\"fa fa-external-link\"></i>\n\t\t\t\t\t</h5>\n\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/tli\">Li, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Li, H., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>\n\t\t\t\t\t\t\t</p>\n\n\n\t\t\t\t\t\t\t<em>ACM Transactions on Graphics</em>, 36(6):194:1-194:17, November 2017, Two first authors contributed equally <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t</p><div id=\"abstractAccordion19823\">\n\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t<div>\n\t\t\t\t\t\t\t\t\t\t\t<span class=\"text-muted-custom\">Abstract</span> <a data-toggle=\"collapse\" data-parent=\"#abstractAccordion19823\" href=\"#abstractContent19823\"><i id=\"abstractIcon\" class=\"fa fa-arrow-circle-right\"></i></a>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id=\"abstractContent19823\" class=\"panel-collapse collapse \">\n\t\t\t\t\t\t\t\t\t\t\t<div class=\"panel-body\">\n\t\t\t\t\t\t\t\t\t\t\t\tThe field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n\n\t\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<p>\n\t\t\t\t\t</p>\n\t\t\t\t\t<p class=\"publication-button-p\">\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://flame.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  data/model</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/36rPTkhiJTM\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Rubikplayer/flame-fitting\"><i class=\"fa fa-github-square\"></i>  code chumpy</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TimoBolkart/TF_FLAME\"><i class=\"fa fa-github-square\"></i>  code tensorflow</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/400/paper.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/401/supplementary.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supplemental</a>\n\t\t\t\t\t\t</span>\n\n\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19823/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t<button type=\"button\" class=\"btn btn-default btn-xs\" data-html=\"true\" data-container=\"body\" data-toggle=\"popover\" data-placement=\"top\" data-content=\"<div style='width:800px'>\n<div class=&quot;share-contianer&quot;> \n\n  <ul class=&quot;social-icons social-icons-color custom-share-buttons&quot;>\n    <li>\n      <a href=&quot;https://www.facebook.com/sharer/sharer.php?u=https://ps.is.tuebingen.mpg.de/person/black/flame-2017&quot; onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_facebook&quot;></a>\n    \n    </li>\n    <li>\n      <a href=&quot;http://twitter.com/home?status=@MPI_IS_Tue - The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n: https://ps.is.tuebingen.mpg.de/person/black/flame-2017&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_twitter&quot;></a>\n    </li>\n    <li>\n      <a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;amp;url=https://ps.is.tuebingen.mpg.de/person/black/flame-2017&amp;amp;title=The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n &amp;amp;summary=Learning a model of facial shape and expression from {4D} scans&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_linkedin&quot;></a>\n    </li>\n    <li>\n     <a href=&quot;https://plus.google.com/share?url=The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n %20https://ps.is.tuebingen.mpg.de/person/black/flame-2017&quot;  onclick=&quot;popupCenter($(this).attr('href'), '', 580, 470); return false;&quot; class=&quot;popup social_googleplus&quot;></a>\n   </li>\n   <li>\n    <a href=&quot;mailto:?subject=The field of 3D face modeling has a large gap between high-end and low-end methods. At the high end, the best facial animation is indistinguishable from real humans, but this comes at the cost of extensive manual labor. At the low end, face capture from consumer depth sensors relies on 3D face models that are not expressive enough to capture the variability in natural facial shape and expression. We seek a middle ground by learning a facial model from thousands of accurately aligned 3D scans. Our FLAME model (Faces Learned with an Articulated Model and Expressions) is designed to work with existing graphics software and be easy to fit to data. FLAME uses a linear shape space trained from 3800 scans of human heads. FLAME combines this linear shape space with an articulated jaw, neck, and eyeballs, pose-dependent corrective blendshapes, and additional global expression from 4D face sequences in the D3DFACS dataset along with additional 4D sequences.We accurately register a template mesh to the scan sequences and make the D3DFACS registrations available for research purposes. In total the model is trained from over 33, 000 scans. FLAME is low-dimensional but more expressive than the FaceWarehouse model and the Basel Face Model. We compare FLAME to these models by fitting them to static 3D scans and 4D sequences using the same optimization method. FLAME is significantly more accurate and is available for research purposes (http://flame.is.tue.mpg.de).\n &amp;amp;body=https://ps.is.tuebingen.mpg.de/person/black/flame-2017&quot; class=&quot;social_mail&quot;></a>\n  </li>\n</ul>\n\n</div>\n</div>\" data-original-title=\"\" title=\"\">\n\t\t\t\t\t\t\t\t<i class=\"fa fa-share-alt\"></i> Share\n\t\t\t\t\t\t\t</button>\n\t\t\t\t\t\t</span>\n\t\t\t\t\t</p>\n\t\t\t\t</div>\n\t\t\t</div>\n\n\n\t\t\t<!-- LIST VIEW -->\n\t\t\t<div id=\"listViewPublicationContainer\" class=\"listViewPublicationContainer hideSection\">\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"col-md-12 publication-container-list publication-year-container-list-margin-top\">\n\t\t\t\t\t\t<div class=\"publication-year-container\">\n\t\t\t\t\t\t\t<h4 class=\"year-h4-span \"><span class=\"label label-default pull-right\">2017</span></h4>\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-20\"></div>\n\t\t\t\t\t\n\t\t\t\t\t<div class=\"row\">\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t<div class=\"col-md-12\">\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<div class=\"list-publication-container\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t<p class=\"default-link-ul publication-list-ul\">\n\n\t\t\t\t\t\t\t\t\t<span class=\"default-link-ul\"><a href=\"/person/tli\">Li, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/tbolkart\">Bolkart, T.</a></span>, <span class=\"default-link-ul\"><a href=\"/person/black\">Black, M. J.</a></span>, Li, H., <span class=\"default-link-ul\"><a href=\"/person/jromero\">Romero, J.</a></span>  \n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t<strong><a href=\"/publications/flame-2017\">Learning a model of facial shape and expression from 4D scans</a></strong> \n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t<em>ACM Transactions on Graphics</em>, 36(6):194:1-194:17, November 2017, Two first authors contributed equally <small class=\"text-muted\">(article)</small>\n\n\t\t\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p><p class=\"publication-button-p \">\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"http://flame.is.tue.mpg.de\"><i class=\"fa fa-file-o\"></i>  data/model</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://youtu.be/36rPTkhiJTM\"><i class=\"fa fa-file-video-o\"></i>  video</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/Rubikplayer/flame-fitting\"><i class=\"fa fa-github-square\"></i>  code chumpy</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"https://github.com/TimoBolkart/TF_FLAME\"><i class=\"fa fa-github-square\"></i>  code tensorflow</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/400/paper.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  paper</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/uploads_file/attachment/attachment/401/supplementary.pdf\"><i class=\"fa fa-file-pdf-o\"></i>  supplemental</a>\n\t\t\t\t\t\t\t\t\t\t</span>\n\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<!-- IW-708 -->\n\t\t\t\t\t\t\t\t\t\t\t\t<a data-toggle=\"tooltip\" data-title=\"Faces and Expressions\" class=\"btn btn-default btn-xs\" href=\"https://ps.is.tuebingen.mpg.de/research_projects/human-face-analysis\"><i class=\"fa fa-external-link\"></i> Project Page</a>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t</span>\t\t\t\n\t\t\t\t\t\t\t\t\t\t<span>\n\t\t\t\t\t\t\t\t\t\t\t<a class=\"btn btn-default btn-xs\" target=\"_blank\" href=\"/publications/19823/get_bibtexfile?export_type=bibtex\">[BibTex]</a>\n\t\t\t\t\t\t\t\t\t\t</span>\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t\t\t<p></p>\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t</div>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\t\t\t</div>\n\n\t\t\t<div id=\"ajax-pagination-container\" class=\"pagingContainer publicationsAjaxPaging\">\n\t\t\t\t\n\t\t\t\t<ul role=\"navigation\" aria-label=\"Pagination\" class=\"pagination\"><li class=\"previous_page disabled custom-pagination-diasbled custom-pagination-links\"><a href=\"#\">\u2190 Previous</a></li> <li class=\"active custom-pagination-links\"><a href=\"/people/black?page=1\">1</a></li> <li class=\"custom-pagination-links\"><a rel=\"next\" href=\"/people/black?page=2\">2</a></li> <li class=\"custom-pagination-links\"><a href=\"/people/black?page=3\">3</a></li> <li class=\"custom-pagination-links\"><a href=\"/people/black?page=4\">4</a></li> <li class=\"custom-pagination-links\"><a href=\"/people/black?page=5\">5</a></li> <li class=\"custom-pagination-links\"><a href=\"/people/black?page=6\">6</a></li> <li class=\"custom-pagination-links\"><a href=\"/people/black?page=7\">7</a></li> <li class=\"next_page custom-pagination-links\"><a rel=\"next\" href=\"/people/black?page=2\">Next \u2192</a></li></ul>\n\t\t\t</div>\n\n\n\n\n    \n                </div>\n              </div>\n\n            </div>\n\n          </div>\n\n\n        </div>\n      </div>\n    </div>\n  </div>\n\n</div>\n</div>\n\n\n\t\t\t\t</div>\n\n\t\t<!-- project - selected projects footer -->\n\n\t\t<!-- department hover footer image (homepage) -->\n\n\t\t<!-- footer -->\n\t\t<div id=\"footer\" class=\"footer-v1\">\n\t<div class=\"footer custom-footer\">\n\t\t<div class=\"container\">\n\t\t\t<div class=\"row\">\n\n\t\t\t\t\n\t\t\t\t<div class=\"col-md-3 md-margin-bottom-40 hidden-xs\">\n\t\t\t\t\t<img class=\"footer-logo custom-footer-logo\" src=\"/assets/header/header_logo_is-0b379987cb051aa0f1eb971f343327f7.png\" alt=\"Header logo is\">\n\t\t\t\t\t<p>\n\t\t\t\t\t\tOur goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems\n\t\t\t\t\t</p>\n\n\t\t\t\t\t<p></p>\n\t\t\t\t</div>\n\t\t\t\t\n\n\t\t\t\t<div class=\"col-md-3 md-margin-bottom-40 hidden-xs\">\n\t\t\t\t\t<div class=\"posts\">\n\t\t\t\t\t\t<div class=\"headline custom-headline-homepage-is\"><h2>Latest News</h2></div>\n\t\t\t\t\t\t<ul class=\"list-unstyled latest-list\">\n\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t<a href=\"/news/top-40-under-40\">Andreas Geiger selected as Top 40 under 40</a>\n\t\t\t\t\t\t\t\t<small>21 November 2019</small>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t<a href=\"/news/color-patch-could-throw-self-driving-vehicles-off-track\">Color patch could throw self-driving vehicles off track</a>\n\t\t\t\t\t\t\t\t<small>25 October 2019</small>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t<a href=\"/news/ten-papers-accepted-at-iccv-2019\">Ten papers accepted at ICCV 2019</a>\n\t\t\t\t\t\t\t\t<small>26 July 2019</small>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t\t<li>\n\t\t\t\t\t\t\t\t<a href=\"/news/siyu-tang-appointed-tenure-track-assistant-professor-at-eth-zurich\">Siyu Tang appointed Tenure-track Assistant Professor at ETH Zurich</a>\n\t\t\t\t\t\t\t\t<small>26 July 2019</small>\n\t\t\t\t\t\t\t</li>\n\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t</div>\n\n\t\t\t\t<div class=\"col-md-3 md-margin-bottom-40 hidden-xs\">\n\t\t\t\t\t<div class=\"headline custom-headline-homepage-is\"><h2>Links</h2></div>\n\t\t\t\t\t<ul class=\"list-unstyled link-list margin-bottom-20\">\n\t\t\t\t\t\n\t\t\t\t\t<li><a target=\"_blank\" href=\"http://cyber-valley.de\">Cyber Valley</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t<li><a target=\"_blank\" href=\"http://imprs.is.mpg.de\">IMPRS-IS</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t<li><a target=\"_blank\" href=\"http://learning-systems.org\">Center for Learning Systems</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t<li><a target=\"_blank\" href=\"https://www.cis.mpg.de/\">Computer Science at Max Planck</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t<li></li>\n\t\t\t\t</ul>\n\n\t\t\t\t\t<!-- IW-594 -->\n\t\t\t\t\t<ul class=\"list-unstyled link-list margin-bottom-20\">\n\t\t\t\t\t\t<li><a target=\"_blank\" href=\"https://www.facebook.com/PerceivingSystems/\"><i class=\"fa fa-twitter footer-link-icon\"></i> FaceBook</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t\t<li><a target=\"_blank\" href=\"https://twitter.com/PerceivingSys\"><i class=\"fa fa-twitter footer-link-icon\"></i> Twitter</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t\t<li><a target=\"_blank\" href=\"https://www.youtube.com/channel/UCqNJuPO0tyV6eWfYB7lcsvw\"><i class=\"fa fa-youtube-play footer-link-icon\"></i> YouTube</a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t</ul>\n\n\n\t\t\t\t<ul class=\"list-unstyled link-list\">\n\t\t\t\t\t<li><a target=\"_blank\" href=\"https://intranet.is.mpg.de/en\">Intranet <small class=\"text-muted\">(internal)</small></a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t\t<li><a target=\"_blank\" href=\"https://atlas.is.localnet/confluence/\">Confluence Wiki <small class=\"text-muted\">(internal)</small></a><i class=\"fa fa-angle-right\"></i></li>\n\t\t\t\t</ul>\n\t\t\t</div>\n\n\n\n\t\t\t<div class=\"col-md-3 map-img md-margin-bottom-40\"> \n\t\t\t\t<div class=\"headline custom-headline-homepage-is\"><h2>Contact Us</h2></div>                     \n\t\t\t\t<address class=\"md-margin-bottom-40\">\n\n\t\t\t\t\t<div class=\"margin-bottom-10\">\n\t\t\t\t\t\tMax Planck Institute for Intelligent Systems <br>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-10\">\n\t\t\t\t\t\tMax-Planck-Ring 4 <br>\n\t\t\t\t\t\t72076 T\u00fcbingen\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-10\">\n\t\t\t\t\t\tHeisenbergstr. 3<br>\n\t\t\t\t\t\t70569 Stuttgart\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-10\">\n\t\t\t\t\t\tGermany<br>\n\t\t\t\t\t</div>\n\t\t\t\t\t<div class=\"margin-bottom-10\">\n\t\t\t\t\t\t <a class=\"footer-link\" href=\"mailto:info@is.mpg.de\">info@is.mpg.de</a><script id=\"mail_to-aomd4jku\">eval(decodeURIComponent('%76%61%72%20%73%63%72%69%70%74%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%67%65%74%45%6c%65%6d%65%6e%74%42%79%49%64%28%27%6d%61%69%6c%5f%74%6f%2d%61%6f%6d%64%34%6a%6b%75%27%29%3b%76%61%72%20%61%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%45%6c%65%6d%65%6e%74%28%27%61%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%63%6c%61%73%73%27%2c%20%27%66%6f%6f%74%65%72%2d%6c%69%6e%6b%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%68%72%65%66%27%2c%20%27%6d%61%69%6c%74%6f%3a%69%6e%66%6f%40%69%73%2e%6d%70%67%2e%64%65%27%29%3b%61%2e%61%70%70%65%6e%64%43%68%69%6c%64%28%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%54%65%78%74%4e%6f%64%65%28%27%69%6e%66%6f%40%69%73%2e%6d%70%67%2e%64%65%27%29%29%3b%73%63%72%69%70%74%2e%70%61%72%65%6e%74%4e%6f%64%65%2e%69%6e%73%65%72%74%42%65%66%6f%72%65%28%61%2c%73%63%72%69%70%74%29%3b'))</script>\n\t\t\t\t\t</div>\n\t\t\t\t</address>\n\t\t\t\t<div class=\"fiiter-border\">\n\t\t\t\t\t<p class=\"margin-top-10\">For website questions and technical problems please contact:</p>\n\t\t\t\t\t<a class=\"footer-link\" href=\"mailto:web@is.mpg.de\">web@is.mpg.de</a><script id=\"mail_to-f92sle9v\">eval(decodeURIComponent('%76%61%72%20%73%63%72%69%70%74%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%67%65%74%45%6c%65%6d%65%6e%74%42%79%49%64%28%27%6d%61%69%6c%5f%74%6f%2d%66%39%32%73%6c%65%39%76%27%29%3b%76%61%72%20%61%20%3d%20%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%45%6c%65%6d%65%6e%74%28%27%61%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%63%6c%61%73%73%27%2c%20%27%66%6f%6f%74%65%72%2d%6c%69%6e%6b%27%29%3b%61%2e%73%65%74%41%74%74%72%69%62%75%74%65%28%27%68%72%65%66%27%2c%20%27%6d%61%69%6c%74%6f%3a%77%65%62%40%69%73%2e%6d%70%67%2e%64%65%27%29%3b%61%2e%61%70%70%65%6e%64%43%68%69%6c%64%28%64%6f%63%75%6d%65%6e%74%2e%63%72%65%61%74%65%54%65%78%74%4e%6f%64%65%28%27%77%65%62%40%69%73%2e%6d%70%67%2e%64%65%27%29%29%3b%73%63%72%69%70%74%2e%70%61%72%65%6e%74%4e%6f%64%65%2e%69%6e%73%65%72%74%42%65%66%6f%72%65%28%61%2c%73%63%72%69%70%74%29%3b'))</script>\n\t\t\t\t</div>\n\t\t\t</div>\n\t\t</div>\n\t</div> \n</div>\n<div class=\"copyright custom-copyright\">\n\t<div class=\"container\">\n\t\t<div class=\"row\">\n\t\t\t<div class=\"col-md-6\">      \n\t\t\t\t<p> \n\t\t\t\t\t\u00a9 2020 Max-Planck-Gesellschaft - \n\t\t\t\t\t<span class=\"custom-copyright-container\">  \n\t\t\t\t\t\t<a href=\"/imprint\">Imprint</a> | \n\t\t\t\t\t\t<a href=\"/privacy-policy\">Privacy Policy</a>\n\t\t\t\t\t</span>\n\t\t\t\t</p>\n\t\t\t</div>\n\t\t\t<div class=\"col-md-6\">  \n\n\t\t\t\t<div class=\"sub-footer-btn-container\">\n\t\t\t\t\t<a class=\"btn btn-footer btn-sm\" href=\"/sign_in\">Sign In</a>\n\t\t\t\t</div>\n\t\t\t\n\t\t\t</div>\n\t\t</div>\n\t</div> \n</div>\n</div>\t\n\t\t\n\t\n\n\t\t<!-- Piwik -->\n\t\t<script type=\"text/javascript\">\n\t\tvar _paq = _paq || [];\n\t\t_paq.push([\"setDocumentTitle\", document.domain + \"/\" + document.title]);\n\t\t_paq.push([\"setCookieDomain\", \"*.is.tuebingen.mpg.de\"]);\n\t\t_paq.push(['trackPageView']);\n\t\t_paq.push(['enableLinkTracking']);\n\t\t(function() {\n\t\t\tvar u=((\"https:\" == document.location.protocol) ? \"https\" : \"http\") + \"://piwik.tuebingen.mpg.de/\";\n\t\t\t_paq.push(['setTrackerUrl', u+'piwik.php']);\n\t\t\t_paq.push(['setSiteId', 14]);\n\t\t\tvar d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';\n\t\t\tg.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);\n\t\t})();\n\t\t</script>\n\t\t<noscript><p><img src=\"http://piwik.tuebingen.mpg.de/piwik.php?idsite=14\" style=\"border:0;\" alt=\"\" /></p></noscript>\n\t\t<!-- End Piwik Code -->\n\n\t<div class=\"eupopup-container eupopup-container-bottom eupopup-color-default\" style=\"display: block;\">\n\t\t<div class=\"eupopup-body\">\n\t\t\t\tWe use cookies to improve your website experience. <a href=\"https://is.mpg.de/privacy-policy\">Find out more about our cookies and how to disable them</a>. By continuing, you consent to our use of cookies. <a href=\"#\" id=\"eupopup-continuelink\">Continue</a>\n\t\t\t\n\t\t</div>\n\t\t<a href=\"#\" class=\"eupopup-closebutton hidden-xs hidden-sm hidden-md\"><i class=\"fa fa-times-circle\"></i></a>\n\t</div>\n\n\t\n\n\n\n\n<div style=\"position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px; margin: 0px;\"><div id=\"MathJax_Font_Test\" style=\"position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-family: STIXSizeOneSym, sans-serif;\"></div></div></body></html>"