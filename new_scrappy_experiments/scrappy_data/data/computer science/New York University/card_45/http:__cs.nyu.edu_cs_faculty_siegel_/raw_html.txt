"<html lang=\"en\"><head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\">\n<meta name=\"GENERATOR\" content=\"Microsoft FrontPage 4.0\">\n<meta name=\"ProgId\" content=\"FrontPage.Editor.Document\">\n\n<title>New Page 1</title>\n</head>\n\n<body>\n\n<h1 align=\"center\">Alan Siegel</h1>\n<h2 align=\"center\">This page is under construction.&nbsp;</h2>\nYou are welcome to look through some selected work in computer\nscience, mathematics and education. The current topics include\ntechnical articles plus two closely related\npublications about mathematics education\nand the TIMSS Videotape of Classroom Study of teaching in Japan, Germany,\nand the U.S.\nThe technical publications include work in the areas of plane geometry,\nprobability (medians and Heoffding bounds), the mathematical analysis of closed\nhashing, and the theory of fast hash functions.\n<p align=\"center\"><font size=\"5\"><b>Education</b></font>\n</p><blockquote>\n  <blockquote>\n    <ol type=\"A\">\n      <li><a href=\"http://www.cs.nyu.edu/faculty/siegel/siegel.pdf\">Understanding and misunderstanding the Third International\nMathematics and Science Study: what is at stake\nand why K-12 education studies matter,</a>\nProceedings of the\nInternational Congress of Mathematicians (ICM2006),\nVolume III: Invited Lectures, M. Sanz-Sole,\nJ. Soria, L.L. Varona, and J. Verdera, Ed., pp. 1599--1630 (2006).\n      </li><li><a href=\"http://www.cs.nyu.edu/faculty/siegel/ST11.pdf\">Telling\n\tLessons</a><a href=\"http://www.cs.nyu.edu/faculty/siegel/ST24F.doc\"> from </a><a href=\"http://www.cs.nyu.edu/faculty/siegel/ST11.pdf\">the\n        TIMSS Videotape. (The paper is here.)</a>\nThis analysis of the TIMSS\n        video tape and the ensuing studies is written to let you decide\n\tfor yourself what made the teaching so special.&nbsp;\n\tBut don't take my word for it; read the paper and do your own thinking.&nbsp;\n        If you still have doubts, go review the tapes and check out the references.&nbsp;\n        After all, that's what I did. This paper appeared as Chapter 5 in\nTesting Student Learning, Evaluating Teaching Effectiveness,\nWilliamson Evers and Herbert Walberg, Ed.\npp. 161--193 (2004).\n</li>\n    </ol>\n    <p>&nbsp;</p>\n    <p>Please note: there are a few links to restore below, and I have to make\n    an alternative page for those of you who do not have an ISO-8859-1 compliant\n    character set loaded into your browsers.&nbsp; (I also used a few overlines,\n    which might not be in the standard.)&nbsp;</p>\n    <p>Here is what you should see.&nbsp; Well, the top row should be Greek\n    characters and a few math characters.</p>\n    <p align=\"center\">\n    <table border=\"1\">\n      <tbody><tr>\n        <td align=\"center\">\u03bc</td>\n        <td align=\"center\">\n    \u03c4&nbsp;</td>\n        <td align=\"center\"> \u03bb&nbsp;</td>\n        <td align=\"center\"> \n    \u03ba</td>\n        <td align=\"center\">\n    \u03c0&nbsp;</td>\n        <td align=\"center\"><font face=\"Symbol\">p</font></td>\n        <td align=\"center\">\n    <font face=\"Symbol\">S</font></td>\n        <td align=\"center\">\u03a3</td>\n        <td align=\"center\">\u03a3</td>\n        <td align=\"center\"><i>\u221a</i><span style=\"text-decoration: overline\">13</span>  &nbsp;</td>\n        <td align=\"center\"><font size=\"4\">\u222b</font>&nbsp;</td>\n      </tr>\n      <tr>\n        <td align=\"center\">&nbsp;mu&nbsp;</td>\n        <td align=\"center\">&nbsp;tau&nbsp;</td>\n        <td align=\"center\">&nbsp;lambda&nbsp;</td>\n        <td align=\"center\">&nbsp;kappa&nbsp;</td>\n        <td align=\"center\">&nbsp;pi&nbsp;</td>\n        <td align=\"center\">&nbsp;pi&nbsp;</td>\n        <td align=\"center\">&nbsp;Sigma&nbsp;</td>\n        <td align=\"center\">&nbsp;Sigma&nbsp;</td>\n \t\t <td align=\"center\">&nbsp;Sigma&nbsp;</td>\n        <td align=\"center\">&nbsp;sqrt(13)&nbsp;</td>\n        <td align=\"center\">&nbsp;integral&nbsp;</td>\n      </tr>\n    </tbody></table>\n    </p><p>&nbsp;</p>\n    <p>I'll eventually post an HTML version of the TIMSS paper above.\n  But for right now, you are on your own.&nbsp;&nbsp;&nbsp; My apologies.\n    A.S.</p>\n    <p>&nbsp;</p>\n  </blockquote>\n</blockquote>\n<p align=\"center\" style=\"MARGIN-BOTTOM: 0.05in\"><font size=\"5\"><b>\n<a name=\"BArea\"></a>Area\nin the plane</b></font>\n</p><blockquote>\n  <p style=\"MARGIN-BOTTOM: 0.05in\"><font size=\"3\">We resolve a number of area\n  optimization questions about polygons and related figures.&nbsp; Some of the\n  problems have a history that goes back 10, 30 and even 50 years.&nbsp; The\n  first proof is not new.  The paper is intended to give an entertaining\nand intuitive proof of the isoperimetric inequality in the plane.\nThe idea originates with Lawler.\n  <font>\n  </font></font></p><p style=\"MARGIN-BOTTOM: 0.05in\"><font size=\"3\"><font><font size=\"4\"><b>Simple polygons</b> </font>(i.e.,\n  polygons that do not intersect themselves).</font></font></p><font size=\"3\"><font>\n  <blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"MARGIN-BOTTOM: 0.05in\">\n\t<a href=\"#Area\">A\n        naive proof of the 2-D Isoperimetric Theorem in Elementary Geometry.</a></p>\n      </li><li>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Area\">An\n        isoperimetric theorem in plane geometry.</a></p>\n      </li>\n    </ol>\n  </blockquote>\n  <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><b><font size=\"4\">Self-intersecting\n  polygons.</font></b>\n  </p><blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"margin><-bottom: .05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Area2\">An\n        isoperimetric inequality for self-intersecting polygons.</a></p>\n      </li>\n    </ol>\n  </blockquote>\n  <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><b><font size=\"+1\">Arrangements\n  of segments and the area they encompass.</font></b></p>\n  <blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"MARGIN-BOTTOM: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Area3\">A\n        Dido Problem as modernized by Fejes T\ufffdth</a>.</p>\n      </li><li>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Area3\">Some\n        Dido-type Inequalities.</a></p>\n      </li>\n    </ol>\n  </blockquote>\n  <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><b><font size=\"+1\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Open\">Open\n  problems.</a>&nbsp;&nbsp;</font></b></p>\n</font></font></blockquote><font size=\"3\"><font>\n<p align=\"center\" style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><b><font size=\"+2\">\n<a name=\"Bhashing\"></a>Performance\nanalyses for closed hashing<br>\n&nbsp;in a model that supports real computation</font></b></p>\n<blockquote>\n  <p align=\"left\" style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><font size=\"3\">For\n  closed hashing,&nbsp; performance estimates are normally based on simplified\n  analyses of idealized programs that cannot be implemented.&nbsp; We show that\n  the same results can be achieved for programmable hash functions that run in\n  constant time---provided certain caching resources are available.</font></p>\n  <p align=\"left\" style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\"><b><font size=\"+1\">The\n  performance of closed hashing with limited randomness.</font></b></p>\n  <blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"MARGIN-BOTTOM: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Hashing\">Double\n        Hashing is Computable and Randomizable with Universal Hash Functions,</a>\n        with J.P. Schmidt.</p>\n      </li><li>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Hashing\">Closed\n        Hashing is Computable and Optimally Randomizable with Universal Hash\n        Functions,</a> with J.P. Schmidt.</p>\n      </li>\n    </ol>\n  </blockquote>\n  <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\"><b><font size=\"+1\">The\n  theory of fast hash functions.</font></b>\n  </p><blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Fasthash\">On\n        Universal Classes of Extremely Random Constant Time Hash Functions and\n        their Time-Space Tradeoff.</a></p>\n      </li>\n    </ol>\n  </blockquote>\n</blockquote>\n<p align=\"center\" style=\"margin-top: .2in; margin-bottom: .07in\"><b><font size=\"+2\">\n<a name=\"BMedians\"></a>Probability</font></b>\n</p><ol>\n  <p style=\"margin-top: 0.00in; margin-bottom: 0.05in\"><b><font size=\"4\">Medians\n  of discrete random variables</font></b></p>\n</ol>\n<blockquote>\n  <blockquote>\n    <ol type=\"A\">\n      <li>\n        <p style=\"MARGIN-BOTTOM: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Median\">Median\n        bounds and their application.</a></p>\n      </li>\n\n    <p style=\"MARGIN-BOTTOM: 0in; MARGIN-LEFT: 0.15in; MARGIN-TOP: 0.1in\">While\n    tail estimates have received significant attention in the probability,\n    statistics, discrete mathematics, and computer science literature, the same\n    cannot be said for medians of probability distributions, and for good\n    reason.\n    </p><ul>\n      <li>First, there are only a few published results in this area,\n        and even they are not at all well known (but should be).\n      </li><li>Second, there seems to be very little\n        mathematical machinery for determining the medians of probability\n        distributions.\n      </li><li>Third (and consequently), median estimates have not been commonly used\n        in the analysis of algorithms, apart from the kinds of analyses\n        frequently used for Quicksort, and the provably good median\n        approximation schemes typified by efficient selection algorithms.</li>\n    </ul>\n    <div style=\"MARGIN-BOTTOM: -0.0in; MARGIN-LEFT: 0.3in; MARGIN-TOP: -0.0in; TEXT-INDENT: -0.15in\">\n      This paper addresses these issues in the following ways.\n    </div>\n    <ul>\n      <li>First, a beginning framework is presented for establishing median\n\testimates.<br>\n        It is strong enough to prove, as simple corollaries, the two or three\n        non-trivial median bounds (not so readily identified) in the\n        literature.&nbsp;\n      </li><li>Second, several new median results are presented, which are all, apart\n        from one, derived via this framework.\n      </li><li>Third, applications are offered: median estimates are shown to\n        simplify the analysis of a variety of probabilistic algorithms and\n        processes.</li>\n    </ul>\n    </ol>\n  </blockquote>\n  <font size=\"4\"><b>\n    Chernoff-Hoefding bounds </b></font><font size=\"3\"> (new material plus handbook-like coverage)</font>\n  <blockquote>\n    <ol type=\"A\">\n      <li>\n        <p><a href=\"http://cs.nyu.edu/faculty/siegel/HHf.pdf\">Toward a\n        Usable Theory of Chernoff Bounds for Heterogeneous and Partially\n        Dependent Random Variables. (The paper is here.)</a>\n        </p><p style=\"MARGIN-BOTTOM: 0.1in; MARGIN-TOP: 0.05in\">Let X be a sum of\n        real valued random variables and have a bounded mean <nobr>E[X].</nobr> The generic\n        Chernoff-Hoeffding estimate for large deviations of X is: \n\t<nobr>Prob{X-E[X]<u>&gt;</u>a} \n        <u>&lt;</u> min<sub>{</sub><font face=\"Symbol\"><sub>l</sub></font><sub><u>&gt;</u>0}</sub>e<sup>(-<font face=\"Symbol\">l</font>(a+E[X]))</sup>E[e<sup><font face=\"Symbol\">l\n        </font>X</sup>],</nobr> which applies with a <u>&gt;</u> 0 to random variables\n        with very small tails. At issue is how to use this method to attain\n        sharp and useful estimates. We present a number of Chernoff-Hoeffding\n        bounds for sums of random variables that can have a variety of dependent\n        relationships and that might be heterogeneously distributed. </p>\n      </li><li>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/Limited.ps\">Chernoff-Hoeffding\n        Bounds for Applications with Limited Independence (The paper is here.)</a>  with J.P. Schmidt\n        and A. Srinivassan.</p>\n      </li>\n    </ol>\n    <blockquote>\n      <p style=\"MARGIN-TOP: -0.1in\">It is all in the title: your favorite tail\n      estimates for sums of random variables that only exhibit limited\n      independence, such as those produced by implementations of hash functions\n      for computers.</p>\n      <p style=\"MARGIN-TOP: -0.1in\">&nbsp;</p>\n      <p style=\"MARGIN-TOP: -0.1in\" align=\"center\"><font size=\"5\">Detailed\n      summaries</font></p>\n    </blockquote>\n  </blockquote>\n</blockquote>\n<p style=\"margin-bottom: .1in\"></p><center><b><font size=\"+2\">\n<a name=\"Area\"></a>Area in the plane</font></b></center>\n<blockquote>\n  <font size=\"+0\">We resolve some area optimization questions about\n  polygons and related figures.&nbsp; The first\npaper presents a known proof of a classical isoperimetric inequality for\nthe circle.  The inequality states\n  that among all regions in the plane with a given perimeter,\n  the disk has the greatest area. There is a related result for polygons,\nwhich states that among all polygonal regions with a given set of side lengths,\nthose whose vertices lie on a circle have the greatest area.\n  As the ancient Greek geometers knew very well, this fact is readily derived\nfrom the isoperimetric inequality for the circle. \n  But as has been noted by the Russian geometer Yaglom, a direct Euclidean\nproof of this basic fact about polygons has been missing. \nThe second paper below presents a Euclidean construction that establishes this\ninequality directly.\nLet P and Q be n-sided polygonal regions with the same side lengths, and\nsuppose that the vertices\nof Q lie on a circle. The algorithm partitions Q into 2n pieces that are then\nrearranged to cover all of P (with possible overlaps and other excesses).\nIt follows that the area of Q is at least as large as that of P.</font>\n  <p style=\"MARGIN-BOTTOM: 0.05in\"><font size=\"+1\"><b>Simple polygons</b> </font>(i.e.,\n  polygons that do not intersect themselves).\n  </p><ol type=\"A\">\n    <li>\n      <div style=\"MARGIN-BOTTOM: 0.05in\">\n        <a href=\"http://www.cs.nyu.edu/faculty/siegel/SCIAM.pdf\">A naive proof\n        of the 2-D Isoperimetric Theorem in Elementary Geometry. (The paper is here.)</a>\n      </div>\n      <p style=\"MARGIN-TOP: 0.05in\">We offer a completely elementary proof of\n      the following basic fact:\n      </p><ol>\n        <div style=\"MARGIN-BOTTOM: 0.1in; MARGIN-LEFT: 0.2in; MARGIN-TOP: 0.1in; TEXT-INDENT: -0.2in\">\n          Among all regions in the plane with a given perimeter p, the circle\n          has the greatest area.\n        </div>\n      </ol>\n      Direct and to the point: no compactness, no calculus, no errors.<br>\n      A brief history of the problem is included, along with a discussion about\n      Steiner and why his approach requires a compactness argument.\n    </li><li>\n      <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/GXX.pdf\">An\n      isoperimetric theorem in plane geometry. (The paper is here.)</a><font size=\"3\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/polyapptest/poly.htm\">Cool\n      applet demo</a><font size=\"3\">\n(Back to \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a>\n      <a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font>\n      </font></p><p style=\"MARGIN-TOP: 0.05in\"><font size=\"3\">We give a new and substantial generalization\n      of A .&nbsp; The naive global analysis of A is extended to derive the following\n      covering property:&nbsp; Imagine\n      walking along the consecutive edges of a simple but not necessarily convex\n      polygon P and drawing a ray from each vertex. Suppose you circulate about\n      P in a clockwise direction, and pick a new direction for each ray that\n      represents a clockwise rotation of the preceding ray. Suppose the sum of\n      all rotational increments adds up to 360<sup>o</sup>.<br>\n      Whenever the rays from two consecutive vertices intersect, let them induce\n      the triangular region defined by the two vertices and the point of\n      intersection.\n      </font></p><ol><font size=\"3\">\n        <div style=\"MARGIN-BOTTOM: 0.1in; MARGIN-LEFT: 0.2in; MARGIN-TOP: 0.1in; TEXT-INDENT: -0.2in\">\n          We show there is a fixed <font face=\"Symbol\">a</font> such that if\n          each ray is rotated by <font face=\"Symbol\">a, </font>then the\n          triangular regions induced by the redirected rays cover the interior\n          of P.\n        </div>\n      </font></ol><font size=\"3\">\n      This covering implies the standard isoperimetric inequalities in two\n      dimensions, as well several new inequalities.<br>\n      The proof is technically elementary, since it does not even use\n      calculus.&nbsp; Unfortunately, a correct proof is non-trivial, and the\n      applications use a little more math.</font></li><font size=\"3\">\n  </font></ol><font size=\"3\">\n  <div style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\">\n    <b><font size=\"+1\">Self-intersecting polygons.</font></b>\n  </div>\n  <ol type=\"A\">\n    <li>\n      <div style=\"MARGIN-BOTTOM: 0.05in\">\n        <a href=\"http://www.cs.nyu.edu/faculty/siegel/Weights4.pdf\" name=\"Area2\">An\n        isoperimetric inequality for self-intersecting polygons. (The paper is here.)</a><font size=\"3\">\n        (Back to <a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a>\n        <a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font>\n      </div>\n      <p style=\"MARGIN-TOP: 0.05in\">The following has been the subject of\n      incremental improvements over a course of 50 years.<br>\n      Take a self-intersecting polygon P, and consider the bounded components of\n      the plane when the polygonal path P is removed (i.e., R<sup><font size=\"-1\">2</font></sup>\n      / P). For each such component, we have, for example:\n      </p><ol type=\"i\">\n        <li>Its area, and the area of its convex hull.\n        </li><li>Various notions of how may times the curve P winds around the\n          component.</li>\n      </ol>\n      The problem is to establish as strong a bound as possible where the\n      smaller side comprises an expression built from the values described in i.\n      and ii. above, and the upper bound is the area of one of the following:\n      <ol type=\"i\">\n        <li>The circle with the same perimeter as P.\n        </li><li>A polygon that is inscribed in a circle and has sides with the same\n          lengths as P.\n        </li><li>A polygon that is simple, convex, and is built from a rearrangement\n          of the edges in P, where only translations (and no rotations) are\n          used.</li>\n      </ol>\n      Let P be a non-simple polygon in R<sup><font size=\"-1\">2</font></sup> with\n      segments that are directed by a traversal along its vertices. Let E be the\n      collection of located directed segments of P, but with some segments split\n      into several pieces as necessary.<br>\n      Let Z be a multiset of oriented simple polygons (cycles) whose edges\n      comprise the same collection of directed located segments as those in E.<br>\n      For any <b>x</b> in ConvHull(P), let w(<b>x</b>) be the maximum of 1 and\n      the number of cycles in Z that contain<b> x </b>in their convex hulls.<br>\n      For any oriented simple cycle C in Z, let W<sub><font face=\"Tahoma\">+</font></sub>(C)\n      be the number of cycles in Z that have the same orientation as C and\n      contain C in their convex hulls. Let W<b>_</b>(C) be the number of cycles\n      in Z that are oriented opposite from C and contain C in their convex\n      hulls.<br>\n      &nbsp;&nbsp;&nbsp; Let <font face=\"Symbol\">k = 2</font><i>\u221a<span style=\"text-decoration: overline\">2</span></i>\n      <font face=\"Symbol\">-</font>\n      2, and let W(C)= W<sub><font face=\"Tahoma\">+</font></sub>(C) + <font face=\"Symbol\">k</font>W<b>_</b>(C).<br>\n      &nbsp;&nbsp;&nbsp; Let <i>a</i> be the area as described in iii., which is\n      the smallest of the three choices.<br>\n      We show that:\n      <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: -0.0in\"></p><center><font size=\"4\">&nbsp;\u222b</font><b><sub><font size=\"-1\"><i>x</i>\n      </font></sub></b><sub><font size=\"-1\"><font face=\"Symbol\">e </font>ConvHull(P)</font></sub>w<sup><font size=\"-1\">2</font></sup>(<b><i>x</i>)</b><i>d<b>x</b></i>+\n      <font size=\"+2\" face=\"Symbol\">S</font><sub><font size=\"-1\">C <font face=\"Symbol\">e</font>\n      Z</font></sub>W(C)<font size=\"+1\">(</font>Area(ConvHull(C))<font face=\"Batang\">-</font>Area(C)<font size=\"+1\">)\n      </font><font size=\"-1\"><u>&lt;</u> </font><i>a</i>.</center>\n      <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in\">Previous bounds used the\n      multiplier w(<b>x</b>) rather than w<sup><font size=\"-1\">2</font></sup>(<b><i>x</i></b>)\n      and set W=0, or set W=0 and replaced <i>a</i> with p<sup><font size=\"-1\">2</font></sup>/\n      4<font face=\"Symbol\">p</font>, where p= Arclength(P). The former bound is\n      not strong, and the latter is an elementary consequence of the\n      Brunn-Minkowski inequality.<br>\n      With the exception of <font face=\"Symbol\">k</font>, no subexpression can\n      be increased by a constant factor, and <font face=\"Symbol\">k </font>cannot\n      exceed <nobr>(<i>\u221a</i><span style=\"text-decoration: overline\">13</span> <font face=\"Batang\">-</font> 1)<font face=\"Symbol\">/</font>3&nbsp;\n      <u><i><font size=\"3\">~</font></i></u> .869.</nobr> We also offer an\n      enhanced bound where <nobr>p<sup><font size=\"-1\">2</font></sup>/ 4<font face=\"Symbol\">p</font></nobr>\n      replaces <i>a</i>.</p>\n    </li>\n  </ol>\n  <div style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\">\n    <b><font size=\"+1\">Arrangements of segments and the area they encompass.</font></b>\n  </div>\n  <ol type=\"A\">\n    <li>\n      <div style=\"MARGIN-BOTTOM: 0.05in\">\n        <a href=\"http://www.cs.nyu.edu/faculty/siegel/D33.pdf\" name=\"Area3\">A\n        Dido Problem as modernized by Fejes T\ufffdth. (The paper is here.)</a>\n      </div>\n      <p style=\"MARGIN-TOP: 0.05in\">Around 30 years ago, Fejes T\ufffdth posed the\n      following problem.<br>\n      Let E be an arbitrary arrangement of line segments in the plane. Let A be\n      the sum of the areas of the bounded components of the plane when the edges\n      in E are removed (i.e., of R<sup><font size=\"-1\">2</font></sup> / E). View\n      the segments as having fixed lengths but as relocatable.\n      </p><ol>\n        <div style=\"MARGIN-BOTTOM: 0.1in; MARGIN-LEFT: 0.2in; MARGIN-TOP: 0.1in; TEXT-INDENT: -0.2in\">\n          Prove that A is maximized precisely when E forms a polygon with the\n          greatest possible area.\n        </div>\n      </ol>\n      The theorem is obviously true; the point of the problem is to devise a\n      sound method to handle the lack of structure imposed by arbitrary\n      arrangements of segments.\n    </li><li>\n      <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">\n<a href=\"http://www.cs.nyu.edu/faculty/siegel/El143.pdf\" name=\"Open\">Some\n      Dido-type Inequalities. (The paper is here.)</a> <font size=\"3\">(Back to \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a>\n      <a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font>\n      </p><p style=\"MARGIN-TOP: 0.05in\">In 1989, A. and K. Bezdek posed the\n      following problem:<br>\n      Let E be an arbitrary arrangement of line segments in the plane. Suppose\n      that E as a pointset is connected. View the segments as having fixed\n      lengths but as relocatable.\n      </p><ol>\n        <div style=\"MARGIN-BOTTOM: 0.1in; MARGIN-LEFT: 0.2in; MARGIN-TOP: 0.1in; TEXT-INDENT: -0.2in\">\n          Show that the area of the convex hull of E is maximized when E forms a\n          polygonal path that is tightly inscribed in a semicircle whose\n          endpoints align with the endpoints of the path.\n        </div>\n      </ol>\n      We establish the bound, and generalize it as follows.<br>\n      The requirement that the edge set be pointwise connected is replaced by a\n      much weaker condition. The polygonal path used to get an upper bound for\n      the area is replaced by a different path to get a smaller upper bound. The\n      path is built from a rearrangement of the edges in P, where only\n      translations (and no rotations) are used. It must be convex, and have a\n      total rotational increase of consecutive edge directions that adds up to\n      at most 180<sup>o</sup>.</li>\n  </ol>\n  <div style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.1in\">\n    <b><font size=\"+1\">Open problems.&nbsp; </font></b>These results raise more\n    questions than answers.\n  </div>\n  <ol type=\"i\">\n    <li>One obvious set of questions concerns generalizations to higher\n      dimensions.<br>\n    </li><li>Another:&nbsp; What is the right value for <font face=\"Symbol\">k? </font>We\n      suspect that it is (<i>\u221a</i><span style=\"text-decoration: overline\">13</span> <font face=\"Batang\">-</font> 1)<font face=\"Symbol\">/</font>3.<br>\n    </li><li>Others are based on the following observation.\n      <ol>\n        The convex hull seems to be ill-suited for defining adjusted notions of\n        area. The objective is to include the area of pockets but not too much\n        more. In the Bezdeks' problem, the dilemma of catching too much area was\n        fixed by the excessive restriction that the collection of segments be\n        pointwise connected.&nbsp; With the Umbra operation as defined below,\n        the area that should be caught by the convex hull still is, but the\n        connectivity requirement can be completely dropped to get the ``the\n        right'' generalization. The same issue arises in the Dido problem of\n        Fejes T\ufffdth. Using the convex hull just creates needless obstacles. The\n        problem reappears when formulating sharper bounds for non-simple\n        polygons. So:\n        <ol>\n          Let F be a finite collection of segments located in the plane.\n          <ol>\n            <div style=\"MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              <div style=\"MARGIN-TOP: 0.05in\">\n                Define <b>Umbra(F) </b>to be the set of points x such that every\n                line through x intersects some segment in F.&nbsp; This should\n                be good for the Bezdeks.\n              </div>\n            </div>\n          </ol>\n          <ol>\n            <div style=\"MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              <div style=\"MARGIN-TOP: 0.05in\">\n                Define <b>Encloak(F)</b> to be the set of points x such that\n                every line through x intersects at least two segments in F. This\n                should be good for Fejes T\ufffdth.\n              </div>\n            </div>\n          </ol>\n          <ol>\n            <div style=\"MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              <div style=\"MARGIN-TOP: 0.05in\">\n                <b>Why stop?</b> Keep counting. This should be good for\n                self-intersecting polygons, and might even give a way to unite\n                the result with the Fejes T\ufffdth problem.\n              </div>\n            </div>\n          </ol>\n        </ol>\n      </ol>\n    </li>\n  </ol>\n  <p align=\"center\"><b><font size=\"+2\"><a name=\"Hashing\"></a>Performance\n  analyses for closed hashing in a model that supports real computation</font></b>\n  </p><p align=\"left\" style=\"MARGIN-BOTTOM: 0.1in\"><font color=\"#000000\">In closed\n  hashing, which is also called hashing with open addressing, data keys are\n  inserted into a table T one-by-one.&nbsp;</font> The hash function <nobr>p(x,j)</nobr>\n  is used to compute the table address of key x as follows:&nbsp; x is inserted\n  into the first vacant table location, in the sequence <nobr>T[p(x,1)],</nobr> <nobr>T[\n  p(x, 2)],</nobr> <nobr>T[ p(x, 3)],</nobr> <nobr>. . . .</nobr> This form of\n  hashing does not use pointers or auxiliary storage. Search for a key x is\n  achieved by testing the table locations in the same sequence until x is\n  located or a vacant location is found, whence it will follow that x is not in\n  T.&nbsp; For specificity, let the keys belong to a finite universe U, and let\n  T have the indices 0..<i>n</i>-1. The performance, as a function of the load\n  factor <font face=\"Symbol\">a</font>, is defined to be the expected number of\n  locations that must be examined to insert the next key when the table contains\n  <font face=\"Symbol\">a</font><i>n</i> keys.\n  </p><div style=\"MARGIN-BOTTOM: 0in; MARGIN-LEFT: 0.3in; MARGIN-TOP: 0.05in; TEXT-INDENT: -0.15in\">\n    <b>Uniform hashing</b> is an idealized model of hashing that assumes the\n    collection of random variables <nobr>p(x, 1), p(x, 2), . . .</nobr> for all\n    x in U is jointly independent and uniformly distributed over <nobr>[0, n-1].</nobr>\n  </div>\n  <div style=\"MARGIN-BOTTOM: 0in; MARGIN-LEFT: 0.3in; MARGIN-TOP: 0.05in; TEXT-INDENT: -0.15in\">\n    <b>Linear probing</b> is more realistic. It defines <nobr>p(x, j)</nobr> =\n<nobr>d(x)-j+1 (mod n)</nobr>, where <nobr>d(x),</nobr>\nfor x in U, are a family of independent uniformly distributed\n    random variables over the range <nobr>[0, n-1].</nobr>\n  </div>\n  <p style=\"MARGIN-BOTTOM: 0in; MARGIN-LEFT: 0.3in; MARGIN-TOP: 0.05in; TEXT-INDENT: -0.15in\"><b>Double\n  hashing</b> defines p(x, j) = <nobr>f(x)+(j-1)d(x) (mod n),</nobr>\nwhere the table size n\n  is prime, <nobr>(f(x), d(x))</nobr> is uniformly distributed over\n<nobr>[0, n-1]<font face=\"Lucida Console\">X</font>[1, n-1],</nobr>\nand the families f(x) and d(y), for <nobr>(x, y)</nobr> in U<font face=\"Lucida Console\">X</font>U,\n  are jointly independent.&nbsp;\n  </p><p style=\"MARGIN-BOTTOM: 0in; MARGIN-LEFT: 0.3in; MARGIN-TOP: 0.05in; TEXT-INDENT: -0.15in\"><b>Tertiary\n  Clustering </b>defines an idealized model that we also analyze. It was\n  probably invented to formalize a model of hashing that is more realistic than\n  uniform hashing, more efficient than linear probing and more tractable than\n  double hashing.\n  </p><p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in\"><b>Prior work</b>\n  </p><ol>\n    <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: -0.1in\">Uniform hashing is trivial\n    to analyze. Linear probing is much more difficult, and the exact analysis\n    originates with Knuth.&nbsp; The analysis of double hashing evolved in\n    sporadic spurts over the course of 20 years. The first milestone was the\n    analysis by Guibas and Szemer\ufffddi, which showed that its performance is\n    asymptotically equivalent to uniform hashing for load factors below 0.3.\n    Subsequently, Lueker and Molodowitch showed that the equivalence holds for\n    any fixed load below1.0. Ajtai, Guibas, Koml\ufffds and Szemer\ufffddi also\n    discovered this fact around the same time.</p>\n  </ol>\n  <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: -0.1in\"><b>What's missing from these\n  analyses.</b>\n  </p><ol>\n    <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: -0.1in\">The most serious issue is\n    that all of the hash functions are by definition unprogrammable because of\n    the assumptions about full independence.&nbsp; Real hash functions are\n    subroutines that are initialized by some number of random seeds that are\n    then used to compute a deterministic function of the seeds and the hash key.\n    Thus, hash functions are really pseudo-random functions whose sole source of\n    randomness comes from the seeds. As far as anyone knows, none of the prior\n    analyses can be adapted to this kind of restricted randomness.</p>\n  </ol>\n  <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in\">These issues are resolved in\n  the following Part I and Part II papers, plus one more that is listed later:\n  </p><p><a href=\"http://www.cs.nyu.edu/faculty/siegel/cl.ps\">Double Hashing is\n  Computable and Randomizable with Universal Hash Functions. (The paper is here.)</a> <font size=\"3\">(back\n  to <a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a>\n  <a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font><br>\n  <a href=\"http://www.cs.nyu.edu/faculty/siegel/dl.ps\">Closed Hashing is\n  Computable and Optimally Randomizable with Universal Hash Functions. (The paper is here.)</a></p>\n</font></blockquote><font size=\"3\">\n<ol>\n  <p style=\"MARGIN-BOTTOM: -0.05in; MARGIN-TOP: -0.05in\">The basic result is:</p>\n</ol>\n<ol>\n  <ol>\n    <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: -0.1in\">Asymptotically, the use of\n    programmable hash functions with limited (but sufficient) numbers of random\n    seeds has the same performance as idealized pure random hash functions. Moreover, our proof techniques even improve and extend some of the results\n    for purely random hash functions.</p>\n  </ol>\n  <ol>\n    <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">These works offer the\n    first analyses for closed hashing and sublinear numbers of seeds. Together\n    with the paper discussed next, the results give an affirmative answer the question:&nbsp; <b>Can the performance results predicted by the idealized\n    analyses be achieved for programmable hash functions that can be evaluated\n    in constant time?</b> This study is also the only analysis where one proof\n    method covers a number of different hashing schemes. The analysis is all\n    about controlling the exponential blowup of error bounds that occurs from\n    inclusion-exclusion arguments.</p>\n  </ol>\n  <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">The\n  commonality and generality in the proofs are a consequence of using the\n  statistical characteristics of the hash functions as opposed to their specific\n  implementation features. For example, in the case of double hashing, the two\n  random variables p(x,j) and p(x,i) are statistically independent provided i is\n  unequal to<font face=\"\"> </font>j.&nbsp; From this perspective, the performance\n  analysis cannot distinguish among double hashing with limited independence,\n  full independence, nor uniform hashing with full or limited\n  independence.&nbsp; Consequently, we \"only\" have to show that if the\n  limited independence is large enough, then the performance, for a fixed load\n  factor <font face=\"Symbol\">a</font>&lt;1, has some very complicated\n  formulation based on inclusion-exclusion semantics, that, apart from a large\n  number of error contributions that sum to <nobr>O(1/n),</nobr> are all the same expression.\n  Since we know that this expression equals <nobr>1/(1- <font face=\"Symbol\">a</font>)\n  +O(1/n)</nobr> for uniform hashing with full independence, we do not have to evaluate\n  the beastly mess.\n  </p><p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">A minor\n  consequence is that the error bound for double hashing is driven down to <nobr>O(1/n),</nobr>\n  which is sharper than the <nobr>O(1/<i>\u221a</i><span style=\"text-decoration: overline\">n</span>)</nobr> or so of Lueker and\n  Molodowitch.\n  </p><p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">Lastly, the Lueker- Molodowitch\n  proof deserves a special acknowledgment. The work is simply beautiful and worth reading.\n  A thumbnail sketch of\n  their work and how its proof schema influenced this study will be prepared at\n  a later date.\n  </p><p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-LEFT: 0in; MARGIN-TOP: 0.1in\">\n<a name=\"Fasthash\"></a>The\n  bad news is that all of these limited independence results use c log n-wise\n  independence. So the next question is:\n  </p><ol>\n    <b>What computational resources are necessary for a hash function p(x) to\n    exhibit clog n-wise independence and be computable in constant time?&nbsp;</b>\n    The answer is in:\n  </ol>\n  <div style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in\">\n    <a href=\"http://www.cs.nyu.edu/faculty/siegel/FASTH.pdf\">On Universal\n    Classes of Extremely Random Constant Time Hash Functions and their\n    Time-space Tradeoff. (The paper is here.)</a><font size=\"3\"> (Back to \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a>\n    <a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font>\n  </div>\n  <p style=\"MARGIN-TOP: 0.1in\">A family of functions F that map [0,m]-&gt;[0,n],\n  is said to be h-wise independent if any h points in [0,m] have an image, for\n  randomly selected f in F, that is uniformly distributed. <b>This paper gives\n  both probabilistic and explicit constructions of (n<sup><font face=\"Symbol\">e</font></sup>)-wise\n  independent functions, for suitably small constant positive <font face=\"Symbol\">e</font>,\n  that can be evaluated in constant time for the standard random access model of\n  computation.&nbsp;</b> As a consequence, many probabilistic algorithms can for\n  the first time be shown to achieve their expected asymptotic performance for a\n  feasible model of computation. While the issue of fast, provably sound hashing\n  has been recognized as important for decades, this work represents the first\n  progress toward solving the problem.\n  </p><dl>\n    <dd>\n      <p style=\"MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">We show that the <i>speed/degrees-of-freedom</i>\n      tradeoff for such provably sound hash functions is actually a tradeoff\n      between the independence h and the caching storage plus precomputation.&nbsp; Loosely put,\n      any h-wise independent hash function that uses fewer than h operations,\n      needs no more than 2h random seeds, which is wonderful. But such a program\n      also must have, for a suitable small but positive constant\n<font face=\"Symbol\">d</font>&lt; 1,\n a cache of z = n<sup><font face=\"Symbol\">d</font></sup>\n      pseudorandom precomputed seeds, which can be computed from the 2h random\n      seeds.&nbsp; The program will use the hash key to locate a few of the\n      cached pseudorandom seeds, which can then be combined to produce the hash\n      value. Of course, the location of the next seed to read can be a function\n      of the seeds already read as well as the key, and our lower bound includes\n      this possibility.<br>\n      We offer one lower bound with two interpretations.</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in\"><b>Lower bound: </b>Let F be an algorithm\n      that hashes keys from a domain [0,D] into a range [0,R]. Suppose that the\n      algorithm works by adaptively reading T pseudorandom seeds from a cache of\n      z words in [0, R].&nbsp; Suppose that F is h-wise independent, with a\n      commonly used non-uniformity in the distribution that is bounded by an\n      error parameter <font face=\"Symbol\">m.</font><br>\n      Then:</p>\n    </dd><dd>\n      <p align=\"center\" style=\"MARGIN-TOP: 0.05in\"><b>&nbsp;z(z-1)(z-2). .\n      .(z-T+1) &gt; (h-2)(h-3). . .(h-T)|D|(1-<font face=\"Symbol\">m</font>/\n      |R|).</b></p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in\">Notice how this bound collapses when T goes\n      from h-1 to h. It says that either T is h or the cache size z must be\n      about as large as D<sup>1/T</sup> words.</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in\">There are two additional observations that\n      must be said.&nbsp; First, the bound shows that <font face=\"Symbol\">m</font>\n      does not appear to be an important parameter, and our constructions show\n      that this extra freedom is useless. Second, when D&gt;&gt;R, it shows that\n      the storage costs can be high.&nbsp; As a consequence, we are obliged to\n      quantify a new kind of relaxation in the statistical characteristics of\n      F.&nbsp; The resulting error is provably insignificant in terms of\n      its influence on the performance of probabilistic algorithms.&nbsp;\nHowever, this change allows D to be replaced by R in this lower\n      bound, and our probabilistic constructions suggest that the resulting bound for \n      T might be achievable to within a factor of 2.&nbsp;</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">The difficult part of\n      the algorithm is in determining which cache seeds to read.&nbsp; In\n      addition to our probabilistic existence arguments,\n      we also explicit constructions that give formally\n      comparable results but which, essentially, increase the cache size by a\n      constant factor and increase the running time by a very large constant\n      factor.</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">Lastly, we show that the\n      problem of finding the right (i.e. truly efficient) graphs is equivalent\n      to defining expander-like bipartite graphs with n input nodes,\nn<sup><font face=\"Symbol\">d</font></sup> output nodes,\nfor <font face=\"Symbol\">d</font> &lt; 1,\nand (small) constant input degree.&nbsp; In addition, the\n      graph should be represented by a program/data set of size n<sup><font face=\"Symbol\">d</font></sup>\n      or less, and the program should be able to compute the adjacency list for\n      an input vertex in constant time.&nbsp; Weaker problems of this type are\n      open. There has been progress on these questions, but at a very slow pace.</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">In summary, the problem\n      of implementing fast, highly random hash functions is formally solved so\n      that the theoretical model is provably sound and usable.&nbsp; The\n      prospects for really implementable functions are enhanced by the\n      identification of the right problem to solve.&nbsp; On the other hand, the\n      problem seems to be very difficult.&nbsp;</p>\n    </dd><dd>\n      <p style=\"MARGIN-TOP: 0.05in; TEXT-INDENT: 0.5in\">Applications are given\n      for closed hashing and for the randomization necessary in a pipelined\n      implementation of Ranade's PRAM emulation scheme on an <nobr>N <font face=\"Lucida Console\">X</font>\n      log N</nobr> butterfly network that uses <nobr>N log N</nobr> switches but\n      only one column of N processors and memory modules. The formal performance\n      results are the same as Ranade's scheme, which requires NlogN\n      processors.&nbsp; In terms of the processor-time characteristics, this\n      implementation gives is an optimal emulation of a PRAM.</p>\n    </dd>\n  </dl>\n  <br>\n  <center><b><font size=\"+2\"><a name=\"Median\"></a>Medians of discrete random\n  variables</font></b></center>\n  <p>&nbsp;<a href=\"http://www.cs.nyu.edu/faculty/siegel/median.pdf\">Median\n  bounds and their application. (The paper is here.)</a> <font size=\"3\">(Back to \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a>\n  <a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font>\n\n \n  </p><ol type=\"A\">\n    <li>\n      <p style=\"MARGIN-TOP: 0in\">Applications of median specific median bounds.\n      </p><ol type=\"1\">\n        <li>Computing tail estimates for functions of weakly dependent random\n          variables.\n        </li><li>The analysis of probabilistic processes.\n          <ol>\n            <div style=\"MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              A fairly direct analysis of the log log n + O(1) time expected performance\n              of interpolation search.\n            </div>\n          </ol>\n          <ol>\n            <div style=\"MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              Bounds applicable to double hashing.\n            </div>\n          </ol>\n          <ol>\n            <div style=\"MARGIN-BOTTOM: 0.05in; MARGIN-LEFT: 0.2in; MARGIN-TOP: 0in; TEXT-INDENT: -0.2in\">\n              Bounds used to prove optimal performance for a pipelined version\n              of Ranade's PRAM emulation scheme.\n            </div>\n          </ol>\n      </li></ol>\n        </li><li>\n          <p style=\"MARGIN-BOTTOM: 0in; MARGIN-TOP: 0.05in\">Systematic methods\n          for computing medians of families of random variables.\n          </p><ol>\n            <div style=\"MARGIN-BOTTOM: 0in\">\n              The approach uses naive analytic symmetrization to replace messy\n              asymptotics with simple global analysis that is sort of a study of\n              shapology. As an elementary \n example, let <nobr>F(x)</nobr> be the cumulative\n              distribution function of a non-negative random variable with mean\n              \u03bc. Then <nobr>F(x)+F(2\u03bc-x)</nobr> is symmetric on <nobr>[0,\n              2\u03bc]</nobr>, and almost flat.  A proof that its average value on\n              <nobr>[0, 2\u03bc]</nobr> exceeds 1, along with an analysis\n              of its critical points can show that <nobr>2F(\u03bc)&gt;1.</nobr>\n            </div>\n          </ol>\n        </li>\n    <li>Specific median results for new families of random variables. Some of\n      the pure probability results are easier to explain by example than with\n      formal definitions of the underlying distributions.<br>\n      <font size=\"+0\">(In the following results, we always assume that various\n      values are integers. If not, the median will be one of the two\n      integers nearest to the value in question.)</font>\n      <ol type=\"i\">\n        <li>First, the main prior result that should be better known: You have\n          1001 coins that are independent but might not be fair or have\n          identical probabilities of success. You know that the expected number\n          of heads is 37. A weak interpretation of the theorem [due to Jogdeo and Samuels] reads:</li>\n      </ol>\n      <ol>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\"><b>When the mean is\n        an integer for sums of Bernoulli Trials it is also the median.</b></p>\n      </ol>\n      <ol>\n        <p style=\"MARGIN-BOTTOM: -0.15in; MARGIN-TOP: 0.05in\">\n        We use self-contained systematic methods to establish this bound and to\n        prove a number of new estimates that do not follow from the Jogdeo-Samuels\n results.\n      </p></ol>\n      <ol>\n        <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: -0.00in\">&nbsp;\n        </p><li>As before, you have 1001 independent heterogeneous coins\n          and know that the expected number of heads is the integer m. You\n          perform the following experiment (designed to eliminate outliers):\n          Toss all the coins. Accept the answer if the number of heads is within\n          r of m, where r is fixed. Otherwise repeat the experiment until the\n          outcome is in the desired interval.\n          <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\"><b>Then\n          regardless of what r you choose, m is still the median.</b></p>\n        </li><li>\n          <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">You have 1001\n          heterogeneous red coins and 30001 heterogeneous green coins. The\n          expected number of green heads is the integer <b><font color=\"#00ff00\">g</font></b>,\n          and the expected number of red heads is the integer <b><font color=\"#ff0000\">r</font></b>.\n          You repeatedly toss the coins until there are exactly <b><font color=\"#00ff00\">g</font></b>\n          + <b><font color=\"#ff0000\">r</font></b> heads.\n          </p><p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\"><b>Then the\n          median number of red heads is <font color=\"#ff0000\">r</font>.</b></p>\n        </li><li>\n          <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">Weighted\n          selection: You have an Urn that contains <b><font color=\"#ff0000\">R</font></b>\n          red balls and <b><font color=\"#00ff00\">G</font></b> green balls. Each\n          red ball weighs <font color=\"#ff0000\">r</font> grams, and each green\n          ball weights <b><font color=\"#00ff00\">g</font></b> grams. Let <b><font color=\"#ff0000\">x</font></b>\n          + <b><font color=\"#00ff00\">y</font></b> balls be drawn from the urn,\n          where <nobr><b><font color=\"#ff0000\">x</font> </b>= <b><font color=\"#ff0000\">R</font></b>(1-e<sup>-<b><font color=\"#ff0000\">r</font></b>t</sup>)</nobr>\n          and <nobr><b><font color=\"#00ff00\">y</font></b> = <b><font color=\"#00ff00\">G</font></b>(1-e<sup><font size=\"+0\">-</font><b><font color=\"#00ff00\">g</font></b>t</sup>)</nobr>\n          for some value of t. Suppose that <b><font color=\"#ff0000\">x</font></b>\n          and <b><font color=\"#00ff00\">y</font> </b>are integers.&nbsp; The\n          balls are selected one-by-one without replacement.&nbsp; The\n          probability of choosing a ball at the next round is its fraction of\n          the total weight among the balls that remain unselected.\n          </p><p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\"><b>Then the\n          median number of red balls selected is <font color=\"#ff0000\">x</font>.</b></p>\n        </li><li>\n          <p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">&nbsp;Jogdeo and\n          Samuels gave a very sharp formulation of their bound i., and we offer\n          a corresponding version for 1.&nbsp; In\n          brief, the J-S bound is this:\n          </p><p style=\"MARGIN-BOTTOM: 0.05in; MARGIN-TOP: 0.05in\">Let X be a sum of\n          independent, arbitrarily distributed Bernoulli Trials with mean E[X]=m\n          where m is an integer. Let <nobr>Prob{X = m} = p.</nobr><br>\n          &nbsp;&nbsp;&nbsp; Then [J-S]: <nobr>| Prob{X &lt; m} + p/2 - \ufffd|</nobr>\n          <u>&lt;</u> p/6, and this bound is tight.<br>\n          This bound is a generalization of a comparable statement for the\n          Poisson distribution, which was conjectured and partially established\n          by Ramanujan.&nbsp;Our methods do not give an independent proof of\n          this result. However, we present (via different methods) an\n          independent result that is comparable in structure and strength.\n          Let X be a sum of independent Bernoulli Trials\n          with mean E[X] = m. Suppose that m is an integer. Let Y be the\n          resulting random variable that has the conditional probability\n          distribution as defined in 1., so that |Y- m| &lt; r for some fixed\n          r.&nbsp; Let <nobr>Prob{Y = m} = p.</nobr><br>\n          <b>Then |Prob{Y &lt; m} + p/2 - \ufffd| <u>&lt;</u> p/4, and this bound is\n          tight.</b><font size=\"3\"><br>(Back to <a href=\"http://www.cs.nyu.edu/faculty/siegel/#BArea\">Area</a>\n          <a href=\"http://www.cs.nyu.edu/faculty/siegel/#Bhashing\">Hashing</a> \n<a href=\"http://www.cs.nyu.edu/faculty/siegel/#BMedians\">Medians</a>)</font></p>\n        </li>\n      </ol>\n    </li>\n  </ol>\n  <hr>\n</ol>\n\n\n\n\n\n</font></font></font></body></html>"