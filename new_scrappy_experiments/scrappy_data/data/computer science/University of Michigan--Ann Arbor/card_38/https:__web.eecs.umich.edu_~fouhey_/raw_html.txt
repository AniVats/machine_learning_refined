"<html><head>\n<meta name=\"google-site-verification\" content=\"OumrM4BOTn7F8EAqZl3fYySp7h0cd1zCrbZVqeLZ8LE\">\n<title>David Fouhey</title>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"default.css\">\n<script type=\"text/javascript\" async=\"\" src=\"https://www.google-analytics.com/analytics.js\"></script><script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-28335121-6\"></script>\n<script>\nwindow.dataLayer = window.dataLayer || [];\nfunction gtag(){dataLayer.push(arguments);}\n    gtag('js', new Date());\n    gtag('config', 'UA-28335121-6');\n</script>\n\n<script>\nvar state = 0;\nvar sstate = 0;\n//the two email addresses\n//var scmu =  \"xdgtneczstcd.tlb.rb\";\n//var s = \"xdgtneczstcd.xdkdjqda.rbdd\";\n//var s= \"tcd-gbhlts`xdgtne'\"\nvar s = \"xdgtnezstcd.gbhlt\"\n//var s = \"xdgtneczstcd.xdkdjqda\";\n\nvar acode = \"a\".charCodeAt(0);\nvar zcode = \"z\".charCodeAt(0);\n\nfunction descramble(){\n    if(sstate == s.length){\n        deobfuscate();        \n    } else {\n        var toBump = s.charCodeAt(sstate);\n        if(s[sstate] != \".\"){\n            toBump += 1;\n            if(toBump > zcode){ toBump = acode; }\n        }\n        s = s.substring(0,sstate)+String.fromCharCode(toBump)+s.substring(sstate+1,s.length);\n        document.getElementById('emailHolder').innerHTML = s;\n        sstate += 1;\n        setTimeout(\"descramble()\",50);\n    } \n}\n\nfunction deobfuscate(){\n    if(state==0){\n        var parts = s.split(\"at\");\n        s = parts[0].split('').reverse().join('')+\"at\"+parts[1];\n    } else if(state==1){\n        var parts = s.split(\"at\");\n        s = parts[0]+\"at\"+parts[1].split('').reverse().join('');\n    } else {\n        s = s.replace(\"at\",\"@\");\n    }\n    document.getElementById('emailHolder').innerHTML = s;\n    state+=1;\n    if(state < 3){\n        setTimeout(\"deobfuscate()\",100);\n    }\n}\n\nfunction setEmail(){ document.getElementById(\"emailHolder\").innerHTML = s; }\n\n//var heads = new Array('newhead.jpg','2_21.jpg','3_51.jpg','6_1.jpg','newhead_normals.jpg');\nvar heads = ['crop4.jpg','head/2_21.jpg','head/newhead_normals.jpg'];\nvar id = -1;\n\nfunction cycleHead(){\n    if(id != -1){\n        document.getElementById('head').src = heads[id]; \n    }     \n    id = (id+1) % heads.length;\n    setTimeout('cycleHead()',5000);\n}\n\n\nfunction onloader(){\n    setEmail();\n    cycleHead();\n}\nwindow.onload = onloader;\n</script>\n</head>\n<body>\n<div class=\"limiter\">\n\n<div class=\"mainbox\" style=\"padding-left:10px\">\n<table style=\"font-size:100%;padding:0px;margin:0px;\"><tbody><tr valign=\"top\"><td>\n<img src=\"crop4.jpg\" style=\"border: 1px solid #000; height:150px;\" id=\"head\">\n</td><td>\n<b>David Fouhey</b><br>\n<b>Quick info:</b>\n<a href=\"docs/cv.pdf\">CV</a> &nbsp; \n<a href=\"http://scholar.google.com/citations?user=FLcpd34AAAAJ\">Google Scholar</a> &nbsp;\n<br>\n<b>Email (<a href=\"#\" onclick=\"descramble()\">Reveal</a>):</b> <span id=\"emailHolder\">xdgtnezstcd.gbhlt</span> (my Berkeley &amp; CMU emails do not work)<br>\n<a href=\"#\" onclick=\"document.getElementById(&quot;joining&quot;).style.display=&quot;block&quot;;\">About joining my research group <b>(please read before emailing)</b></a> \n<br>\n<noscript>If you're the type of person who browses without javascript, your first guess for my email is probably going to be right.<br/></noscript>\n<b>Name FAQ:</b> `foe'-`eee'. It rhymes with snowy or Joey: the key is to forget how it is spelled. It (but not me) is from County Cork, Ireland. \n<br>\n<b>Photos:</b> One picture is hard to identify a person with. <a href=\"pics/index.htm\">Here</a> are some more.\n</td></tr></tbody></table> \n<p><i>Summary:</i> \nI am an Assistant Professor of \nComputer Science and Engineering at\n<a href=\"https://www.cse.umich.edu/\">\nThe University of Michigan</a>. \nI was previously a postdoctoral fellow at\n<a href=\"https://eecs.berkeley.edu/\">UC Berkeley</a>, \nworking with\n<a href=\"http://people.eecs.berkeley.edu/~efros/\">Alyosha Efros</a>\nand\n<a href=\"http://people.eecs.berkeley.edu/~malik/\">Jitendra Malik</a>.\nBefore that, I received a Ph.D. in robotics from\n<a href=\"http://www.ri.cmu.edu/\">CMU</a> at\nwhere I worked with <a href=\"http://www.cs.cmu.edu/~abhinavg/\">Abhinav Gupta</a>\nand <a href=\"http://www.cs.cmu.edu/~hebert/\">Martial Hebert</a>.\n<!--                   \nI <b>was</b> a postdoctoral fellow (2016-)\nat UC Berkeley, working with \nI received a Ph.D. in Robotics (2011-2016) from\n<a href='http://www.ri.cmu.edu/'>The Robotics Institute</a> at\n<a href='http://cmu.edu/'>Carnegie Mellon University</a>, where\nI worked with <a href='http://www.cs.cmu.edu/~abhinavg/'>Abhinav Gupta</a>\nand <a href='http://www.cs.cmu.edu/~hebert/'>Martial Hebert</a> and\nwas a <a href='https://www.nsfgrfp.org/'>NSF</a>\nand then <a href='http://ndseg.asee.org'>NDSEG</a> fellow. \nNow I am elsewhere!\n-->\n</p>\n<p>\nFor more information about computer vision at Michigan, see\n<a href=\"http://vision.eecs.umich.edu/\">here</a>.\n</p>\n</div>\n\n<div id=\"joining\" class=\"mainbox contentbox\" style=\"display:none\">\n<h3>About Joining My Group</h3>\n<br>\nLong story short, I am always looking for motivated and talented students.\nI also receive hundreds of emails a day and cannot respond to\nall inquiries about joining my group. This document is not meant to be\nunfriendly, but just to save both of us time (many of these things are things\nI'd say).  \n<ul>\n<li><b>If you are a UM student</b>:<br> \nContact me by email and let's talk! \nBut to save time, please include your resume, an unofficial transcript, and\nwhy you're interested. I don't always\nhave the bandwidth to take on new mentees, but if I don't, I'll try to point\nyou in the right direction of someone who might have the bandwidth. I usually do a good job \nof responding to these emails and if I haven't, let me know.\n<br>\n<br>\n</li><li><b>If you are not in a UM degree program and would like to be my student</b>:<br>\nPlease <a href=\"https://www.eecs.umich.edu/eecs/graduate/cse/apply/\">apply directly</a> \nand mention my name (as well as others that interest you) in your research statement. That's\nall that is required! Here are answers to common questions:\n<ul>\n<li>I am always looking for students and will almost always be recruiting. \n</li><li>Admissions in anything related to computer vision and machine learning is\ncurrently extraordinarily competitive and I cannot assess your chances for you -- \nit depends highly on my group's needs at the moment and the full pool of applicants.\nHowever, you can look at current vision PhD students at Michigan to get a sense of typical profiles.\n</li><li>Emailing me genuinely does not improve your chances.\n</li><li>I genuinely read all PhD applications that mention my name, but \napplications are read and decided on by a central committee (I encourage reading <a href=\"http://www.cs.cmu.edu/~harchol/gradschooltalk.pdf\">Mor Harchol-Balter's guide to graduate school admissions in CS</a> for a good sense of how this process works)\n</li><li>I am not involved in the MS process at all\n</li><li>I am delighted to talk to people who have been admitted; I cannot talk to people before they are admitted due to time constraints.\n</li><li>I cannot reveal information about the process or give status updates.\n</li><li>I do not respond to most emails about this.\n</li></ul>\n<br>\n</li><li><b>If you are not in a UM degree program and would like to visit</b>:<br>\nI do host visitors occasionally but have obligations to my students first and foremost, as\nwell as to UM students. Here are answers to common questions:\n<ul>\n<li>I am not looking for visitors without a recommendation from somebody <i>I personally know or know of</i>. If you are currently\nworking with someone, please tell me who.\n</li><li>I do not advise capstone projects\n</li><li>I do not take any short-term (e.g., less-than-three month) undergraduate visitors except in highly exceptional circumstances.\n</li><li>I do not respond to the overwhelming majority of emails about this.\n</li></ul>\n</li></ul>\nIf you've read this page and think you have a question that has not been answered, please feel free to email\nand put ``(JJ Gibson)'' in the subject line.\n<br><br>\n\n</div>\n\n\n<div class=\"navcontainer\">\n<div class=\"buttonbox\"><a href=\"#news\">News</a></div>\n<div class=\"buttonbox\"><a href=\"#students\">Students</a></div>\n<div class=\"buttonbox\"><a href=\"#research\">Research</a></div>\n<div class=\"buttonbox\"><a href=\"#teaching\">Teaching</a></div>\n<div class=\"buttonbox\"><a href=\"#publications\">Publications</a></div>\n<div class=\"buttonbox\"><a href=\"#miscellaneous\">Misc.</a></div>\n<div class=\"buttonbox\"><a href=\"#fun\">Fun</a></div>\n</div>\n\n\n\n\n<a name=\"news\"></a>\n<div class=\"mainbox contentbox\">\n<h3>News</h3>\n<ul>\n<li>Three papers accepted at CVPR. Congratulations to Dandan, Mohamed, Nilesh, Jiaqi and Michelle!<br>\nWe are also releasing pre-release versions of our model for hand state detection -- contact me if you are interested. \n</li><li>Our paper on predicting extreme UV solar irradiance is out and \n<a href=\"https://www.techexplorist.com/monitoring-suns-ultraviolet-emission-using-deep-learning/26846/\">getting</a>\n<a href=\"https://www.inverse.com/article/59765-a-key-nasa-discovery-could-protect-mega-constellations-like-spacex-starlink\">some</a>\n<a href=\"https://www.theregister.co.uk/2019/10/03/nasa_sun_ai/\">press</a>\n<a href=\"https://phys.org/news/2019-10-team-deep-sun-ultraviolet-emission.html\">coverage</a>. There are some more down-to-earth things in the pipeline too...\n<!--\n<li>Stop using contrived versions of MNIST to test your algorithms!\nCheck out our new dataset of 8 years of\nsolar data <a href='https://github.com/dfouhey/sdodemo'>here</a>,\nincluding 60K observations in 512x512 glory across 12 modalities (plus\n14 scalar labels too). \n<li>I started at the University of Michigan.\n<li>I visited \nJosef Sivic and Ivan Laptev in \nINRIA, <a href='http://www.di.ens.fr/willow/'>Willow Group</a>,\nand had a wonderful time.  \n<li>Check out our new video dataset, <a href='2017/VLOG/'>VLOG</a>\n<li>The next iteration of our <a href='http://bridgesto3d.github.io/'>Bridges to 3D</a> workshop, which is about how 3D vision can help and be helped by other fields, will appear at CVPR 2018. \n<li>I'm on the job market, looking for a tenure-track job!\n<li>I organized a workshop, <a href='https://bridgesto3d.github.io/'>Bridges to 3D</a>, at CVPR 2017 about how 3D vision can help and be helped by other fields with <a href='http://www.cs.utexas.edu/~huangqx/'>Qixing Huang</a>\nand <a href='http://people.csail.mit.edu/lim/'>Joseph Lim</a>. \n<li><a href='http://www.oneweirdkerneltrick.com/rank.pdf'>Visual Rank Estimation</a>\n(<i>N.B.: not a real paper</i>) featured on K&aacute;roly Zsolnai-Feh&eacute;r's \n<a href='https://www.youtube.com/watch?v=bLFISzfQCDQ'>Two Minute Paper</a>\nVideo Series!\n<li>I started a postdoc in Fall 2016 at UC Berkeley with Jitendra Malik and Alyosha Efros.\n-->\n<!--\n<li>My picks for CVPR 2016 were featured in <a href='http://www.rsipvision.com/CVPR2016-Tuesday/'>CVPR Daily News</a>\n<li>I co-taught <a href='http://graphics.cs.cmu.edu/courses/16-824/2016_spring/'>16-824 (Visual Learning and Recognition)</a> in Spring 2016.\n<li> I visited the <a href='http://www.robots.ox.ac.uk/~vgg/'>VGG group at Oxford</a> for Summer 2015.\n-->\n</li></ul>\n</div>\n\n<a name=\"research\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Research</h3>\nI work on computer vision and machine learning.\nI want to develop autonomous systems that can learn to build representations\nof the underlying state and dynamics of the world through observation (and\npotentially interaction). <br><br>\n\nTowards this end, \nI'm particularly interested in two aspects of vision.\nBoth are motivated by the somewhat obvious observation that images are the projection of a real, physical, and 3D\nworld to a 2D image plane of an agent. This has lead to research on\nunderstanding the world in terms of:\n<ul>\n<li><b>physical properties</b>: How do we recover a rich 3D world from a 2D image? \nI am especially interested in representations -- the answers that are obvious\nare also obviously defective -- as well as how we should reconcile our strong\nprior knowledge about this structure of the problem with data-driven\ntechniques.<br> Lately, I've become interested in applying this more broadly with the\nhope that we can develop AI systems that can learn how the physical world\nworks from observation, including work on solar physics.\n</li><li><b>functional properties</b>: How do we infer and\nunderstand opportunities for\ninteraction? I am interested how an agent (e.g., human or robot) can\ninteract with the world, including in terms of what this implies for 3D understanding.\n</li></ul>\n</div>\n\n<a name=\"students\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Students</h3>\nA full list of people I work with is <a href=\"http://fouheylab.eecs.umich.edu/people.html\">here</a>, but my current\nPhD students are:\n<ul>\n<li><a href=\"http://relh.net/\">Richard Higgins</a> (Sep 2019 -- present)\n</li><li><a href=\"https://nileshkulkarni.github.io/\">Nilesh Kulkarni</a> co-advised with <a href=\"https://web.eecs.umich.edu/~justincj/\">Justin Johnson</a> (Sep 2019 -- present) \n</li><li><a href=\"https://jasonqsy.github.io/\">Shengyi Qian</a> (Sep 2019 -- present)\n</li><li><a href=\"https://www.linkedin.com/in/liz-olson-424b2911b\">Liz Olson</a> co-advised with <a href=\"https://web.eecs.umich.edu/~emilykmp/\">Emily Mower Provost</a> (Sep 2019 -- present)\n</li></ul>\nMS Students:\n<ul>\n<li><a href=\"https://crockwell.github.io/\">Christopher Rockwell</a> (May 2019 -- present)\n</li><li><a href=\"https://ddshan.github.io/\">Dandan Shan</a> (January 2019 -- present)\n</li><li><a href=\"https://jinlinyi.github.io/\">Linyi Jin</a> (May 2019 -- present)\n</li></ul>\n<br>\n</div>\n\n\n<a name=\"teaching\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Teaching</h3>\nUniversity of Michigan:\n<ul>\n<li>EECS 598 -- Special Topics: The Ecological Approach to Visual Perception\n<a href=\"teaching/EECS598_W20/\">Winter 2020</a>\n</li><li>EECS 442 (Computer Vision), \n    <a href=\"teaching/EECS442_F19/\">Fall 2019</a>, \n    <a href=\"teaching/EECS442_W19/\">Winter 2019</a>\n</li></ul>\nEarlier:\n<ul>\n<li>UC Berkeley 294-43 (Object and Activity Recognition) <a href=\"http://sites.google.com/site/ucbcs29443/\">Spring 2018</a>,\n<a href=\"https://sites.google.com/site/ucbcs29443fall2017/\">Fall 2017</a>,\n<a href=\"https://sites.google.com/site/ucbcs29443spring2017/\">Spring 2017</a>. With Trevor Darrell and Alexei Efros.\n</li><li>CMU <a href=\"http://graphics.cs.cmu.edu/courses/16-824/2016_spring/\">16-824 (Visual Learning and Recognition)</a>, Spring 2016. \nWith Abhinav Gupta.\n</li><li><a href=\"http://www.cs.cmu.edu/~dfouhey/ECCV2014Tutorial/\">Tutorial on 3D Scene Understanding</a> \nat ECCV 2014 with \n<a href=\"http://www.cs.cmu.edu/~abhinavg/\">Abhinav Gupta</a>, <a href=\"http://www.cs.cmu.edu/~hebert/\">Martial Hebert</a>, and \n<a href=\"http://dhoiem.cs.illinois.edu/\">Derek Hoiem</a>.\n<a <br=\"\"><br></a></li></ul></div><a <br=\"\">\n\n\n</a><a name=\"publications\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Publications</h3>\n\n<table cellspacing=\"20\">\n\n<tbody><tr><td>\n<img src=\"2020/100DOH/teaser.jpg\" width=\"300\">\n</td><td>\nD. Shan, J. Geng*, M. Shu*, <b>D.F. Fouhey</b><br>\n<i>Understanding Human Hands in Contact at Internet Scale</i><br>\nCVPR 2020<br>\n[PDF TBD]\n</td></tr>\n\n<tr><td>\n<img src=\"2020/novelVP/teaser.jpg\" width=\"300\">\n</td><td>\nM. Banani, J. Corso, <b>D.F. Fouhey</b><br>\n<i>Novel Object Viewpoint Estimation through Reconstruction Alignment</i><br>\nCVPR 2020<br>\n[PDF TBD]\n</td></tr>\n\n<tr><td>\n<img src=\"2020/acsm/teaser.jpg\" width=\"300\">\n</td><td>\nN. Kulkarni, A. Gupta, <b>D.F. Fouhey</b>, S. Tulsiani<br>\n<i>Articulation-aware Canonical Surface Mapping</i><br>\nCVPR 2020<br>\n[PDF TBD]\n</td></tr>\n\n<tr><td>\n<img src=\"2019/sa/teaser.jpg\" width=\"300\"> \n</td><td>\nA. Szenicer*, <b>D.F. Fouhey*</b>, A. Munoz-Jaramillo, P.J. Wright, R. Thomas, R. Galvez, M. Jin, M.C.M. Cheung<br>\n<i>A Deep Learning Virtual Instrument for Monitoring Extreme UV Solar Spectral Irradiance</i><br>\nScience Advances, Vol. 5, Number 10, 2019<br>\n[<a href=\"https://advances.sciencemag.org/content/5/10/eaaw6548\">Open Acess</a><a>] &nbsp;\n[</a><a href=\"2019/sa/szenicer19.bib\">Bibtex</a>] &nbsp;\n[<a href=\"2019/sa/media/0.htm\">Prediction Video</a>] &nbsp; \n[<a href=\"2019/sa/media/1.htm\">Activations Video</a>] &nbsp; \n[<a href=\"2019/sa/media/2.htm\">Overview Video</a>] &nbsp; \n</td></tr>\n<tr><td> </td><td>\nPress coverage/releases:<br>\n<a href=\"https://www.inverse.com/article/59765-a-key-nasa-discovery-could-protect-mega-constellations-like-spacex-starlink\">\n <img src=\"2019/sa/coverage/inverse.png\" alt=\"inverse.com\" height=\"30\">(Inverse.com)</a> &nbsp;\n<a href=\"https://seti.org/press-release/nasa-frontier-development-lab-uses-deep-learning-monitor-suns-ultraviolet-emission\">\n<img src=\"2019/sa/coverage/seti.png\" alt=\"SETI institute\" height=\"30\">(SETI Institute)</a><br>\n<a href=\"https://www.theregister.co.uk/2019/10/03/nasa_sun_ai/\">\n<img src=\"2019/sa/coverage/register.png\" alt=\"theregister.co.uk\" height=\"30\">(theregister.co.uk)</a> &nbsp;\n<a href=\"https://phys.org/news/2019-10-team-deep-sun-ultraviolet-emission.html\">\n<img src=\"2019/sa/coverage/phys.org.png\" alt=\"phys.org\" height=\"30\">(phys.org)</a><br>\n<a href=\"https://www.techexplorist.com/monitoring-suns-ultraviolet-emission-using-deep-learning/26846/\">\n<img src=\"2019/sa/coverage/techexplorist.png\" alt=\"techexplorist\" height=\"30\">(TechExplorist.com)</a> &nbsp;\n<a href=\"https://www.scientificamerican.com/article/black-holes-volcanic-scrolls-and-a-teeny-tiny-heartbeat-science-gifs-to-start-your-week/\">\n<img src=\"2019/sa/coverage/sa.png\" alt=\"scientific american\" height=\"30\">(ScientificAmerican.com)</a><br>\n<a href=\"https://eos.org/articles/virtual-super-instrument-enhances-solar-spacecraft\">\n<img src=\"2019/sa/coverage/eos.png\" alt=\"eos.org\" height=\"30\">(Earth &amp; Space Science News)</a> &nbsp;\n<a href=\"https://www.hpcwire.com/2019/10/17/nasa-uses-deep-learning-to-monitor-solar-weather/\">\n<img src=\"2019/sa/coverage/hpcwire_logo.png\" height=\"30\">(hpcwire.com)</a><br>\n<a href=\"https://www.sciencedaily.com/releases/2019/10/191007103609.htm\">\n<img src=\"2019/sa/coverage/sd.png\" height=\"30\">(sciencedaiy.com)</a>\n</td></tr> \n\n<!-- Instructional -->\n<tr><td>\n<img src=\"2019/instructional/teaser.jpg\" width=\"300\">\n</td><td>\nD. Zhukov, J.-B. Alayrac, G. Cinbis, <b>D.F. Fouhey</b>, I. Laptev, J. Sivic<br>\n<i>Cross-task weakly-supervised learning from instructional videos</i><br>\nCVPR 2019<br>\n[<a href=\"2019/instructional/paper.pdf\">PDF</a>] &nbsp;\n[<a href=\"https://github.com/DmZhukov/CrossTask\">Project Page</a>] &nbsp;\n[<a href=\"https://arxiv.org/abs/1903.08225\">Arxiv</a>] &nbsp;\n[<a href=\"2019/instructional/zhukov19.bib\">Bibtex</a>] \n\n\n</td></tr><tr><td>\n<!-- APJs -->\n</td></tr><tr><td>\n<img src=\"2019/apjs/teaser.jpg\" width=\"300\">\n</td><td>\nR. Galvez*, <b>D.F. Fouhey*</b>, M. Jin, A. Szenicer, A. Munoz-Jaramillo, M.C.M. Cheung, P.J. Wright, M.G.\nBobra, Y. Liu, J. Mason, R. Thomas<br>\n<i>A Machine Learning Dataset Prepared From the NASA Solar Dynamics Observatory Mission</i><br>\nThe Astrophysical Journal Supplement, 242:1, 2019<br>\n[<a href=\"https://iopscience.iop.org/article/10.3847/1538-4365/ab1005/pdf\">PDF</a>] &nbsp; \n[<a href=\"https://arxiv.org/abs/1903.04538\">Arxiv</a>] &nbsp; \n[<a href=\"2019/apjs/galvez19.bib\">Bibtex</a>] &nbsp; \n[<a href=\"2019/apjs/vis.mp4\">Movie</a> &amp; <a href=\"2019/apjs/vis.txt\">Explanation</a>] &nbsp; \n[<a href=\"https://github.com/dfouhey/sdodemo\">Small dataset + demo</a>]<br>\n</td></tr> \n\n\n\n<!-- Path Following -->\n<tr><td>\n<img src=\"2018/navigation/teaser.jpg\" width=\"300\">\n</td><td>\nA. Kumar, S. Gupta, <b>D. F. Fouhey</b>, S. Levine, J. Malik<br>\n<i>Visual Memory for Robust Path Following</i><br>\nNeurIPS 2018 (<span style=\"color:#a00\"><b>Oral</b></span>)<br>\n[<a href=\"https://ashishkumar1993.github.io/rpf/\">Project Page</a>] &nbsp;\n[<a href=\"https://saurabhg.web.illinois.edu/pdfs/kumar2018visual.pdf\">PDF</a>] &nbsp; \n[<a href=\"2018/navigation/kumar18.bib\">Bibtex</a>] &nbsp;\n</td></tr>\n\n\n<!-- VLOG -->\n<tr><td>\n<img src=\"2017/VLOG/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, W. Kuo, A. A. Efros, J. Malik<br>\n<i>From Lifestyle VLOGs to Everyday Interactions</i><br>\nCVPR 2018<br>\n[<a href=\"2017/VLOG/index.html\">Project Page</a>] &nbsp;\n[<a href=\"https://arxiv.org/abs/1712.02310\">Arxiv</a>]  &nbsp;\n[<a href=\"2017/VLOG/fouhey17b.bib\">Bibtex</a>] &nbsp; \n</td></tr>\n\n\n<!-- Factored 3D -->\n<tr><td>\n<img src=\"2017/factored3d/teaser.jpg\" width=\"300\">\n</td><td>\nS. Tulsiani, S. Gupta, <b>D. F. Fouhey</b>, A. A. Efros, J. Malik<br>\n<i>Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene</i><br>\nCVPR 2018<br>\n[<a href=\"https://shubhtuls.github.io/factored3d/\">Project Page</a>] &nbsp;\n[<a href=\"https://arxiv.org/abs/1712.01812\">Arxiv</a>] &nbsp;\n[<a href=\"2017/factored3D/tulsiani17.bib\">Bibtex</a>] <br>\n</td></tr>\n\n<tr><td>\n<img src=\"2018/geons/teaser.jpg\" width=\"300\">\n</td><td>\nM. Lescroart, <b>D. F. Fouhey</b>, J. Malik<br>\n<i>Convolutional neural networks represent shape dimensions -- but not as accurately as humans</i>\n<br>\nAbstract at VSS 2018<br> \n[<a href=\"https://jov.arvojournals.org/article.aspx?articleid=2699413&amp;resultClick=1\">Abstract</a>]\n</td></tr>\n\n\n<!-- Visual Navigation -->\n<tr><td align=\"center\">\n<img src=\"2017/VisualNav/teaser.jpg\" width=\"250\">\n</td><td>\nS. Gupta, <b>D.F. Fouhey</b>, S. Levine, J. Malik<br>\n<i> Unifying Map and Landmark Based Representations for Visual Navigation</i><br>\nArxiv 2017<br>\n[<a href=\"https://s-gupta.github.io/cmpl/\">Project Page</a>] &nbsp;\n[<a href=\"https://arxiv.org/abs/1712.08125\">Arxiv</a>] &nbsp;\n[<a href=\"2017/VisualNav/gupta17.bib\">Bibtex</a>]\n</td></tr>\n\n\n\n\n<!-- Shape Attributes Journal Arxiv -->\n<tr><td>\n<img src=\"2017/shapeAttrJ/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, A. Gupta, A. Zisserman<br>\n<i>From Images to 3D Shape Attributes</i><br>\nTPAMI (Pre-print on Arxiv)<br>\nThe TPAMI version has ugly typesetting (full-width tables on the bottom?)\nthat I was unable to change. <b>Read the Arxiv one.</b><br>\n[<a href=\"https://arxiv.org/abs/1612.06836\">Arxiv</a>] &nbsp;\n[<a href=\"2017/shapeAttrJ/fouhey16b.bib\">Bibtex</a>]\n</td></tr>\n\n<!-- TL Networks -->\n<tr><td>\n<img src=\"2016/tlnetworks/teaser.jpg\" width=\"300\">\n</td><td>\nR. Girdhar, <b>D. F. Fouhey</b>, M. Rodriguez, A. Gupta<br>\n<i>Learning a Predictable and Generative Vector Representation for Objects</i><br>\nECCV 2016  (<span style=\"color:#00a\"><b>Spotlight</b></span>)<br>\n[<a href=\"2016/tlnetworks/tlnetworks.pdf\">Publication (PDF)</a>] &nbsp;\n[<a href=\"2016/tlnetworks/girdhar16b.bib\">Bibtex</a>] <br>\n[<a href=\"https://rohitgirdhar.github.io/GenerativePredictableVoxels/\">Project Page</a>]\n\n</td></tr>\n\n<tr><td>\n<img src=\"2016/dissertation/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b><br>\n<i>Factoring Scenes into 3D Structure and Style</i><br>\nDoctoral Dissertation<br>\n[<a href=\"2016/dissertation/dissertation.pdf\">Dissertation (PDF)</a>] &nbsp;\n[<a href=\"2016/dissertation/fouhey16b.bib\">Bibtex</a>] <br>\n[<a href=\"2016/dissertation/dissertation_talk.pdf\">Defense Slides (PDF)</a>]\n</td></tr>\n\n<!-- 3D Shape Attributes -->\n<tr><td>\n<img src=\"2016/shapeAttr/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, A. Gupta, A. Zisserman<br>\n<i>3D Shape Attributes</i><br>\nCVPR 2016 (<span style=\"color:#a00\"><b>Oral</b></span> - \n<a href=\"https://www.youtube.com/watch?v=0jW8lIYNbjo\">Watch the presentation on Youtube</a>)\n<br>\n[<a href=\"2016/shapeAttr/shapeAttr.pdf\">Publication (PDF)</a>] &nbsp;\n[<a href=\"2016/shapeAttr/fouhey16.bib\">Bibtex</a>] <br>\n[<a href=\"http://www.robots.ox.ac.uk/~vgg/data/sculptures/\">Project Page</a>] &nbsp;\n[<a href=\"2016/shapeAttr/sa_poster.pdf\">Poster (PDF)</a>] &nbsp; \n[<a href=\"2016/shapeAttr/talk_final.pptx\">Talk (PPTX)</a>] &nbsp; \n[<a href=\"2016/shapeAttr/talk_final.pdf\">Talk (PDF)</a>] &nbsp; \n</td></tr>\n\n<!-- Memex -->\n<tr><td>\n<img src=\"2016/clutter/teaser.jpg\" width=\"300\">\n</td><td>\nR. Girdhar, <b>D. F. Fouhey</b>, K. M. Kitani, A. Gupta, M. Hebert<br> \n<i>Cutting through the Clutter: Task-Relevant Features for Image Matching</i><br>\nWACV 2016<br>\n[<a href=\"2016/clutter/clutter.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2016/clutter/girdhar16.bib\">Bibtex</a>]\n</td></tr>\n\n<!-- Unsupervised 3D -->\n<tr><td>\n<img src=\"2015/no3d/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, W. Hussain, A. Gupta, M. Hebert<br>\n<i>Single Image 3D Without a Single 3D Image</i><br>\nICCV 2015<br>\n[<a href=\"2015/no3d/no3d.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2015/no3d/fouhey15.bib\">Bibtex</a>] <br>\n[<a href=\"2015/no3d/no3d_poster.pdf\">Poster (PDF)</a>] &nbsp;\n[<a href=\"2015/no3d/supp.pdf\">Supplemental (PDF)</a>] &nbsp;\n[<a href=\"2015/no3d/suppDetails.pdf\">Bonus Details (PDF)</a>] \n</td></tr>\n\n\n<!-- Designing 3D -->\n<tr><td>\n<img src=\"2015/deep3d/teaser.jpg\" width=\"300\">\n</td><td>\nX. Wang, <b>D. F. Fouhey</b>, A. Gupta<br>\n<i>Designing Deep Networks for Surface Normal Estimation</i><br>\nCVPR 2015<br>\n[<a href=\"2015/deep3d/deep3d.pdf\">Publication (PDF)</a>] &nbsp;\n[<a href=\"2015/deep3d/wang15.bib\">Bibtex</a>]\n</td></tr>\n\n\n<tr><td align=\"center\">\n<img src=\"2014/origami/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, A. Gupta, and M. Hebert<br>\n<i>Unfolding an Indoor Origami World</i><br>\nECCV 2014\n(<span style=\"color:#a00\"><b>Oral - <a href=\"http://videolectures.net/eccv2014_ford_fouhey_origami_world/?q=fouhey\">Watch the presentation on VideoLectures.net</a></b></span>)<br>\n[<a href=\"2014/origami/dfouhey_origami.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2014/origami/fouhey14c.bib\">Bibtex</a>] <br>\n[<a href=\"2014/origami/index.html\">Project Page</a>] &nbsp; \n[<a href=\"2014/origami/scgGal.pdf\">Extended Results (PDF)</a>] &nbsp; \n</td></tr>\n\n<!--Scene Dynamics-->\n<tr><td>\n<img src=\"2014/dynamics/teaser.jpg\">\n</td><td>\n<b>D. F. Fouhey</b> and C.L. Zitnick<br>\n<i>Predicting Object Dynamics in Scenes</i><br>\nCVPR 2014<br>\n[<a href=\"2014/dynamics/fouhey_zitnick_dynamics.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2014/dynamics/fouhey14b.bib\">Bibtex</a>] <br>\n<!-- [<a href='http://research.microsoft.com/en-us/um/people/larryz/clipart/abstract_scenes.html'>Project Page]</a> &nbsp;  -->\n[<a href=\"2014/dynamics/fouhey_zitnick_poster.pdf\">Poster (PDF)</a>] &nbsp; \n[<a href=\"2014/dynamics/fouhey_zitnick_supp.pdf\">Supplemental (PDF)</a>]  \n<br>\n</td>\n</tr>\n\n\n<!--People Watching IJCV-->\n<tr><td>\n<img src=\"2014/pwijcv/teaser.jpg\">\n</td><td>\n<b>D. F. Fouhey</b>, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic.<br>\n<i>People Watching: Human Actions as a Cue for Single View Geometry</i>.<br>\nIJCV (extended version of ECCV 2012 paper)<br>\n[<a href=\"2014/pwijcv/pw_IJCV_preprint.pdf\">Preprint (PDF)</a>] &nbsp;\n[<a href=\"http://link.springer.com/article/10.1007/s11263-014-0710-z\">Final version (via Springer)</a>]\n<br>\n</td>\n</tr>\n\n\n<!--Data-driven 3D Primitives-->\n<tr><td>\n<img src=\"2013/3dp/teaser.jpg\">\n</td><td>\n<b>D. F. Fouhey</b>, A. Gupta, and M. Hebert<br>\n<i>Data-Driven 3D Primitives for Single Image Understanding</i><br>\nICCV 2013<br>\n[<a href=\"2013/3dp/dfouhey_primitives.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2013/3dp/fouhey13.bib\">Bibtex</a>] <br>\n[<a href=\"2013/3dp/index.html\">Project Page</a>] &nbsp; \n[<a href=\"2013/3dp/poster_v3.pdf\">Poster (PDF)</a>] &nbsp; \n<br>\n</td>\n</tr>\n\n<!--People Watching-->\n<tr><td>\n\n<img src=\"2012/pw/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, V. Delaitre, A. Gupta, A. Efros, I. Laptev, and J. Sivic.<br>\n<i>People Watching: Human Actions as a Cue for Single View Geometry</i>.<br>\nECCV 2012 (<span style=\"color:#a00\"><b>Oral - \n<a href=\"http://videolectures.net/eccv2012_fouhey_geometry/\">Watch the presentation on VideoLectures.net</a>\n</b></span>)<br>\n[<a href=\"2012/pw/dfouhey_people.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2012/pw/fouhey12.bib\">Bibtex</a>]<br>\n[<a href=\"http://graphics.cs.cmu.edu/projects/peopleWatching/\">Project Page</a>] &nbsp; \n<br>\n</td>\n</tr>\n\n<!--Scene Semantics-->\n<tr><td>\n<img src=\"2012/sem/teaser.jpg\" width=\"300\">\n</td><td>\nV. Delaitre, <b>D. F. Fouhey</b>, I. Laptev, J. Sivic, A. Gupta, and A. Efros.<br>\n<i>Scene Semantics from Long-term Observation of People</i>.<br>\nECCV 2012<br>\n[<a href=\"2012/sem/delaitre_ECCV12.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"2012/sem/delaitre12.bib\">Bibtex</a>] <br>\n[<a href=\"http://www.di.ens.fr/willow/research/scenesemantics/\">Project Page</a>] \n</td>\n\n<!--- MOPED 3D -->\n</tr><tr><td>\n<img src=\"2012/objrec/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, A. Collet, M. Hebert, and S. Srinivasa<br>\n<i>Object Recognition Robust to Imperfect Depth Data</i>.<br>\nCDC4CV 2012 Workshop at ECCV 2012<br>\n[<a href=\"2012/objrec/dfouhey_cdc4cv12.pdf\">Publication (PDF)</a>] &nbsp;\n[<a href=\"2012/objrec/fouhey12b.bib\">Bibtex</a>]<br>\n[<a href=\"2012/objrec/supp_cdc4cv.pdf\">Supplemental(PDF)</a>] &nbsp;\n[<a href=\"2012/objrec/video_qualitative_cdc4cv.avi\">Supp. Video 1</a>] &nbsp;\n[<a href=\"2012/objrec/video_run_cdc4cv.avi\">Supp. Video 2</a>] &nbsp;\n</td></tr>\n\n\n\n<!--REV-->\n<tr><td>\n<img src=\"earlier/agu/teaser.jpg\" width=\"300\">\n</td><td>\nM. Costanza-Robinson, B. Estabrook, and <b>D. F. Fouhey</b><br>\n<i>Representative elementary volume \nestimation for porosity, moisture saturation, and air-water \ninterfacial areas in unsaturated porous media: Data quality implications</i><br>\n<u>(Sorry for not posting a pre-print!)</u><br>\nIn Water Resources Research, Volume 47, 2011<br>\n[<a href=\"http://onlinelibrary.wiley.com/doi/10.1029/2010WR009655/abstract\">Official Version</a>] &nbsp; \n[<a href=\"earlier/agu/costanzarobinson11.bib\">Bibtex</a>]<br>\n</td></tr>\n\n\n\n<!--Plane Detection-->\n<tr><td>\n<img src=\"earlier/mpd/teaser.jpg\" width=\"300\">\n</td><td>\n<b>D. F. Fouhey</b>, D. Scharstein, and A. Briggs.<br>\n<i>Multiple plane detection in image pairs using J-linkage.</i><br>\nICPR 2010<br>\n[<a href=\"earlier/mpd/Fouhey_PlaneDetect_ICPR2010.pdf\">Publication (PDF)</a>] &nbsp; \n[<a href=\"earlier/mpd/fouhey10.bib\">Bibtex</a>]<br>\nImplementation (Python and C) [<a href=\"earlier/mpd/MPD.zip\">Code (Zip)</a>] &nbsp;\n[<a href=\"earlier/mpd/ICPR2010Poster.pdf\">Poster (PDF)</a>] &nbsp; \n</td></tr> \n</tbody></table>\n</div>\n\n\n<a name=\"miscellaneous\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Miscellaneous</h3>\n\nYou may be interested in the following.<br><br>\n\n\n<b>Writing (arranged in chronological order):</b>\n<ul>\n<li>My Ph.D. dissertation, \n<i>Factoring Scenes into 3D Structure and Style</i> \n(<a href=\"2016/dissertation/dissertation.pdf\">Dissertation</a> and \n<a href=\"2016/dissertation/dissertation_talk.pdf\">Defense Slides</a>)<br>\n</li><li><a href=\"2016/evalSN/evalSN.html\">A note on some practical considerations when evaluating surface normals</a>\n</li><li>My A.B. thesis,\n<i>Multi-Model Estimation in the Presence of Outliers</i>\n(<a href=\"earlier/thesis/dfouhey_thesis.pdf\">Thesis</a> and\n<a href=\"earlier/thesis/dfouhey_thesisPoster.pdf\">Poster</a> and\n<a href=\"earlier/thesis/dfouhey_thesisPresentation.pdf\">Presentation</a>)\n</li></ul>\n<b>Miscellaneous</b>\n<ul>\n<li>\n<a href=\"misc/thesisTemplate/template.zip\">CMU RI Thesis Template (zip)</a> based off of\na CMU RI tech report template from Daniel Morris.\n</li></ul>\n</div>\n\n<a name=\"fun\"></a>\n<div class=\"mainbox contentbox\">\n<h3>Joke Papers</h3>\n<br>\nSometimes when I feel a creative itch, the end result is a joke publication\n(which despite the name often have a serious point to make).\nThese are all done with the one and only \n<a href=\"http://www.dimatura.net\">Daniel Maturana</a>.\n<ul>\n<li><b>Keras4Kindergartners.com:</b> Check out <a href=\"http://keras4kindergartners.com/\">our secret backup plan</a> for if research doesn't\nwork out. \n</li><li><b>Deep Excel:</b> Everybody knows that deep learning brings about synergy and so does Excel, \nso \n<a href=\"http://www.dimatura.net\">Daniel Maturana</a> and I released <a href=\"http://www.deepexcel.net/\">ExcelNet</a>, \na break-through technology that merges the power of Deep Learning with Excel.<br>\n<a href=\"fun/deepexcel/cnn.xls\">The spreadsheet</a> &nbsp; \n<a href=\"fun/deepexcel/paper.pdf\">The whitepaper</a> &nbsp; \n<a href=\"fun/deepexcel/slides.pdf\">The pitch slides</a> &nbsp; \n<a href=\"fun/deepexcel/protips.txt\">Protips (actually quite helpful)</a> &nbsp; \n</li><li><b>Visual Rank Estimation:</b> <a href=\"http://www.oneweirdkerneltrick.com/rank.pdf\">Visually Identifying Rank</a>\nwith <a href=\"http://www.dimatura.net/\">Daniel Maturana</a> proves that linear algebra can be replaced\nwith machine learning. It also shows that if you are a CNN, the much-hated jet colormap is actually the best colormap.\nWinner of the ``Most Frighteningly Like Real Research'' award at SIGBOVIK 2016.\n</li><li><b>Cat Basis Purrsuit:</b> I've been told that this is the highlight of my research career\nand it can only go downhill from here:\n<a href=\"http://oneweirdkerneltrick.com/catbasis.pdf\">Cat Basis Pursuit</a>\n</li><li><b>Celebrity Learning:</b> You may also know my award-winning work \nwith <a href=\"http://www.dimatura.net/\">Daniel Maturana</a> on celebrity-themed \nlearning, making money at home from Hilbert's Nulstellensatz, and more from \n<a href=\"http://www.oneweirdkerneltrick.com\">OneWeirdKernelTrick.com</a> \n</li><li><b>Kardashian Kernel:</b> The original, rarely imitated, never duplicated. Originator\nof the alphabetically-related-work section:\n<a href=\"http://oneweirdkerneltrick.com/sigbovik_2012.pdf\">The Kardashian Kernel</a>\n</li></ul>\n\n</div>\n\n\n<div class=\"mainbox contentbox\">\n<h3>Fun &amp; Games</h3>\n\n<ul>\n<li><b>The LaCroix Flavors, Ranked:</b> <a href=\"fun/lacroix/\">Uh, yeah</a>\n</li><li><b>Venus Transit Poster:</b> I wanted a poster of the <a href=\"https://en.wikipedia.org/wiki/Transit_of_Venus\">transit of venus</a>, and didn't like\nwhat I could find. So I made one myself with the original <a href=\"http://jsoc.stanford.edu/\">JSOC SDO/AIA data</a>. Here:\n<a href=\"fun/venus_poster_5k.pdf\">Highish-res (5k)</a> <a href=\"fun/venus_poster_8k.pdf\">Higher-res (8k)</a>\n<a href=\"fun/venus_poster_7p5k.pdf\">Padded to 30\"x40\"</a> \n\n</li><li><b>Paris 20/20:</b> The only way to see Paris is to walk through all 20 <a href=\"https://en.wikipedia.org/wiki/Arrondissements_of_Paris\">arrondissements</a>\nin a single journey by foot. \nCo-conspirators (so far!): <a href=\"http://imagine.enpc.fr/~aubrym/\">Mathieu Aubry</a> <a href=\"http://andrewowens.com/\">Andrew Owens</a>.<br>\nPossible variants: <a href=\"https://www.mappedometer.com/?maproute=711518\">here</a> \n<a href=\"https://www.mappedometer.com/?maproute=620052\">another</a> \n<a href=\"https://www.mappedometer.com/?maproute=619319\">the originally planned one (and 20 miles too)</a>.\n</li><li><b>Warning: Deep Nets!</b> As mentioned in the New Yorker! You too can have\n    <a href=\"fun/deepnets.pdf\">a warning sign for your door</a>.\n</li><li><b>Free parking in Berkeley:</b> Parking people hate him. Find out how to park on Berkeley's campus\nfor free with <a href=\"fun/parking.pdf\">this 1 weird trick</a>.\n</li><li><b>Caffe64:</b> dependency-free deep learning in &lt; 12KB via the magic of assembly. Clone and\nstar it <a href=\"https://github.com/dfouhey/caffe64\">here</a>\n</li><li><b>miniml:</b> Everybody these days has a deep learning toolkit that takes forever to download, has tons\nof dependencies, and produces inscrutable models. In the spirit of simplicity and transparency, I wrote\na new package that's only 2172 bytes when compiled, has zero dependencies (not even the C standard library -- that's total bloat), \nand fits a hidden-layer-free network aka logistic regression. Meet miniml here: <a href=\"fun/miniml/\">miniml</a> .\n</li><li><b>Programming in Postscript?:</b> Found in the archives, a postscript file (<a href=\"fun/can.ps\">can.ps</a>)that solves \n<a href=\"https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem\">The Missionaries/Cannibals</a> problem,\nthen renders the solution. Send it to a printer, open it in vim, be amazed at how much time I had as an undergrad.\n</li><li><b>Revisiting Monet in Light of that Other Painter of light:</b> \nMonet is so <i>booooring</i> and tranquil. Click <a href=\"fun/monet/index.html\">here</a> to see him \nspiced up to look like Thomas Kinkade\n</li><li><b>Academic Ancestry and Erd\u00f6s Number:</b> <a href=\"fun/aancestry/index.htm\">See where I fit in!</a>\n</li><li><b>Sculpture at Berkeley:</b> <a href=\"fun/SculpturesOfBerkeley/\">3D shapes are fun and Berkeley has a lot of them</a>\n\n</li><li><b>#1 Messiest Desk:</b> My <a href=\"fun/desk/desk.jpg\">desk</a> was <a href=\"fun/desk/votes.jpg\">voted</a> #1 messiest desk in\nthe second floor of Smith Hall at Carnegie Mellon during the RI's open house for 2016.<br>\nIf a cluttered desk is an indication of a cluttered mind, then what does an empty desk indicate?\n\n\n</li><li>\n<b>Award-Winning Optimization Sheet:</b> I wrote an award-winning one page \n<a href=\"fun/review/review.pdf\">cheat-sheet</a> for Convex Optimization at CMU (10-725). Be sure to\ncheck the watermark!\n</li></ul> \n\n</div>\n\n</div>\n\n\n</body></html>"