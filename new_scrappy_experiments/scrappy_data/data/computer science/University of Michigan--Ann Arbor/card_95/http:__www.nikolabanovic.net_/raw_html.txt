"<html xmlns=\"http://www.w3.org/1999/xhtml\"><head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Nikola Banovic</title>\n<link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"screen_style.css\">\n<script async=\"\" src=\"//www.google-analytics.com/analytics.js\"></script><script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-20934741-2', 'auto');\n  ga('send', 'pageview');\n\n</script>\n<script>\n/**\n* Function that tracks a click on an outbound link in Google Analytics.\n* This function takes a valid URL string as an argument, and uses that URL string\n* as the event label.\n*/\nvar trackOutboundLink = function(url) {\n   ga('send', 'event', 'outbound', 'click', url, {'hitCallback':\n     function () {\n     document.location = url;\n     }\n   });\n}\n</script>\n<script>\n/**\n* Function that tracks a click on an outbound link in Google Analytics.\n* This function takes a valid URL string as an argument, and uses that URL string\n* as the event label.\n*/\nvar trackView = function(url) {\n   ga('send', 'event', 'inpage', 'click', url);\n}\n</script>\n<script type=\"text/javascript\">\n\n  function toggleAbstract(abstractId) {\n\t  var abstractDiv = document.getElementById(abstractId);\n\t  \n\t  if(abstractDiv.style.display == '' || abstractDiv.style.display == 'none') {\n\t\t  abstractDiv.style.display = 'block';\n\t  } else {\n\t\t  abstractDiv.style.display = 'none';\n\t  }\n\t  \n\t  return false;\n  }\n\n</script>\n</head>\n<body>\n<div id=\"Page\">\n<div id=\"ContactInformation\">\n\t<h1>Nikola Banovic</h1>\n\t<div class=\"Photo\">\n\t\t<img src=\"Nikola_Banovic_Portrait_2018.jpg\" alt=\"Portrait of Nikola Banovic\">\n\t\t<div class=\"PhotoCredit\">Photo: Joseph Xu/Michigan Engineering Communications &amp; Marketing</div>\n\t</div>\n\t<p>\n\t\t</p><div>Assistant Professor, <a href=\"http://www.eecs.umich.edu\" onclick=\"trackOutboundLink('http://www.eecs.umich.edu'); return false;\">Electrical Engineering &amp; Computer Science</a></div>\n\t\t<div>Director, Computational HCI Lab</div>\n\t<p></p>\n\t<p></p><div><a href=\"http://www.umich.edu/\" onclick=\"trackOutboundLink('http://www.umich.edu/'); return false;\">University of Michigan, Ann Arbor</a></div><p></p>\n\t<p>\n\t\t</p><div>Bob and Betty Beyster Building, Room 2630</div>\n\t\t<div>2260 Hayward</div>\n\t\t<div>Ann Arbor, <abbr title=\"Michigan\">MI</abbr> 48109, USA</div>\n\t<p></p>\n\t<p>\n\t</p><div>Email: <a href=\"mailto:nbanovic@umich.edu\" onclick=\"trackOutboundLink('mailto:nbanovic@umich.edu'); return false;\">nbanovic@umich.edu</a></div>\n\t<div>Twitter: <a href=\"https://twitter.com/nikola_banovic\" onclick=\"trackOutboundLink('https://twitter.com/nikola_banovic'); return false;\">@nikola_banovic</a></div>\n\t<p></p>\n\t<p></p><div><a href=\"nikolabanovic_cv.pdf\" onclick=\"ga('send', 'event', 'PDF', 'Download', 'nikolabanovic_cv.pdf');\">Curriculum Vitae (CV)</a></div><p></p>\n</div>\n<div id=\"About\">\n<h2>ABOUT ME</h2>\n<p>My research focuses on understanding and modeling human behavior to support innovative information technology that will change how we study and design interactive user experiences. I envision modeling the human accurately across domains as a theoretical foundation for work in Human-Computer Interaction (HCI) in which computational models provide a foundation to study, describe, and understand complex human behaviors and support optimization and evaluation of user interfaces.</p>\n<p>I work in domains of Behavior-aware User Interfaces, Behavior Data Analytics, and Computational Interaction to enable a future in which technology will automatically infer user goals, predict future user actions, describe common user behaviors, and even coach users. I create technology that automatically reasons about and acts in response to people\u2019s behavior to help them be productive, healthy, and safe.</p>\n<p>Before joining <a href=\"http://www.umich.edu/\" onclick=\"trackOutboundLink('http://www.umich.edu/'); return false;\">the University of Michigan</a>, I received my <abbr title=\"Doctor of Philosophy\">Ph.D. degree from the <a href=\"http://www.hcii.cmu.edu/\" onclick=\"trackOutboundLink('http://www.hcii.cmu.edu/'); return false;\">Human-Computer Interaction Institute (HCII)</a> at <a href=\"http://www.cmu.edu\" onclick=\"trackOutboundLink('http://www.cmu.edu/'); return false;\">Carnegie Mellon University</a>, and my <abbr title=\"Bachelor of Science\">B.Sc.</abbr> and <abbr title=\"Master of Science\">M.Sc.</abbr> degrees from the <a href=\"http://www.utoronto.ca\" onclick=\"trackOutboundLink('http://www.utoronto.ca'); return false;\">University of Toronto</a>.</abbr></p>\n<p>For more information, see my <a href=\"nikolabanovic_cv.pdf\" onclick=\"ga('send', 'event', 'PDF', 'Download', 'nikolabanovic_cv.pdf');\">Curriculum Vitae (CV)</a>.</p>\n</div>\n<div id=\"Teaching\">\n<h2>TEACHING</h2>\n<h3>Winter 2020</h3>\n<div class=\"Course\">\n\t<div class=\"CourseDetail\">\n\t\t<div class=\"Title\">EECS 598 - Human-Computer Interaction (3 credits)</div>\n\t\t<div class=\"Links\"><a href=\"https://umich.instructure.com/courses/343395\" class=\"PublicationPDF\">Course Website</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('CourseHCIW20'); trackView('course_HCI_W20'); return false;\" class=\"PublicationPDF\">Course Summary</a></div>\n\t\t<div class=\"Abstract\" id=\"CourseHCIW20\">This course will teach students principles and methods of technical Human-Computer Interaction (HCI) research. It will also include a survey of important research threads. Short individual assignments will give students exposure to existing research methods in HCI. Midterm and final exams will test the student knowledge of the topic. <strong>Prerequisites:</strong> Graduate standing; or permission from instructor.</div>\n\t</div>\n</div>\n<h3>Fall 2019</h3>\n<div class=\"Course\">\n\t<div class=\"CourseDetail\">\n\t\t<div class=\"Title\">EECS 598 - Computational Modeling in Human-Computer Interaction (3 credits)</div>\n\t\t<div class=\"Links\"><a href=\"javascript:void()\" onclick=\"toggleAbstract('CourseCompHCIF19'); trackView('course_CompHCI_F19'); return false;\" class=\"PublicationPDF\">Course Summary</a></div>\n\t\t<div class=\"Abstract\" id=\"CourseCompHCIF19\">This seminar course will review current computational approaches to describe, simulate, and predict human behavior from empirical behavior traces data. It will contrast computational modeling with other methodologies to understand human behavior and compare computational modeling with existing behavior modeling methodologies in Human-Computer Interaction (HCI). Short assignments will give students exposure to some of the cutting-edge methods, while the final project will give them an opportunity to push the boundaries of computational modeling in HCI by modeling behaviors of their choice from an existing data set. <strong>Prerequisites:</strong> Programming experience in Java, Python, MATLAB or R.</div>\n\t</div>\n</div>\n<h3>Winter 2019</h3>\n<div class=\"Course\">\n\t<div class=\"CourseDetail\">\n\t\t<div class=\"Title\">EECS 498 - Modeling Human Behavior (4 credits)</div>\n\t\t<div class=\"Links\"><a href=\"https://umich.instructure.com/courses/286096\" class=\"PublicationPDF\">Course Website</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('CourseCompHCIW19'); trackView('course_CompHCI_W18'); return false;\" class=\"PublicationPDF\">Course Summary</a></div>\n\t\t<div class=\"Abstract\" id=\"CourseCompHCIW19\">This course will teach students methods to track, collect, and express human behavior data as computational models of behavior. The course will have a particular focus on computational approaches to describe, simulate, and predict human behavior from empirical behavior traces data. It will contrast computational modeling with other methodologies to understand human behavior and compare computational modeling with existing behavior modeling methodologies in Human-Computer Interaction (HCI). Short individual assignments will give students exposure to existing modeling methods in HCI. Large, group-based final project will give students an opportunity to push the boundaries of computational modeling in HCI by modeling behaviors of their choice from an existing data set to design and implement a novel Computational Modeling system from scratch. <strong>Prerequisites:</strong> EECS 281 and (EECS 370 or EECS 376) or permission from instructor.</div>\n\t</div>\n</div>\n<h3>Fall 2018</h3>\n<div class=\"Course\">\n\t<div class=\"CourseDetail\">\n\t\t<div class=\"Title\">EECS 598 - Computational Modeling in Human-Computer Interaction (3 credits)</div>\n\t\t<div class=\"Links\"><a href=\"EECS_598_Computational_Modeling_HCI/index.html\" class=\"PublicationPDF\">Course Website</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('CourseCompHCIF18'); trackView('course_CompHCI_F18'); return false;\" class=\"PublicationPDF\">Course Summary</a></div>\n\t\t<div class=\"Abstract\" id=\"CourseCompHCIF18\">This seminar course will review current computational approaches to describe, simulate, and predict human behavior from empirical behavior traces data. It will contrast computational modeling with other methodologies to understand human behavior and compare computational modeling with existing behavior modeling methodologies in Human-Computer Interaction (HCI). Short assignments will give students exposure to some of the cutting-edge methods, while the final project will give them an opportunity to push the boundaries of computational modeling in HCI by modeling behaviors of their choice from an existing data set. <strong>Prerequisites:</strong> Programming experience in Java, Python, MATLAB or R.</div>\n\t</div>\n</div>\n</div>\n<div id=\"Research\">\n<h2>PUBLICATIONS (<a href=\"http://scholar.google.com/citations?hl=en&amp;user=8SCyF8AAAAAJ&amp;view_op=list_works&amp;sortby=pubdate\" onclick=\"trackOutboundLink('http://scholar.google.com/citations?hl=en&amp;user=8SCyF8AAAAAJ&amp;view_op=list_works&amp;sortby=pubdate'); return false;\">Google Scholar</a>)</h2>\n\n<h3>2019</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"autocorrect_thumbnail.png\">\n\t\t<div class=\"Title\">The Limits of Expert Text Entry Speed on Mobile Keyboards with Autocorrect</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Ticha Sethapakdi, Yasasvi Hari, Anind K. Dey, and Jennifer Mankoff</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">21st International Conference on Human-Computer Interaction with Mobile Devices and Services</span> (MobileHCI \u201919). ACM, New York, NY, USA, Article 15, 12 pages.</div>\n\t\t<div class=\"Links\"><a href=\"https://dl.acm.org/authorize?N684932\" onclick=\"trackOutboundLink('https://dl.acm.org/authorize?N684932'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"https://doi.org/10.1145/3338286.3340126\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3338286.3340126'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC15'); trackView('abstract_autocorrect'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC15\">Improving mobile keyboard typing speed increases in value as more tasks move to a mobile setting. Autocorrect reduces the time it takes to manually fix typing errors, which results in typing speed increase. However, recent user studies uncovered an unexplored side-effect: participants' aversion to typing errors despite autocorrect. We present a computational model of typing on keyboards with autocorrect, which enables precise study of expert typists' aversion to typing errors on such keyboards. Unlike empirical typing studies that last days, our model evaluates this phenomenon for any autocorrect accuracy in seconds. We show that typists' aversion to typing errors imposes a limit on upper bound typing speeds, even for highly accurate autocorrect. Our findings motivate future keyboard designs that reduce typists' aversion to typing errors to increase typing speeds.</div>\n\t</div>\n</div>\n\n<h3>2018</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"minutes_thumbnail.png\">\n\t\t<div class=\"Title\">In Only 3 Minutes: Perceived Exertion Limits of Smartwatch Use</div>\n\t\t<div class=\"Authors\">Rushil Khurana, <span class=\"Me\">Nikola Banovic</span>, and Kent Lyons</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2018 ACM International Symposium on Wearable Computers</span> (ISWC '18). ACM, New York, NY, USA.</div>\n\t\t<div class=\"Links\"><a href=\"https://doi.org/10.1145/3267242.3267285\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3267242.3267285'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC14'); trackView('abstract_threemins'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC14\">Glanceability and low access time are arguably the key assets of a smartwatch. Smartwatches are designed for, and excel at micro-interactions--simple tasks that only take seconds to complete. However, if a user desires to transition to a task requiring sustained usage, we show that there are additional factors that prevent possible longer usage of the smartwatch. In this paper, we conduct a study with 18 participants to empirically demonstrate that interacting with the smartwatch on the wrist leads to fatigue after only a few minutes. In our study, users performed three tasks in two different poses while using a smartwatch. We demonstrate that only after three minutes of use, the change in perceived exertion of the user was anchored as \u201csomewhat strong\u201d on the Borg CR10 survey scale. These results place an upper bound for smartwatch usage that needs to be considered in application and interaction design.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"mappingml_thumbnail.png\">\n\t\t<div class=\"Title\">Mapping Machine Learning Advances From HCI Research to Reveal Starting Places for Design Innovation</div>\n\t\t<div class=\"Authors\">Qian Yang, <span class=\"Me\">Nikola Banovic</span>, and John Zimmerman</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2018 CHI conference on Human Factors in Computing Systems</span> (CHI '18). ACM, New York, NY, USA, Paper 130, 11 pages.</div>\n\t\t<div class=\"Links\"><a href=\"https://doi.org/10.1145/3173574.3173704\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3173574.3173704'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC13'); trackView('abstract_mappingml'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC13\">HCI has become particularly interested in using machine learning (ML) to improve user experience (UX). However, some design researchers claim that there is a lack of design innovation in envisioning how ML might improve UX. We investigate this claim by analyzing 2,494 related HCI research publications. Our review confirmed a lack of research integrating UX and ML. To help span this gap, we mined our corpus to generate a topic landscape, mapping out 7 clusters of ML technical capabilities within HCI. Among them, we identified 3 under-explored clusters that design researchers can dig in and create sensitizing concepts for. To help operationalize these technical design materials, our analysis then identified value channels through which the technical capabilities can provide value for users: self, context, optimal, and utility-capability. The clusters and the value channels collectively mark starting places for envisioning new ways for ML technology to improve people\u2019s lives.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"computation_thumbnail.png\">\n\t\t<div class=\"Title\">Computational Model of Human Routine Behaviours</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Jennifer Mankoff, and Anind K. Dey</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Computational Interaction</span>, Antti Oulasvirta, Per Ola Kristensson, Xiaojun Bi, and Andrew Howes (Eds.). Oxford University Press. 22 pages.</div>\n\t\t<div class=\"Links\"><a href=\"https://global.oup.com/academic/product/computational-interaction-9780198799603?cc=us&amp;lang=en&amp;\" onclick=\"trackOutboundLink('https://global.oup.com/academic/product/computational-interaction-9780198799603?cc=us&amp;lang=en&amp;'); return false;\" class=\"PublicationPDF\">Publisher</a></div>\n\t</div>\n</div>\n<h3>2017</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"warmingup_thumbnail.png\">\n\t\t<div class=\"Title\">Warming Up to Cold Start Personalization</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span> and John Krumm</div>\n\t\t<div class=\"Venue\"><span class=\"Proceedings\">PACM Interactive, Mobile, Wearable and Ubiquitous Technology</span> (IMWUT), Volume 1, Issue 4, Article 124 (December 2017). ACM, New York, NY, USA, 13 pages.</div>\n\t\t<div class=\"Links\"><a href=\"https://dl.acm.org/authorize?N42074\" onclick=\"trackOutboundLink('https://dl.acm.org/authorize?N42074'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"https://doi.org/10.1145/3161175\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3161175'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractJ2'); trackView('abstract_coldstart'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\n\t\t<div class=\"Abstract\" id=\"AbstractJ2\">Smart agents face abandonment if they are unable to provide value to the users from the very first interaction. Existing smart agents take time to learn about new users before they can offer them personalized services. We present a method for learning personalization information about users quickly and without placing unnecessary hardship on them. Our method enables smart agents to pick which questions to ask the user when they first interact to maximize the agent\u2019s overall knowledge about the user. We demonstrate our method on two publically available US census datasets containing 172 user variables from 1,799,394 training and 1,618,489 testing users. The questions selected using our method improve the agent\u2019s accuracy when inferring information about future users, including information that they did not ask about. Our work enables smart agents that assist the user with personalized services soon after they start interacting.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"leverageroutine_thumbnail.png\">\n\t\t<div class=\"Title\">Leveraging Human Routine Models to Detect and Generate Human Behaviors</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Anqi Wang, Yanfeng Jin, Christie Chang, Julian Ramos, Anind K. Dey, and Jennifer Mankoff</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2017 CHI conference on Human Factors in Computing Systems</span> (CHI '17). ACM, New York, NY, USA, 6683-6694.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N38188\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N38188'); return false;\" class=\"PublicationPDF\">PDF</a>  <a href=\"https://doi.org/10.1145/3025453.3025571\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3025453.3025571'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC12'); trackView('abstract_leverageroutine'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC12\">An ability to detect behaviors that negatively impact people\u2019s wellbeing and show people how they can correct those behaviors could enable technology that improves people\u2019s lives. Existing supervised machine learning approaches to detect and generate such behaviors require lengthy and expensive data labeling by domain experts. In this work, we focus on the domain of routine behaviors, where we model routines as a series of frequent actions that people perform in specific situations. We present an approach that bypasses labeling each behavior instance that a person exhibits. Instead, we weakly label instances using people\u2019s demonstrated routine. We classify and generate new instances based on the probability that they belong to the routine model. We illustrate our approach on an example system that helps drivers become aware of and understand their aggressive driving behaviors. Our work enables technology that can trigger interventions and help people reflect on their behaviors when those behaviors are likely to negatively impact them.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"experttextentry_thumbnail.png\">\n\t\t<div class=\"Title\">Quantifying Aversion to Costly Typing Errors in Expert Mobile Text Entry</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Varun Rao, Abinaya Saravanan, Anind K. Dey, and Jennifer Mankoff</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2017 CHI conference on Human Factors in Computing Systems</span> (CHI '17). ACM, New York, NY, USA, 4229-4241.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N38187\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N38187'); return false;\" class=\"PublicationPDF\">PDF</a>  <a href=\"https://doi.org/10.1145/3025453.3025695\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3025453.3025506'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC11'); trackView('abstract_experttextentry'); return false;\" class=\"PublicationPDF\">Abstract</a> <strong><em>Honorable Mention Award</em></strong></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC11\">Text entry is an increasingly important activity for mobile device users. As a result, increasing text entry speed of expert typists is an important design goal for physical and soft keyboards. Mathematical models that predict text entry speed can help with keyboard design and optimization. Making typing errors when entering text is inevitable. However, current models do not consider how typists themselves reduce the risk of making typing errors (and lower error frequency) by typing more slowly. We demonstrate that users respond to costly typing errors by reducing their typing speed to minimize typing errors. We present a model that estimates the effects of risk aversion to errors on typing speed. We estimate the magnitude of this speed change, and show that disregarding the adjustments to typing speed that expert typists use to reduce typing errors leads to overly optimistic estimates of maximum errorless expert typing speeds.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"missing_thumbnail.png\">\n\t\t<div class=\"Title\">Method for Understanding Complex Human Routine Behaviors from Large Behavior Logs</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span></div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems</span> (CHI EA '17). ACM, New York, NY, USA, 254-258.</div>\n\t\t<div class=\"Links\"><a href=\"https://doi.org/10.1145/3027063.3027135\" onclick=\"trackOutboundLink('https://doi.org/10.1145/3027063.3027135'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractDC1'); trackView('abstract_dc'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractDC1\">The increasing ability to collect large amounts of human behavior data can inform technology that has the potential to help people improve their behaviors and thus improve the quality of their lives. To design and implement such technology requires understanding of those very behaviors that the technology is trying to diagnose and improve. However, existing methods to explore and make sense of human behaviors are not well suited to address the increasingly large amount of data collected in behavior logs. My research focuses on the domain of human routines where I model behaviors as sequences of actions people perform in specific situations. I leverage those computational models of routines together with different visualization tools to aid researchers and domain experts in exploring, making sense of, and generating new insights about human behavior in a principled way. My research informs the design of technology that helps people be productive, healthy, and safe.</div>\n\t</div>\n</div>\n<h3>2016</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"routines_thumbnail.png\">\n\t\t<div class=\"Title\">Modeling and Understanding Human Routine Behavior</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Tofi Buzali, Fanny Chevalier, Jennifer Mankoff, and Anind K. Dey</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2016 CHI conference on Human Factors in Computing Systems</span> (CHI '16). ACM, New York, NY, USA, 248-260.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N03134\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N03134'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://dx.doi.org/10.1145/2858036.2858557\" onclick=\"trackOutboundLink('http://dx.doi.org/10.1145/2858036.2858557'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC10'); trackView('abstract_routines'); return false;\" class=\"PublicationPDF\">Abstract</a> <strong><em>Honorable Mention Award</em></strong></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC10\">Human routines are blueprints of behavior, which allow people to accomplish purposeful repetitive tasks at many levels, ranging from the structure of their day to how they drive through an intersection. People express their routines through actions that they perform in the particular situations that triggered those actions. An ability to model routines and understand the situations in which they are likely to occur could allow technology to help people improve their bad habits, inexpert behavior, and other suboptimal routines. However, existing routine models do not capture the causal relationships between situations and actions that describe routines. Our main contribution is the insight that byproducts of an existing activity prediction algorithm can be used to model those causal relationships in routines. We apply this algorithm on two example datasets, and show that the modeled routines are meaningful\u2014that they are predictive of people\u2019s actions and that the modeled causal relationships provide insights about the routines that match findings from previous research. Our approach offers a generalizable solution to model and reason about routines.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"missing_thumbnail.png\">\n\t\t<div class=\"Title\">To Replicate or Not to Replicate?</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span></div>\n\t\t<div class=\"Venue\"><span class=\"Proceedings\">GetMobile: Mobile Computing and Communications</span> 19, 4 (March 2016), 23-27.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N40121\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N40121'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://dx.doi.org/10.1145/2904337.2904346\" onclick=\"trackOutboundLink('http://dx.doi.org/10.1145/2904337.2904346'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractA1'); trackView('abstract_replicate'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractA1\">To replicate or not to replicate experiments and studies seems to be a burning question in the Human-Computer Interaction (HCI) community. This question is equally relevant to the field of mobile computing, where researchers face unique challenges when attempting to replicate studies of mobile device usage in the field. Some of those challenges are inherent in conducting such studies to understand complex socio-technical systems, which involve different technologies and diverse populations of users from different social, economic, and cultural backgrounds. However, those same challenges point to a great opportunity to expand our knowledge of how people use and interact with their mobile devices and understand how those interactions evolve over time.</div>\n\t</div>\n</div>\n<h3>2015</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"mobile_challenges_thumbnail.jpg\">\n\t\t<div class=\"Title\">Understanding the challenges of mobile phone usage data</div>\n\t\t<div class=\"Authors\">Karen Church, Denzil Ferreira, <span class=\"Me\">Nikola Banovic</span>, Kent Lyons</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 17th international conference on Human-computer interaction with mobile devices and services</span> (MobileHCI '15). ACM, New York, NY, USA, 504-514.</div>\n\t\t<div class=\"Links\"><a href=\"http://dx.doi.org/10.1145/2785830.2785891\" onclick=\"trackOutboundLink('http://dx.doi.org/10.1145/2785830.2785891'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC9'); trackView('abstract_mobile_challenges'); return false;\" class=\"PublicationPDF\">Abstract</a> <strong><em>Honorable Mention Award</em></strong></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC9\">Driven by curiosity and our own three diverse smartphone application usage datasets, we sought to unpack the nuances of mobile device use by revisiting two recent Mobile HCI studies. Our goal was to add to our broader understanding of smartphone usage by investigating if differences in mobile device usage occurred not only across our three datasets, but also in relation to prior work. We found differences in the top-10 apps in each dataset, in the durations and types of interactions as well as in micro-usage patterns. However, it proved very challenging to attribute such differences to a specific factor or set of factors: was it the time frame in which the studies were executed? The recruitment procedure? The experimental method? Using our somewhat troubled analysis, we discuss the challenges and issues of conducting mobile research of this nature and reflect on caveats related to the replicability and generalizability of such work.</div>\n\t</div>\n</div>\n<h3>2014</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"proactive_thumbnail.jpg\">\n\t\t<div class=\"Title\">ProactiveTasks: the short of mobile device use sessions</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Christina Brant, Jennifer Mankoff, and Anind K. Dey</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 16th international conference on Human-computer interaction with mobile devices &amp; services</span> (MobileHCI '14). ACM, New York, NY, USA, 243-252.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81233\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81233'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2628363.2628380\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2628363.2628380'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC8'); trackView('abstract_proactive'); return false;\" class=\"PublicationPDF\">Abstract</a> <strong><em>Best Paper Award</em></strong></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC8\">Mobile devices have become powerful ultra-portable personal computers supporting not only communication but also running a variety of complex, interactive applications. Because of the unique characteristics of mobile interaction, a better understanding of the time duration and context of mobile device uses could help to improve and streamline the user experience. In this paper, we first explore the anatomy of mobile device use and propose a classification of use based on duration and interaction type: glance, review, and engage. We then focus our investigation on short review interactions and identify opportunities for streamlining these mobile device uses through proactively suggesting short tasks to the user that go beyond simple application notifications. We evaluate the concept through a user evaluation of an interactive lock screen prototype, called ProactiveTasks. We use the findings from our study to create and explore the design space for proactively presenting tasks to the users. Our findings underline the need for a more nuanced set of interactions that support short mobile device uses, in particular review sessions.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"indoor_thumbnail.jpg\">\n\t\t<div class=\"Title\">Indoor-ALPS: an adaptive indoor location prediction system</div>\n\t\t<div class=\"Authors\">Christian Koehler, <span class=\"Me\">Nikola Banovic</span>, Ian Oakley, Jennifer Mankoff, and Anind K. Dey</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</span> (UbiComp '14). ACM, New York, NY, USA, 171-181.</div>\n\t\t<div class=\"Links\"><a href=\"http://doi.acm.org/10.1145/2632048.2632069\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2632048.2632069'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC7'); trackView('abstract_indoor'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC7\">Location prediction enables us to use a person's mobility history to realize various applications such as efficient temperature control, opportunistic meeting support, and automated receptionists. Indoor location prediction is a challenging problem, particularly due to a high density of possible locations and short transition distances between these locations. In this paper we present Indoor-ALPS, an Adaptive Indoor Location Prediction System that uses temporal-spatial features to create individual daily models for the prediction of when a user will leave their current location (transition time) and the next location she will transition to. We tested Indoor-ALPS on the Augsburg Indoor Location Tracking Benchmark and compared our approach to the best performing temporal-spatial mobility prediction algorithm, Prediction by Partial Match (PPM). Our results show that Indoor-ALPS improves the temporal-spatial prediction accuracy over PPM for look-aheads up to 90 minutes by 6.2%, and for up to 30 minute look-aheads by 10.7%. These results demonstrate that Indoor-ALPS can be used to support a wide variety of indoor mobility prediction-based applications.</div>\n\t</div>\n</div>\n<h3>2013</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"uncovering_thumbnail.jpg\">\n\t\t<div class=\"Title\">Uncovering information needs for independent spatial learning for users who are visually impaired</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Rachel L. Franz, Khai N. Truong, Jennifer Mankoff, and Anind K. Dey</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 15th international ACM SIGACCESS conference on Computers and accessibility</span> (ASSETS '13). ACM, New York, NY, USA, Article 24, 8 pages.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81234\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81234'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2513383.2513445\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2513383.2513445'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC6'); trackView('abstract_uncovering'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC6\">Sighted individuals often develop significant knowledge about their environment through what they can visually observe. In contrast, individuals who are visually impaired mostly acquire such knowledge about their environment through information that is explicitly related to them. This paper examines the practices that visually impaired individuals use to learn about their environments and the associated challenges. In the first of our two studies, we uncover four types of information needed to master and navigate the environment. We detail how individuals\u2019 context impacts their ability to learn this information, and outline requirements for independent spatial learning. In a second study, we explore how individuals learn about places and activities in their environment. Our findings show that users not only learn information to satisfy their immediate needs, but also to enable future opportunities \u2013 something existing technologies do not fully support. From these findings, we discuss future research and design opportunities to assist the visually impaired in independent spatial learning.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"error_thumbnail.jpg\">\n\t\t<div class=\"Title\">The effect of time-based cost of error in target-directed pointing tasks</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Tovi Grossman, and George Fitzmaurice</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2013 ACM annual conference on Human Factors in Computing Systems</span> (CHI '13). ACM, New York, NY, USA, 1373-1382.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81245\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81245'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2466110.2466181\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2466110.2466181'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC5'); trackView('abstract_cost'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC5\">One of the fundamental operations in today\u2019s user interfaces is pointing to targets, such as menus, buttons, and text. Making an error when selecting those targets in real-life user interfaces often results in some cost to the user. However, the existing target-directed pointing models do not consider the cost of error when predicting task completion time. In this paper, we present a model based on expected value theory that predicts the impact of the error cost on the user\u2019s completion time for target-directed pointing tasks. We then present a target-directed pointing user study, which results show that time-based costs of error significantly impact the user\u2019s performance. Our results also show that users perform according to an expected completion time utility function and that optimal performance computed using our model gives good prediction of the observed task completion times.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"escape_thumbnail.jpg\">\n\t\t<div class=\"Title\">Escape-Keyboard: a sight-free one-handed text entry method for mobile touch-screen devices</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Koji Yatani, and Khai N. Truong</div>\n\t\t<div class=\"Venue\"><span class=\"Proceedings\">International Journal of Mobile Human Computer Interaction (IJMHCI)</span>, Volume 5, Issue 3, (July 2013) 42-61.</div>\n\t\t<div class=\"Links\"><a href=\"http://www.igi-global.com/article/escape-keyboard/81286\" onclick=\"trackOutboundLink('http://www.igi-global.com/article/escape-keyboard/81286'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractJ1'); trackView('abstract_escape'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractJ1\">Mobile text entry methods traditionally have been designed with the assumption that users can devote full visual and mental attention on the device, though this is not always possible. We present our iterative design and evaluation of Escape-Keyboard, a sight-free text entry method for mobile touch-screen devices. Escape-Keyboard allows the user to type letters with one hand by pressing the thumb on different areas of the screen and performing a flick gesture. We then examine the performance of Escape-Keyboard in a study that included 16 sessions in which participants typed in sighted and sight-free conditions. Qualitative results from this study highlight the importance of reducing the mental load with using Escape-Keyboard to improve user performance over time. We thus also explore features to mitigate this learnability issue. Finally, we investigate the upper bound on the sight-free performance with Escape-Keyboard by performing theoretical analysis of the expert peak performance.</div>\n\t</div>\n</div>\n<h3>2012</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"waken_thumbnail.jpg\">\n\t\t<div class=\"Title\">Waken: reverse engineering usage information and interface structure from software videos</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Tovi Grossman, Justin Matejka, and George Fitzmaurice</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 25th annual ACM symposium on User interface software and technology</span> (UIST '12). ACM, New York, NY, USA, 83-92.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81246\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81246'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2380116.2380129\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2380116.2380129'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC4'); trackView('abstract_waken'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC4\">We present Waken, an application-independent system that recognizes UI components and activities from screen captured videos, without any prior knowledge of that application. Waken can identify the cursors, icons, menus, and tooltips that an application contains, and when those items are used. Waken uses frame differencing to identify occurrences of behaviors that are common across graphical user interfaces. Candidate templates are built, and then other occurrences of those templates are identified using a multi-phase algorithm. An evaluation demonstrates that the system can successfully reconstruct many aspects of a UI without any prior application-dependant knowledge. To showcase the design opportunities that are introduced by having this additional meta-data, we present the Waken Video Player, which allows users to directly interact with UI components that are displayed in the video.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"triggers_thumbnail.jpg\">\n\t\t<div class=\"Title\">Triggering triggers and burying barriers to customizing software</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Fanny Chevalier, Tovi Grossman, and George Fitzmaurice</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems</span> (CHI '12). ACM, New York, NY, USA, 2717-2726.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81247\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81247'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2207676.2208666\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2207676.2208666'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC3'); trackView('abstract_triggering'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC3\">General-purpose software applications are usually not tailored for a specific user with specific tasks, strategies or preferences. In order to achieve optimal performance with such applications, users typically need to transition to an alternative efficient behavior. Often, features of such alternative behaviors are not initially accessible and first need to be customized. However, few research works formally study and empirically measure what drives a user to customize. In this paper, we describe the challenges involved in empirically studying customization behaviors, and propose a methodology for formally measuring the impact of potential customization factors. We then demonstrate this methodology by studying the impact of different customization factors on customization behaviors. Our results show that increasing exposure and awareness of customization features, and adding social influence can significantly affect the user's customization behavior.</div>\n\t</div>\n</div>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"space_thumbnail.jpg\">\n\t\t<div class=\"Title\">SpaceSense: representing geographical information to visually impaired people using spatial tactile feedback</div>\n\t\t<div class=\"Authors\">Koji Yatani, <span class=\"Me\">Nikola Banovic</span>, and Khai Truong</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems</span> (CHI '12). ACM, New York, NY, USA, 415-424.</div>\n\t\t<div class=\"Links\"><a href=\"http://doi.acm.org/10.1145/2207676.2207734\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2207676.2207734'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC2'); trackView('abstract_space'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC2\">Learning an environment can be challenging for people with visual impairments. Braille maps allow their users to understand the spatial relationship between a set of places. However, physical Braille maps are often costly, may not always cover an area of interest with sufficient detail, and might not present up-to-date information. We built a handheld system for representing geographical information called SpaceSense, which includes custom spatial tactile feedback hardware-multiple vibration motors attached to different locations on a mobile touch-screen device. It offers high-level information about the distance and direction towards a destination and bookmarked places through vibrotactile feedback to help the user maintain the spatial relationships between these points. SpaceSense also adapts a summarization technique for online user reviews of public and commercial Venuees. Our user study shows that participants could build and maintain the spatial relationships between places on a map more accurately with SpaceSense compared to a system without spatial tactile feedback. They pointed specifically to having spatial tactile feedback as the contributing factor in successfully building and maintaining their mental map.</div>\n\t</div>\n</div>\n<h3>2011</h3>\n<div class=\"Publication\">\n\t<div class=\"PublicationDetail\">\n\t\t<img src=\"design_thumbnail.jpg\">\n\t\t<div class=\"Title\">Design of unimanual multi-finger pie menu interaction</div>\n\t\t<div class=\"Authors\"><span class=\"Me\">Nikola Banovic</span>, Frank Chun Yat Li, David Dearman, Koji Yatani, and Khai N. Truong</div>\n\t\t<div class=\"Venue\">In <span class=\"Proceedings\">Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces</span> (ITS '11). ACM, New York, NY, USA, 120-129.</div>\n\t\t<div class=\"Links\"><a href=\"http://dl.acm.org/authorize?N81248\" onclick=\"trackOutboundLink('http://dl.acm.org/authorize?N81248'); return false;\" class=\"PublicationPDF\">PDF</a> <a href=\"http://doi.acm.org/10.1145/2076354.2076378\" onclick=\"trackOutboundLink('http://doi.acm.org/10.1145/2076354.2076378'); return false;\" class=\"PublicationPDF\">Publisher</a> <a href=\"javascript:void()\" onclick=\"toggleAbstract('AbstractC1'); trackView('abstract_design'); return false;\" class=\"PublicationPDF\">Abstract</a></div>\n\t\t<div class=\"Abstract\" id=\"AbstractC1\">Context menus, most commonly the right click menu, are a traditional method of interaction when using a keyboard and mouse. Context menus make a subset of commands in the application quickly available to the user. However, on tabletop touchscreen computers, context menus have all but disappeared. In this paper, we investigate how to design context menus for efficient unimanual multi-touch use. We investigate the limitations of the arm, wrist, and fingers and how it relates to human performance of multi-targets selection tasks on multi-touch surface. We show that selecting targets with multiple fingers simultaneously improves the performance of target selection compared to traditional single finger selection, but also increases errors. Informed by these results, we present our own context menu design for horizontal tabletop surfaces.</div>\n\t</div>\n</div>\n<div class=\"Publication\"></div>\n</div>\n</div>\n<div id=\"Footer\">\u00a9 Nikola Banovic - Last updated January 6, 2020</div>\n\n\n</body></html>"