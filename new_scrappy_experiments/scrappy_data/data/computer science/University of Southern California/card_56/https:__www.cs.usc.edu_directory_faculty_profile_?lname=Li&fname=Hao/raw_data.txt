{"email": ["CS@SC"], "image": ["https://www.cs.usc.edu/wp-content/uploads/2017/03/Footer-Map.png", "https://viterbi.usc.edu/directory/images/be19941c9e5916f8be5a6975c4475597.jpg", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/USC-Shield.png", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/USC-Viterbi.png", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/Small-Use-Shield_BlackOnTrans.png", "https://www.cs.usc.edu/wp-content/uploads/2020/01/Informal_Viterbi_GoldOnBlack-600x111-1.jpg"], "research_blurb": ["<hr/>I am a tenure-track assistant professor, fashion m\u00f6del, and zombie at the Computer Science Department of the University of Southern California (USC) in Los Angeles and CEO of Pinscreen, a well-funded startup bringing AR to mobile communication. Here is our USC website and my official song! Before my faculty appointment, I was a research lead at Industrial Light &amp; Magic (ILM) where I developed the next generation real-time performance capture technology for visual effects. Prior to joining the force, I spent an exciting year as a postdoc at Columbia and Princeton Universities. I worked on geometric capture of human performances with Eitan Grinspun and Szymon Rusinkiewicz who helped me bridge the gap between data capture and physical simulation. Right after obtaining my PhD from ETH Zurich, I spent a few months at EPFL, visiting my PhD advisor Mark Pauly. During grad school, I spent a wonderful summer atILM and another one at Stanford University visiting Leo Guibas. I got my Master\u2019s degree in CS from theUniversity of Karlsruhe in Germany and went to a French high school in a boonie town called Saarbr\u00fccken.<br/> <br/> My algorithms on dynamic shape reconstruction and non-rigid registration are widely deployed in the industry, ranging from leading visual effects studios to defense projects and manufacturers of state-of-the-art radiation therapy systems. Hey, some of my stuff even works in industry! I received the Google Faculty Research Award, the Okawa Foundation Research Grant, and the Andrew and Erna Viterbi Early Career Chair in 2015. I\u2019ve been named one of the world\u2019s top 35 innovator under 35 by MIT Technology review in 2013 and NextGen 10: Innovators under 40 by C-Suite Quaterly in 2014. I am ranked #1 in 2016, on the Top 10 Leaderboard of Computer Graphics research for the past five years by Microsoft Academic. My research is also featured in the March 2014 issue of Wired magazine. I was awarded the SNF fellowship for prospective researchers in 2011 and Best Paper Award at SCA 2009. Our 3D hair scanning technology is deployed at Weta Digital, where I also worked on the 3D facial tracking for animating the digital Paul Walkerin Furious 7. Oh! That gives me an Erd\u00f6s-Bacon number of 5. In collaboration with Artec Group, we developed Shapify.me, a free software that allows anyone to create their own 3D miniature from home. I\u2019m also the creator of the dynamic geometry processing tool, BeNTO3D, and the original co-author of the Kinect-based facial performance capture software, faceshift. Together with Oculus &amp; Facebook, we prototyped the world\u2019s first facial performance sensing virtual reality HMD to enable social interactions in cyberspace. Here is my interview on this technology from Oculus Connect 2. I\u2019m member of the New York Academy of Sciences, ACM SIGGRAPH, and Eurographics Association.<br/><br/><div class=\"research-piece\"><h4>Research Summary</h4><hr/>With 3D imaging being democratized, I develop algorithms that enable unobtrusive digitization, processing, and understanding of dynamic subjects from the physical world, such as humans and animals.<br/> By the end of this decade, our homes will be equipped with 3D sensors that digitally monitor our actions, habits, and health. In contrast to video sensors, 3D cameras directly capture spatial representations of a scene and facilitate many unsolved computer vision problems. I develop efficient algorithms and computational models for unobtrusive 3D digitization of dynamic shapes. My goal is to capture human body performances and facial expressions at unprecedented resolution and in unconstrained settings. These ad- vances will help machines understand our movements and intentions, revolutionizing the way we interact with computers through natural gestures, and developing new forms of live communication through com- pelling virtual avatars. My research lies at the intersection of computer graphics, vision, and machine learning, with immediate impact in entertainment (film/game production), health (unobtrusive diagnostics), and social science (behavioral analytics).<br/><br/><div class=\"awards-piece\">"]}