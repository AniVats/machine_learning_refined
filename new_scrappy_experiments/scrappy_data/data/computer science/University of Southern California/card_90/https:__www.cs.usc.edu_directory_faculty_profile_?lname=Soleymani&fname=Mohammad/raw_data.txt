{"email": ["CS@SC"], "image": ["https://www.cs.usc.edu/wp-content/uploads/2017/03/Footer-Map.png", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/USC-Shield.png", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/USC-Viterbi.png", "https://www.cs.usc.edu/wp-content/themes/viterbischool_4_0_2/assets/images/Small-Use-Shield_BlackOnTrans.png", "https://www.cs.usc.edu/wp-content/uploads/2020/01/Informal_Viterbi_GoldOnBlack-600x111-1.jpg", "https://viterbi.usc.edu/directory/images/7c7d53bdc57a14365711f22c71acf142.jpg"], "research_blurb": ["<hr/>Mohammad Soleymani is a research assistant professor with the USC Institute for Creative Technologies. He received his PhD in computer science from the University of Geneva in 2011. From 2012 to 2014, he was a Marie Curie fellow at Imperial College London. Prior to joining ICT, he was a research scientist at the Swiss Center for Affective Sciences, University of Geneva. His main line of research involves developing automatic emotion recognition and behavior understanding methods using physiological signals and facial expressions. He is also interested in understanding subjective attributes in multimedia content, e.g., predicting whether an image is interesting from its pixels or automatic recognition of music mood from acoustic content. He is a recipient of the Swiss National Science Foundation Ambizione grant and the EU Marie Curie fellowship. He has served on multiple conference organization committees and editorial roles, most notably as associate editor for the IEEE Transactions on Affective Computing and technical program chair for ACM ICMI 2018 and ACII 2017. He is one of the founding organizers of the MediaEval multimedia retrieval benchmarking campaign and the president elect for the Association for the Advancement of Affective Computing (AAAC). <br/><br/><div class=\"research-piece\"><h4>Research Summary</h4><hr/>I am generally interested in the automatic perception of human behavior and emotions which involves  nonverbal and verbal behavior understanding, physiological signal analysis and multimodal machine learning. My research at ICT focuses on applications of nonverbal behavior sensing in dyadic interactions to support mental health applications and human-machine interaction. Together with my team, we work on emotion recognition, mental health assessment, multimodal sentiment analysis and automatic behavior generation for virtual agents, among others. I also direct the development of ICT MultiSense framework, a multimodal behavior sensing tool that can track facial expression, head gestures, voice activity and voice quality features.<br/><br/><div class=\"awards-piece\"><div class=\"profileModuleRight\"><div class=\"contactInformation\">"]}