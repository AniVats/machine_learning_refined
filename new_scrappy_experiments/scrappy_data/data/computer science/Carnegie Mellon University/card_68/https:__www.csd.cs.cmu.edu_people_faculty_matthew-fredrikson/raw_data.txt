{"email": ["mfredrik@andrew.cmu.edu", "csd-marcom-info@cmu.edu?subject=Website%20issue%20or%20question"], "image": ["https://www.csd.cs.cmu.edu/sites/default/files/styles/directory_photos/public/mfredrik.jpg?itok=gX8RHryQ", "/sites/default/files/media-icons/default/email.png", "/sites/default/files/media-icons/default/facebook.png", "/sites/default/files/media-icons/default/twitter.png", "/sites/default/files/media-icons/default/linkedin.png"], "research_blurb": ["\nMy research is directed at understanding fundamental security and privacy issues that lead to failures in real systems. Some of the key outstanding challenges in this area lie in figuring out why promising theoretical approaches oftentimes do not translate into effective defenses. Much of my work is concerned with developing formal analysis techniques that provide insight into the problems that might exist in a system, building countermeasures that give provable guarantees, and measuring the effectiveness of these solutions in real settings.\n\nMost of my current research focuses on issues of privacy and data confidentiality. To an even greater extent than with other security issues, our scientific understanding of this area lags far behind the need for rigorous defensive strategies. I believe that in order to reason effectively about privacy in software systems, we need ways to characterize and limit  Predictive models generated by machine learning methods are used extensively in modern applications. They allow analysts to refine complex data sources into succinct programs that produce valuable information about underlying trends and patterns. A large body of previous research examines the risks that arise when these data sources contain sensitive or proprietary information, and are leaked either in their original form or after \"anonymization\". Much less well-understood are the risks that arise when machine learning models trained over these data sources are made available through applications. Although recent frameworks like differential privacy have started to shed light on this issue, it is often unclear how the underlying mathematical guarantees offered by these frameworks impact specific, tangible privacy concerns in real applications. The goal of this work is to develop a precise characterization of this threat, so that we can identify troublesome applications before they are published, as well as understand how to design and apply countermeasures that prevent it."]}