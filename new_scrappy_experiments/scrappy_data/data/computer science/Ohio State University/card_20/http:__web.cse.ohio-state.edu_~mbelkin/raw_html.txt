"<html class=\"isJS\"><head>\n<title>Web page of Mikhail Belkin, Machine Learning and Geometry</title>\n<style type=\"text/css\">\n\na:link {color: darkred} \na:visited {color: darkblue}\na:hover {color: #FF00FF}\n\na.two:link {color: red; text-decoration: none}\na.two:visited {color: red;text-decoration: none}\na.two:hover {color: #FF00FF}\n</style>\n<style type=\"text/css\">\n \n.togList\n{\n\n}\n\n.togList dt\n{\ncursor: pointer; cursor: hand;\n}\n\n.togList dt span\n{\n\ncolor: blue;\n}\n\n.togList dd\n{\nwidth: 700px;\npadding-bottom: 15px;\n}\n \t\t\t\t\t\t\t\t\nhtml.isJS .togList dd\n{\ndisplay: none;\n}\n\n</style>\n<script type=\"text/javascript\">\n\n/* Only set closed if JS-enabled */\ndocument.getElementsByTagName('html')[0].className = 'isJS';\n\nfunction tog(dt)\n{\nvar display, dd=dt;\n/* get dd */\ndo{ dd = dd.nextSibling } while(dd.tagName!='DD');\ntoOpen =!dd.style.display;\ndd.style.display = toOpen? 'block':''\ndt.getElementsByTagName('span')[0].innerhtml = toOpen? '-':'+' ;\n}\n</script>\n</head>\n\n\n\n \n\n<body style=\"color:black; background-color:#FFFFFF\" alink=\"#ff3366\" link=\"#cc0000\" vlink=\"#660000\">\n\n<p>\n&nbsp;\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"100%\">\n\n  <tbody>\n\n    <tr>\n\n      <td align=\"left\"><img src=\"mb.jpg\" border=\"1\" height=\"250\"> </td>\n\n      <td align=\"left\" width=\"870%\"><font face=\"helvetica, ariel, 'sans serif'\" size=\"3\"> <font size=\"+1\"> <b>Mikhail\nBelkin</b><br>\n\n      </font><br>\n\n      </font> <font face=\"helvetica, Geneva, ariel\"> <font size=\"-1\"><b>\nProfessor<br> <a href=\"http://www.cse.ohio-state.edu\">Department of Computer Science and Engineering</a><br>\n<a href=\"http://www.stat.osu.edu/\"> Department of Statistics</a> (courtesy) <br>\n<a href=\"http://www.cog.ohio-state.edu/\"> Center for Cognitive Science </a></b>\n<br>\n<br>\n\nThe Ohio State University,<br> Computer Science and Engineering,<br>\n2015 Neil Avenue, Dreese Labs 597. <br>\n\nColumbus, OH 43210 [<a href=\"http://www.osu.edu/map/building.php?building=279\">map</a>]<br>\n\nphone: 614-292-5841<br>\n\nemail: mbelkin at cse.ohio-state.edu </font> </font>\n<br><br>[<a href=\"https://scholar.google.com/citations?user=Iwd9DdkAAAAJ&amp;hl=en\">Papers in Google Scholar</a>] \n [<a href=\"#ref1\">Selected/recent papers</a>] [<a href=\"cv.pdf\">CV</a>] [<a href=\"talks.html\">Talks</a>]</td>\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n    </tr>\n  </tbody>\n</table>\n\n\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"70%\">\n<tbody><tr>\n<td style=\"text-align: left; width: 20%;\"> \n<b> <font color=\"#993300\"> Research\ninterests and directions. \n  </font></b> \n<hr>\n<p>  I am  interested in  questions concerning computation, statistics an optimization in \nMachine Learning, particularly for high dimensional data. Recently much of my research has been  focused on the fundamental understanding of modern ML and deep learning, particularly the phenomenon of <b>interpolation</b>. \n\n</p><p>\nIn the past I have worked on a range of topics \nincluding manifold and semi-supervised learning introducing <a href=\"https://www.mitpressjournals.org/doi/10.1162/089976603321780317\">Laplacian Eigenmaps</a>, \na method for dimensionality \nreduction, data representation and visualization based on the geometry of the heat equation,   <a href=\"http://web.cse.ohio-state.edu/~belkin.8/papers/SSL_ML_04.pdf\"> Graph Regularization</a> and  \n<a href=\"http://www.jmlr.org/papers/v7/belkin06a.html\">Manifold Regularization</a> for semi-supervised learning. Other work includes \n<a href=\"https://projecteuclid.org/euclid.aos/1205420511\">spectral clustering</a>,\n<a href=\"https://epubs.siam.org/doi/10.1137/13090818X\">learning Gaussian mixture models</a>, <a href=\"https://arxiv.org/abs/1706.06516\">Stochastic Block Models</a> and   \n<a href=\"https://epubs.siam.org/doi/abs/10.1137/17M1122013?journalCode=smjcat\">generalizations of Independent Components Analysis</a> among others.  \n\n</p><p> In recent years the practice of deep learning has presented a number of foundational challenges to statistical learning theory, necessitating revisiting some of the foundations of the subject. There are two key challenges of modern machine  learning I am particularly interested in:\n</p><ul>\n<li> First is the question of <b>generalization</b>. \nWhy  do classifiers with zero training error (we call this interpolation) \nstill perform well on the test set, even <a href=\"http://proceedings.mlr.press/v80/belkin18a.html\">\nfor very noisy data</a>, in contradiction to the accepted statistical wisdom on overfitting? \n</li><li> The second  challenge is understanding <b>optimization</b>. Why do methods such as Stochastic Gradient Descent (SGD) perform so well for modern non-convex deep architectures? \n</li></ul>\n\nWe are closing on satisfactory answers to both of these questions. \n<p>\nWhile much theoretical and empirical work needs to be done, the outline of the new generalization theory is becoming clear. \nThe classical analyses  (and their resulting practical recommendations for model selection) rely on bounding the difference between the  <i>training</i> loss and the <i>test</i> loss through the tools of the empirical process theory, such as VC or margin bounds. \nHowever, in the modern interpolating setting the empirical loss is identically zero and yields no information about the  expected loss. \nThus we do not expect the classical bounds to be informative in this regime (except for the special case of zero test loss).  \nInstead, we need to analyze algorithms that maximize or ensure functional <i>smoothness</i> subject to the interpolation constraints, a form of the Occam's razor.\nAs we <a href=\"https://papers.nips.cc/paper/7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate\">show</a>, \ninterpolating methods can indeed be statistically optimal or near-optimal (particularly in high dimension). As also discussed in that work, the same point of view sheds light on the \nubiquity of adversarial examples in modern learning, a topic of significant recent interest. \nFurthermore, our very recent <a href=\"https://arxiv.org/abs/1812.11118\">work</a> shows how interpolation and the \ntextbook U-shaped bias-variance trade-off  can be reconciled within a single <i>\"double descent\"</i> risk curve,  \nthus resolving the tension between the classical generalization theory and the modern practice. \nSee our recent <a href=\"https://arxiv.org/abs/1903.07571\"> paper</a> for a detailed mathematical analysis of double  descent for two simple linear regression models.\n\n\n</p><p>\nOnce we accept that interpolation plays nice with generalization, \noptimization is easier to address. Indeed, it is easy to <a href=\"https://arxiv.org/abs/1811.02564\">show</a> that \ninterpolation results in  exponential covergence for SGD with a constant learning rate, \neven in broad non-convex settings (under the Polyak-Lojasiewicz (PL) condition).\nAs a demonstration of the power of this point of view, theoretical convergence bounds for SGD <a href=\"http://proceedings.mlr.press/v80/ma18a.html\"> in the interpolated convex setting</a> can be directly applied \nto construct state-of-the-art kernel machines adaptive to parallel modern hardware, such as GPU (<a href=\"https://arxiv.org/abs/1806.06144\">EigenPro</a>). \nFurthermore, a new version of accelerated SGD (<a href=\"https://arxiv.org/abs/1810.13395\">MaSS</a>) based on the theoretical analysis of \nthe interpolation setting can be applied to a wide range of deep networks with excellent optimization and generalization performance. \n\n\n</p><p>\nHere is  my recent <a href=\"https://youtu.be/JS-Bl36aVPs\">talk</a> at the Center for Brains, Minds and Machines at MIT,  outlining the new perspective on modern machine learning.\n\n\n\n\n</p></td>\n</tr>\n\n</tbody></table>\n\n\n\n\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"70%\">\n<tbody><tr>\n<td style=\"text-align: left; width: 30%;\"> \n<a name=\"ref1\"></a>\n<b> <font color=\"#993300\"> Selected and recent papers. Complete publication list in  <a href=\"https://scholar.google.com/citations?user=Iwd9DdkAAAAJ&amp;hl=en\">Google Scholar</a>. </font>    </b> \n<hr>\n\n\n<ul>\n<li>\n        <b> Reconciling modern machine learning practice and the bias-variance trade-off </b>\n       [<a href=\"https://www.pnas.org/content/116/32/15849.short\">PNAS</a>, <a href=\"https://arxiv.org/abs/1812.11118\">arxiv</a>]<br>\n Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal, PNAS, 2019, 116 (32).\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   The question of generalization in machine learning---how algorithms are able to learn predictors from a training sample to make accurate predictions out-of-sample---is revisited in light of the recent breakthroughs in modern machine learning technology.\nThe classical approach to understanding generalization is based on bias-variance trade-offs, where model complexity is carefully calibrated so that the fit on the training sample reflects performance out-of-sample.\nHowever, it is now common practice to fit highly complex models like deep neural networks to data with (nearly) zero training error, and yet these interpolating predictors are observed to have good out-of-sample accuracy even for noisy data.\nHow can the classical understanding of generalization be reconciled with these observations from modern machine learning practice?\nIn this paper, we bridge the two regimes by exhibiting a new \"double descent\" risk curve that extends the traditional U-shaped bias-variance curve beyond the point of interpolation.\nSpecifically, the curve shows that as soon as the model complexity is high enough to achieve interpolation on the training sample---a point that we call the \"interpolation threshold\"---the risk of suitably chosen interpolating predictors from these models can, in fact, be decreasing as the model complexity increases, often below the risk achieved using non-interpolating models.\nThe double descent risk curve is demonstrated for a broad range of models, including neural networks and random forests, and a mechanism for producing this behavior is posited.           </dd> \n        </dl>\n \n</li> \n<li>\n        <b>Two models of double descent for weak features</b>\n       [<a href=\"https://arxiv.org/abs/1903.07571\">arxiv</a>]<br>\nMikhail Belkin, Daniel Hsu, Ji Xu. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd> \nThe \"double descent\" risk curve was recently proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features p is close to the sample size n, but also that the risk decreases towards its minimum as p increases beyond n. \nThis behavior is contrasted with that of \"prescient\" models that select features in an a priori optimal order. \t\t  \n         </dd> \n        </dl>\n \n</li> \n\n\n\n\n<li>\n        <b> On exponential convergence of SGD in non-convex over-parametrized learning </b>\n       [<a href=\"https://arxiv.org/abs/1811.02564\">arxiv</a>]<br>\n Raef Bassily, Mikhail Belkin, Siyuan Ma. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   Large over-parametrized models learned via stochastic gradient descent (SGD) methods have become a key element in modern machine learning. Although SGD methods are very effective in practice, most theoretical analyses of SGD suggest slower convergence than what is empirically observed. In our recent work [8] we analyzed how interpolation, common in modern over-parametrized learning, results in exponential convergence of SGD with constant step size for convex loss functions. In this note, we extend those results to a much broader non-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A number of important non-convex problems in machine learning, including some classes of neural networks, have been recently shown to satisfy the PL condition. We argue that the PL condition provides a relevant and attractive setting for many machine learning problems, particularly in the over-parametrized regime.  \n          </dd> \n        </dl>\n\n</li> \n\n<li>\n        <b> Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate</b>\n       [<a href=\"https://arxiv.org/abs/1806.05161\">arxiv</a>]<br>\n Mikhail Belkin, Daniel Hsu, Partha Mitra, Neural Inf. Proc. Systems (NeurIPS) 2018. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for \"overfitted\" / interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is robust even when the data contain large amounts of label noise.\nVery little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and weighted k-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. These schemes have an inductive bias that benefits from higher dimension, a kind of \"blessing of dimensionality\". Finally, connections to kernel machines, random forests, and adversarial examples in the interpolated regime are discussed. \n          </dd> \n        </dl>\n\n</li> \n<li>\n        <b> Kernel machines that adapt to GPUs for effective large batch training</b> [<a href=\"https://arxiv.org/abs/1806.06144\">arxiv</a>, <a href=\"https://github.com/EigenPro/EigenPro2\">EigenPro2.0 code</a>]<br>\n\t\tSiyuan Ma, Mikhail Belkin, SysML 2019. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\tModern machine learning models are typically trained using Stochastic Gradient Descent (SGD) on massively parallel computing resources such as GPUs. Increasing mini-batch size is a simple and direct way to utilize the parallel computing capacity. For small batch an increase in batch size results in the proportional reduction in the training time, a phenomenon known as linear scaling. \n\tHowever, increasing batch size beyond a certain value leads to no further improvement in training time. In this paper we develop the first analytical framework that extends linear scaling to match the parallel computing capacity of a resource. The framework is designed for a class of classical kernel machines. It automatically modifies a standard kernel machine to output a mathematically equivalent prediction function, yet allowing for extended linear scaling, i.e., higher effective parallelization and faster training time on given hardware.\nThe resulting algorithms are accurate, principled and very fast. For example, using a single Titan Xp GPU, training on ImageNet with 1.3\ufffd106 data points and 1000 labels takes under an hour, while smaller datasets, such as MNIST, take seconds. As the parameters are chosen analytically, based on the theoretical bounds, little tuning beyond selecting the kernel and the kernel parameter is needed, further facilitating the practical use of these methods. \n          </dd> \n        </dl>\n\n</li> \n\n<li>\n        <b> Accelerating Stochastic Training for Over-parametrized Learning  </b>\n       [<a href=\"https://arxiv.org/abs/1810.13395\">arxiv</a>]<br>\nChaoyue Liu, Mikhail Belkin, ICLR 2020. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   In this paper we introduce MaSS (Momentum-added Stochastic Solver), an accelerated SGD method for optimizing over-parameterized networks. Our method is simple and efficient to implement and does not require changing parameters or computing full gradients in the course of optimization. We provide a detailed theoretical analysis for convergence and parameter selection including their dependence on the mini-batch size in the quadratic case. We also provide theoretical convergence results for a more general convex setting.\nWe provide an experimental evaluation showing strong performance of our method in comparison to Adam and SGD for several standard architectures of deep networks including ResNet, convolutional and fully connected networks. We also show its performance for convex kernel machines. \n          </dd> \n        </dl>\n\n</li> \n\n<li>\n        <b>   Overparameterized Neural Networks Can Implement Associative Memory</b>\n       [<a href=\"https://arxiv.org/abs/1909.12362\">arxiv</a>]<br>\nAdityanarayanan Radhakrishnan, Mikhail Belkin, Caroline Uhler.\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   Memorization of data in deep neural networks has become a subject of significant research interest. We prove that over-parameterized single layer fully connected autoencoders memorize training data: they produce outputs in (a non-linear version of) the span of the training examples. In contrast to fully connected autoencoders, we prove that depth is necessary for memorization in convolutional autoencoders. Moreover, we observe that adding nonlinearity to deep convolutional autoencoders results in a stronger form of memorization: instead of outputting points in the span of the training images, deep convolutional autoencoders tend to output individual training images. Since convolutional autoencoder components are building blocks of deep convolutional networks, we \n\t   envision that our findings will shed light on the important phenomenon of memorization in over-parameterized deep networks. \n          </dd> \n        </dl>\n\n</li> \n\n\n\n<li>\n        <b> Kernel Machines Beat Deep Neural Networks on Mask-based Single-channel Speech Enhancement  </b>\n       [<a href=\"https://arxiv.org/abs/1811.02095\">arxiv</a>]<br>\nLike Hui, Siyuan Ma, Mikhail Belkin, INTERSPEECH 2019.\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t   We apply a fast kernel method for mask-based single-channel speech enhancement. Specifically, our method solves a kernel regression problem associated to a non-smooth kernel function (exponential power kernel) with a highly efficient iterative method (EigenPro). Due to the simplicity of this method, its hyper-parameters such as kernel bandwidth can be automatically and efficiently selected using line search with subsamples of training data. We observe an empirical correlation between the regression loss (mean square error) and regular metrics for speech enhancement. This observation justifies our training target and motivates us to achieve lower regression loss by training separate kernel model per frequency subband. We compare our method with the state-of-the-art deep neural networks on mask-based HINT and TIMIT. Experimental results show that our kernel method consistently outperforms deep neural networks while requiring less training time. \n          </dd> \n        </dl>\n\n</li> \n\n\n\n\n<li>\n        <b> Does data interpolation contradict statistical optimality?</b>\n       [<a href=\"https://arxiv.org/abs/1806.09471\">arxiv</a>]<br>\n Mikhail Belkin, Alexander Rakhlin, Alexandre B. Tsybakov, AI&amp;Stats 2019. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t    We show that learning methods interpolating the training data can achieve optimal rates for the problems of nonparametric regression and prediction with square loss. \n          </dd> \n        </dl>\n\n</li> \n<li>\n        <b> To understand deep learning we need to understand kernel learning</b>\n\t\t[<a href=\"https://arxiv.org/abs/1802.01396\">arxiv</a>]<br>\n Mikhail Belkin, Siyuan Ma, Soumik Mandal, ICML 2018. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t\t  Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly.  Despite this overfitting, they perform well on test data, a  phenomenon  not yet fully understood.\n\nThe first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world  and two synthetic datasets, we establish experimentally that kernel classifiers trained to have zero classification error (overfitting) or  zero regression error (interpolation) perform very well on test data.  <br>\n\n\nWe proceed to prove lower bounds on the norm of overfitted solutions for smooth kernels, showing that they increase nearly exponentially with  data size.  Since most generalization bounds depend polynomially on the norm of the solution, this result implies that they  diverge  as data increases.  Furthermore, the existing bounds do not apply to interpolated classifiers.      <br>\n\n\nWe also show experimentally that (non-smooth) Laplacian kernels easily fit random labels using a version of SGD, \na finding that parallels results recently reported for ReLU neural networks. In contrast, as expected from theory, fitting noisy data requires many more epochs for smooth Gaussian kernels.  The observation that the ultimate performance of overfitted Laplacian and Gaussian classifiers on the test is quite similar, suggests that  generalization is tied to the properties of the kernel function  rather than the  optimization process. <br>\n\n\nWe see that some  key phenomena of  deep learning are manifested similarly in  kernel methods in the ``modern'' overfitted regime.  We argue that progress on understanding  deep learning will be difficult, until more analytically tractable ``shallow'' kernel methods are better understood.\nThe combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas  for understanding properties of classical kernel methods.\n          </dd> \n        </dl>\n\n</li> \n\n\n\n<li>\n        <b> The power of interpolation: understanding the effectiveness of SGD in modern over-parametrized learning</b>\n\t\t[<a href=\"https://arxiv.org/abs/1712.06559\">arxiv</a>]<br>\nSiyuan Ma, Raef Bassily, Mikhail Belkin, ICML 2018. \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t\t  Stochastic Gradient Descent (SGD) with small mini-batch is a key\ncomponent in modern large-scale machine learning. However, its\nefficiency has not been easy to analyze as most theoretical results\nrequire adaptive rates and show convergence rates far slower than that\nfor gradient descent, making computational comparisons difficult.<br>\n\nIn this paper we aim to clarify the issue of fast SGD convergence. The\nkey observation is that most modern architectures are\nover-parametrized and are trained to interpolate the data by driving\nthe empirical loss (classification and regression) close to zero.\nWhile it is still unclear why these interpolated solutions perform\nwell on test data, these regimes allow for very fast convergence of\nSGD, comparable in the number of iterations to gradient descent.<br>\n\nSpecifically, consider the setting with quadratic objective function,\nor near a minimum, where the quadratic term is dominant. We show that:\n(1) Mini-batch size 1with constant step size is optimal in terms of\ncomputations to achieve a given error. (2) There is a critical\nmini-batch size such that: (a:linear scaling) SGD iteration with\nmini-batch size m smaller than the critical size is nearly equivalent\nto m iterations of mini-batch size 1. (b:saturation) SGD iteration\nwith mini-batch larger than the critical size is nearly equivalent to\na gradient descent step. <br>\nThe critical mini-batch size can be viewed as\nthe limit for effective mini-batch parallelization. It is also nearly\nindependent of the data size, implying O(n) acceleration over GD per\nunit of computation.<br>\n\nWe give experimental evidence on real data, with the results closely\nfollowing our theoretical analyses.<br>\n\nFinally, we show how the interpolation perspective and our results fit\nwith recent developments in training deep neural networks and discuss\nconnections to adaptive rates for SGD and variance reduction.\n          </dd> \n        </dl>\n\n</li> \n\n\n\n<li>\n        <b> Approximation beats concentration? An approximation view on inference with smooth kernels </b>[<a href=\"https://arxiv.org/abs/1801.03437\">arxiv</a>]<br>\n Mikhail Belkin, COLT 2018 \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>  \n\t\t  Positive definite kernels and their associated Reproducing Kernel Hilbert Spaces provide a mathematically compelling and practically competitive framework for learning from data.\nIn this paper we take the approximation theory point of view to explore various aspects of smooth kernels related to their inferential properties. We analyze eigenvalue decay of kernels operators and matrices, properties of eigenfunctions/eigenvectors and \"Fourier\" coefficients of functions in the kernel space restricted to a discrete set of data points. We also investigate the fitting capacity of kernels, giving explicit bounds on the fat shattering dimension of the balls in Reproducing Kernel Hilbert spaces. Interestingly, the same properties that make kernels very effective approximators for functions in their \"native\" kernel space, also limit their capacity to represent arbitrary functions. We discuss various implications, including those for gradient descent type methods.\n<br>\nIt is important to note that most of our bounds are measure independent. Moreover, at least in moderate dimension, the bounds for eigenvalues are much tighter than the bounds which can be obtained from the usual matrix concentration results. For example, we see that the eigenvalues of kernel matrices show nearly exponential decay with constants depending only on the kernel and the domain. We call this \"approximation beats concentration\" phenomenon as even when the data are sampled from a probability distribution, some of their aspects are better understood in terms of approximation theory. \n\t\t  \n\t\t  \n\t\t  \n          </dd> \n        </dl>\n\n</li> \n\n<li>\n        <b> Unperturbed: spectral analysis beyond Davis-Kahan </b> [<a href=\"https://arxiv.org/abs/1706.06516\">arxiv</a>]<br>\n         Justin Eldridge, Mikhail Belkin, Yusu Wang, ALT 2018<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>Classical matrix perturbation results, such as Weyl's theorem for eigenvalues and the Davis-Kahan theorem for eigenvectors, are general purpose. These classical bounds are tight in the worst case, but in many settings sub-optimal in the typical case. In this paper, we present perturbation bounds which consider the nature of the perturbation and its interaction with the unperturbed structure in order to obtain significant improvements over the classical theory in many scenarios, such as when the perturbation is random. We demonstrate the utility of these new results by analyzing perturbations in the stochastic blockmodel where we derive much tighter bounds than provided by the classical theory. We use our new perturbation theory to show that a very simple and natural clustering algorithm -- whose analysis was difficult using the classical tools -- nevertheless recovers the communities of the blockmodel exactly even in very sparse graphs. \n          </dd> \n        </dl>\n</li> \n\n\n<li>\n        <b> Eigenvectors of Orthogonally Decomposable Functions </b> [<a href=\"http://arxiv.org/abs/1411.1420\">arxiv</a>]<br>\n        Mikhail Belkin, Luis Rademacher, James Voss,  Siam Journal on Computing (SICOMP), 2018,<br> short version COLT 2016 (Learning a Hidden Basis Through Imperfect Measurements: An Algorithmic Primitive)<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t   n this paper, we generalize the eigendecomposition of quadratic forms (symmetric matrices) to a broad class of \"orthogonally decomposable\" functions. We focus on extending two characterizations of eigenvectors: First, that the eigenvectors of a quadratic form arise from the optima structure of the quadratic form on the sphere, and second that the eigenvectors are the fixed points of the matrix power iteration. We identify a key role of convexity in extending these characterizations to our setting. The generalized power iteration is a simple first order method which we call gradient iteration. Further, our framework captures as special cases recent methods for inferential problems in machine learning in areas including orthogonal tensor decompositions, Independent Component Analysis (ICA), topic modeling, spectral clustering, and Gaussian mixture learning.\nWe provide a complete theoretical analysis of gradient iteration using the structure theory of discrete dynamical systems to show almost sure convergence and fast (super-linear) convergence rates. The analysis extends to the case when the observed function is only approximately orthogonally decomposable, with bounds that are polynomial in dimension and other relevant parameters, such as perturbation size. Our perturbation results can be considered as a non-linear version of the classical Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. \n          </dd> \n        </dl>\n</li> \n\n\n\n\n<li>\n        <b> Diving into the shallows: a computational perspective on large-scale shallow learning </b> [<a href=\"https://arxiv.org/abs/1703.10622\">arxiv</a>, <a href=\"https://github.com/EigenPro\">EigenPro code (Keras/Matlab)</a>]<br>\n         Siyuan Ma, Mikhail Belkin, NIPS 2017 (spotlight, 5% of submissions).<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd> In this paper we first identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. An analysis based on the spectral properties of the kernel demonstrates that only a vanishingly small portion of the function space is reachable after a polynomial number of gradient descent iterations. This lack of approximating power drastically limits gradient descent for a fixed computational budget leading to serious over-regularization/underfitting. The issue is purely algorithmic, persisting even in the limit of infinite data.\nTo address this shortcoming in practice, we introduce EigenPro iteration, based on a preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a new kernel optimized for gradient descent. It turns out that injecting this small (computationally inexpensive and SGD-compatible) amount of approximate second-order information leads to major improvements in convergence. For large data, this translates into significant performance boost over the standard kernel methods. In particular, we are able to consistently match or improve the state-of-the-art results recently reported in the literature with a small fraction of their computational budget.\nFinally, we feel that these results show a need for a broader computational perspective on modern large-scale learning to complement more traditional statistical and convergence analyses. In particular, many phenomena of large-scale high-dimensional inference are best understood in terms of optimization on infinite dimensional Hilbert spaces, where standard algorithms can sometimes have properties at odds with finite-dimensional intuition. A systematic analysis concentrating on the approximation power of such algorithms within a budget of computation may lead to progress both in theory and practice. \n          </dd> \n        </dl>\n</li> \n\n\n\n<li>\n        <b> Graphons, mergeons, and so on! </b> [<a href=\"https://arxiv.org/abs/1607.01718\">arxiv, </a> <a href=\"https://youtu.be/ZaggyzY02lY\">3-min pre-NIPS video</a>, <a href=\"https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Graphons-mergeons-and-so-on\">NIPS video</a>]<br>\n        Justin Eldridge, Mikhail Belkin, Yusu Wang, NIPS 2016 (oral presentation, 2% of submissions)<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t  In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the \"correct\" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties. \n          </dd> \n        </dl>\n</li> \n\n<li>\n        <b> Clustering with Bregman Divergences: an Asymptotic Analysis </b> [<a href=\"http://papers.nips.cc/paper/by-source-2016-1227\">link</a>]<br>\n        Chaoyue Liu, Mikhail Belkin, NIPS 2016 <br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\nClustering, in particular k-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of k-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters k is large. We establish quantization rates and describe the limiting distribution of the centers as k tends to infinity, extending well-known results for k-means clustering.\n          </dd> \n        </dl>\n</li> \n\n<li>\n        <b> Back to the future: Radial Basis Function networks revisited </b>[<a href=\"http://jmlr.org/proceedings/papers/v51/que16.html\">link</a>] <br>\n        Qichao Que, Mikhail Belkin, AI &amp; Statistics 2016.<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\tRadial Basis Function (RBF) networks are a classical family of algorithms for supervised learning. The most popular approach for training RBF networks has relied on kernel methods using regularization based on a norm in a Reproducing Kernel Hilbert Space (RKHS), which is a principled and empirically successful framework. In this paper we aim to revisit some of the older approaches to training the RBF networks from a more modern perspective. Specifically, we analyze two common regularization procedures, one based on the square norm of the coefficients in the network and another on using centers obtained by k-means clustering. We show that both of these RBF methods can be recast as certain data-dependent kernels. We provide a theoretical analysis of these methods as well as a number of experimental results, pointing out very competitive experimental performance as well as certain advantages over the standard kernel methods in terms of both flexibility (incorporating of unlabeled data) and computational complexity. Finally, our results shed light on some impressive recent successes of using soft k-means features for image recognition and other tasks. \n          </dd> \n        </dl>\n</li> \n\n\n<li>\n        <b> The Hidden Convexity of Spectral Clustering </b> [<a href=\"http://arxiv.org/abs/1403.0667\">arxiv</a>]<br>\n        James Voss, Mikhail Belkin, Luis Rademacher, AAAI 2016 (oral presentation)<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t  In recent years, spectral clustering has become a standard method for data analysis used in a broad range of applications. In this paper we propose a new class of algorithms for multiway spectral clustering based on optimization of a certain \"contrast function\" over a sphere. These algorithms are simple to implement, efficient and, unlike most of the existing algorithms for multiclass spectral clustering, are not initialization-dependent. Moreover, they are applicable without modification for normalized and un-normalized clustering, which are two common variants of spectral clustering.\nGeometrically, the proposed algorithms can be interpreted as recovering a discrete weighted simplex by means of function optimization. We give complete necessary and sufficient conditions on contrast functions for the optimization to guarantee recovery of clusters. We show how these conditions can be interpreted in terms of certain \"hidden convexity\" of optimization over a sphere. \n          </dd> \n        </dl>\n</li> \n\n<li>\n        <b> Learning Privately from Multiparty Data </b> [<a href=\"http://arxiv.org/abs/1602.03552\">arxiv</a>]<br>\n        Jihun Hamm, Paul Cao, Mikhail Belkin, ICML 2016<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t  Learning a classifier from private data collected by multiple parties is an important problem that has many potential applications. How can we build an accurate and differentially private global classifier by combining locally-trained classifiers from different parties, without access to any party's private data? We propose to transfer the `knowledge' of the local classifier ensemble by first creating labeled data from auxiliary unlabeled data, and then train a global \u03b5-differentially private classifier. We show that majority voting is too sensitive and therefore propose a new risk weighted by class probabilities estimated from the ensemble. Relative to a non-private solution, our private solution has a generalization error bounded by O(\u03b5^2M^2) where M is the number of parties. This allows strong privacy without performance loss when M is large, such as in crowdsensing applications. We demonstrate the performance of our method with realistic tasks of activity recognition, network intrusion detection, and malicious URL detection. \n          </dd> \n        </dl>\n</li> \n\n<li>\n        <b>A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA  </b>[<a href=\"http://arxiv.org/abs/1502.04148\">link</a>]<br>\n        James Voss, Mikhail Belkin, Luis Rademacher, NIPS 2015 <br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+abstract</span> <font color=\"blue\"> </font>\n          </dt>\n          <dd>\n\t\t  Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite \"inner product\") space. The use of this indefinite \"inner product\" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing.\nOur second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise.\n          </dd> \n        </dl>\n</li> \n\n\n<li>\n<b>Polynomial Learning of Distribution Families</b> [<a href=\"http://epubs.siam.org/doi/abs/10.1137/13090818X\">link</a>]<br>\n\t\t\t\tM. Belkin, K. Sinha, Siam Journal on Computing (SICOMP),44(4), 889-911, 2015.<br> (Short version FOCS 2010).\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n           <span>+ abstract</span>\n          </dt>\n          <dd>\n\t\t\t\tThe question of polynomial learnability of probability distributions, particularly Gaussian mixture distributions, has recently received significant attention in theoretical computer science and machine learning. However, despite major progress, the general question of polynomial learnability of Gaussian mixture distributions still remained open. The current work resolves the question of polynomial learnability for Gaussian mixtures in high dimension with an arbitrary fixed number of components. The result on learning Gaussian mixtures relies on an analysis of distributions belonging to what we call \"polynomial families\" in low dimension. These families are characterized by their moments being polynomial in parameters and include almost all common probability distributions as well as their mixtures and products. Using tools from real algebraic geometry, we show that parameters of any distribution belonging to such a family can be learned in polynomial time and using a polynomial number of sample points. The result on learning polynomial families is quite general and is of independent interest. To estimate parameters of a Gaussian mixture distribution in high dimensions, we provide a deterministic algorithm for dimensionality reduction. This allows us to reduce learning a high-dimensional mixture to a polynomial number of parameter estimations in low dimension. Combining this reduction with the results on polynomial families yields our result on learning arbitrary Gaussian mixtures in high dimensions. \n        </dd> \n        </dl>\n</li>    \n\n<li>\n        <b>Beyond Hartigan Consistency: Merge Distortion Metric for\nHierarchical Clustering  </b>[<a href=\"http://www.jmlr.org/proceedings/papers/v40/Eldridge15.pdf\">link</a>]<br>\n        Justin Eldridge, Mikhail Belkin, Yusu Wang, COLT 2015, <font color=\"red\">Mark Fulk award (best student paper)!</font><br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t  Hierarchical clustering is a popular method for analyzing data which associates a tree to a\ndataset. Hartigan consistency has been used extensively as a framework to analyze such clustering\nalgorithms from a statistical point of view. Still, as we show in the paper, a tree which is Hartigan\nconsistent with a given density can look very different than the correct limit tree. Specifically, Hartigan\nconsistency permits two types of undesirable configurations which we term over-segmentation\nand improper nesting. Moreover, Hartigan consistency is a limit property and does not directly\nquantify difference between trees.\nIn this paper we identify two limit properties, separation and minimality, which address both\nover-segmentation and improper nesting and together imply (but are not implied by) Hartigan consistency.\nWe proceed to introduce a merge distortion metric between hierarchical clusterings and\nshow that convergence in our distance implies both separation and minimality. We also prove that\nuniform separation and minimality imply convergence in the merge distortion metric. Furthermore,\nwe show that our merge distortion metric is stable under perturbations of the density.\nFinally, we demonstrate applicability of these concepts by proving convergence results for two\nclustering algorithms. First, we show convergence (and hence separation and minimality) of the\nrecent robust single linkage algorithm of Chaudhuri and Dasgupta (2010). Second, we provide\nconvergence results on manifolds for topological split tree clustering.\n          </dd> \n        </dl>\n</li> \n\n<li>\n        <b> Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart Devices </b><br>\n        J. Hamm, A. Champion, G. Chen, M. Belkin, and D. Xuan, ICDCS 2015<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t Smart devices with built-in sensors, computational\ncapabilities, and network connectivity have become increasingly\npervasive. Crowds of smart devices offer opportunities to collec-\ntively sense and perform computing tasks at an unprecedented\nscale. This paper presents Crowd-ML, a privacy-preserving\nmachine learning framework for a crowd of smart devices, which\ncan solve a wide range of learning problems for crowdsensing\ndata with differential privacy guarantees. Crowd-ML endows\na crowdsensing system with the ability to learn classifiers or\npredictors online from crowdsensing data privately with minimal\ncomputational overhead on devices and servers, suitable for\npractical large-scale use of the framework. We analyze the\nperformance and scalability of Crowd-ML and implement the\nsystem with off-the-shelf smartphones as a proof of concept.\nWe demonstrate the advantages of Crowd-ML with real and\nsimulated experiments under various conditions.\n          </dd> \n        </dl>\n</li> \n\n\n<li>\n        <b> Learning with Fredholm Kernels </b>[<a href=\"https://papers.nips.cc/paper/5237-learning-with-fredholm-kernels.pdf\">link</a>]<br>\n        Qichao Que, Mikhail Belkin, Yusu Wang, NIPS 2014<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n\t\t  In this paper we propose a framework for supervised and semi-supervised learning\nbased on reformulating the learning problem as a regularized Fredholm integral\nequation. Our approach fits naturally into the kernel framework and can be in-\nterpreted as constructing new data-dependent kernels, which we call Fredholm\nkernels. We proceed to discuss the \ufffdnoise assumption\ufffd for semi-supervised learn-\ning and provide both theoretical and experimental evidence that Fredholm kernels\ncan effectively utilize unlabeled data under the noise assumption. We demonstrate\nthat methods based on Fredholm learning show very competitive performance in\nthe standard semi-supervised learning setting. \n          </dd> \n        </dl>\n</li> \n\n\n\n<li>\n        <b> The More, the Merrier: the Blessing of Dimensionality for Learning Large Gaussian Mixtures. \n        </b> [<a href=\"http://arxiv.org/abs/1311.2891\">arxiv</a>]<br>\n        Joseph Anderson, Mikhail Belkin, Navin Goyal, Luis Rademacher, James Voss, COLT 2014<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n       In this paper we show that very large mixtures of Gaussians with known and identical covariance matrix are efficiently learnable in high dimension. More precisely, we prove that a mixture whose number of components is a polynomial of any fixed degree in the dimension n is polynomially learnable as long as a certain non-degeneracy condition on the means is satisfied. It turns out that this condition is generic in the sense of smoothed complexity, as soon as the dimensionality of the space is high enough. Moreover, we prove that no such condition can exist in low dimension. Our main result on mixture recovery relies on a new \"Poissonization\"-based technique, which transforms a mixture of Gaussian to a projection of a product distribution. The problem of learning the projection can be efficiently solved using some recent results on tensor decompositions, and this gives an efficient algorithm for learning the mixture.\nWhile our results require fixed known covariance matrix, we believe that this work is among the first steps toward better understanding the rare phenomenon of the \"blessing of dimensionality\" in the computational aspects of statistical inference. \n          </dd> \n        </dl>\n</li>\n\n<li>\n        <b> Fast Algorithms for Gaussian Noise Invariant Independent Component Analysis.  \n        </b>  [<a href=\"http://papers.nips.cc/paper/5134-fast-algorithms-for-gaussian-noise-invariant-independent-component-analysis\">NIPS archive</a>, <a href=\"http://sourceforge.net/projects/giica/\">GI-ICA code</a>]<br>\n        James Voss, Luis Rademacher, Mikhail Belkin, NIPS 2013<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n       The performance of standard algorithms for Independent Component Analysis quickly deteriorates under the addition of Gaussian noise. \n\t   This is partially due to a common first step that typically consists of whitening, i.e., applying Principal Component Analysis (PCA) and \n\t   rescaling the components to have identity covariance, which is not invariant under Gaussian noise. In our paper we develop the first practical \n\t   algorithm for Independent Component Analysis that is provably invariant under Gaussian noise. The two main contributions of this work are as follows: \n\t   1. We develop and implement a more efficient version of a Gaussian noise invariant decorrelation (quasi-orthogonalization) algorithm using Hessians of the cumulant functions. 2. We propose a very simple and efficient fixed-point GI-ICA (Gradient Iteration ICA) algorithm, which is compatible with quasi-orthogonalization, as well as with the usual PCA-based whitening in the noiseless case. The algorithm is based on a special form of gradient iteration (different from gradient descent). We provide an analysis of our algorithm demonstrating fast convergence following from the basic properties of cumulants. We also present a number of experimental comparisons with the existing methods, \n\t   showing superior results on noisy data and very competitive performance in the noiseless case.\n          </dd> \n        </dl>\n</li>\n<li>\n        <b> Inverse Density as an Inverse Problem: The Fredholm Equation Approach\n        </b> [<a href=\"http://arxiv.org/abs/1304.5575v2\">arxiv</a>]<br>\n        Qichao Que, Mikhail Belkin, NIPS 2013<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n        In this paper we address the problem of estimating the ratio $\\frac{q}{p}$ where $p$ is a density function and $q$ is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration, in particular, when one needs to average a function with respect to one probability distribution, given a sample from another. It is often referred as {\\it importance sampling} in statistical inference and is also closely related to the problem of {\\it covariate shift} in transfer learning as well as to various MCMC methods. It may also be useful for separating the underlying geometry of a space, say a manifold, from the density function defined on it.\nOur approach is based on reformulating the problem of estimating $\\frac{q}{p}$ as an inverse problem in terms of an integral operator corresponding to a kernel, and thus reducing it to an integral equation, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization and kernel methods, leads to a principled kernel-based framework for constructing algorithms and for analyzing them theoretically.\nThe resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement.\nWe provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel in the case of densities defined on $\\R^d$, compact domains in $\\R^d$ and smooth $d$-dimensional sub-manifolds of the Euclidean space.\nWe also show experimental results including applications to classification and semi-supervised learning within the covariate shift framework and demonstrate some encouraging experimental comparisons. We also show how the parameters of our algorithms can be chosen in a completely unsupervised manner. \n          </dd> \n        </dl>\n</li>\n\n<li>\n        <b> Blind Signal Separation in the Presence of Gaussian Noise\n        </b> [<a href=\"http://arxiv.org/abs/1211.1716\">arxiv</a>]<br>\n        Mikhail Belkin, Luis Rademacher, James Voss, COLT 2013<br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n        A prototypical blind signal separation problem is the so-called cocktail party problem, with n people talking simultaneously and n different microphones within a room. The goal is to recover each speech signal from the microphone inputs. Mathematically this can be modeled by assuming that we are given samples from a n-dimensional random variable X=AS, where S is a vector whose coordinates are independent random variables corresponding to each speaker. The objective is to recover the matrix A^{-1} given random samples from X. A range of techniques collectively known as Independent Component Analysis (ICA) have been proposed to address this problem in the signal processing and machine learning literature. Many of these techniques are based on using the kurtosis or other cumulants to recover the components.\nIn this paper we propose a new algorithm for solving the blind signal separation problem in the presence of additive Gaussian noise, when we are given samples from X=AS + \\eta, where {\\eta} is drawn from an unknown n-dimensional Gaussian distribution. Our approach is based on a method for decorrelating a sample with additive Gaussian noise under the assumption that the underlying distribution is a linear transformation of a distribution with independent components. Our decorrelation routine is based on the properties of cumulant tensors and can be combined with any standard cumulant-based method for ICA to get an algorithm that is provably robust in the presence of Gaussian noise. We derive polynomial bounds for sample complexity and error propagation of our method.\nOur results generalize the recent work of Arora et al. which deals with a special case of ICA when S is the uniform probability distribution over the binary cube. \n          </dd> \n        </dl>\n</li> \n\t\n<li>\n<b>Graph Laplacians on Singular Manifolds: Toward understanding complex spaces: graph Laplacians on manifolds with singularities and boundaries</b> [<a href=\"http://arxiv.org/abs/1211.6727\">arxiv</a>]<br>\n\t\t\t\tMikhail Belkin, Qichao Que, Yusu Wang, Xueyuan Zhou, COLT 2012.\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n           <span>+ abstract</span>\n          </dt>\n          <dd>\n\t\t\tRecently, much of the existing work in manifold learning has been done under the assumption that the data is sampled from a manifold without boundaries and singularities or that the functions of interest are evaluated away from such points. At the same time, it can be argued that singularities and boundaries are an important aspect of the geometry of realistic data.\nIn this paper we consider the behavior of graph Laplacians at points at or near boundaries and two main types of other singularities: intersections, where different manifolds come together and sharp \"edges\", where a manifold sharply changes direction. We show that the behavior of graph Laplacian near these singularities is quite different from that in the interior of the manifolds. In fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of Fourier series, can be observed in the behavior of graph Laplacian near such points. Unlike in the interior of the domain, where graph Laplacian converges to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a first-order differential operator, which exhibits different scaling behavior as a function of the kernel width. One important implication is that while points near the singularities occupy only a small part of the total volume, the difference in scaling results in a disproportionately large contribution to the total behavior. Another significant finding is that while the scaling behavior of the operator is the same near different types of singularities, they are very distinct at a more refined level of analysis.\nWe believe that a comprehensive understanding of these structures in addition to the standard case of a smooth manifold can take us a long way toward better methods for analysis of complex non-linear data and can lead to significant progress in algorithm design. \t\n        </dd> \n        </dl>\n</li>    \t \n\n\n\t\n<li>\n        <b> Data Skeletonization via Reeb Graphs \n        </b> [<a href=\"http://books.nips.cc/papers/files/nips24/NIPS2011_0559.pdf\">pdf</a>]<br>\n        X. Ge, I. Safa, M. Belkin, Y. Wang, NIPS 2011. <br> \n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n        Recovering hidden structure from complex and noisy non-linear data is one of the most fundamental problems in machine learning and statistical inference. While such data is often high-dimensional, it is of interest to approximate it with a low-dimensional or even one-dimensional space, since many important aspects of data are often intrinsically low-dimensional. Furthermore, there are many scenarios where the underlying structure is graph-like, e.g, river/road networks or various trajectories. In this paper, we develop a framework to extract, as well as to simplify, a one-dimensional \"skeleton\" from unorganized data using the Reeb graph. Our algorithm is very simple, does not require complex optimizations and can be easily applied to unorganized high-dimensional data such as point clouds or proximity graphs. It can also represent arbitrary graph structures in the data. We also give theoretical results to justify our method. We provide a number of experiments to demonstrate the effectiveness and generality of our algorithm, including comparisons to existing methods, such as principal curves. We believe that the simplicity and practicality of our algorithm will help to promote skeleton graphs as a data analysis tool for a broad range of applications. \n          </dd> \n        </dl>\n</li> \n\t \n\n\n <li>\n       <b>Convergence of Laplacian Eigenmaps</b> [<a href=\"./papers/CLEM_08.pdf\">pdf</a>, <a href=\"./papers/bib.html#CLEM_08\">bib</a>]<br>\n        M. Belkin, P. Niyogi<br>\n        preprint, short version NIPS 2008.<br>\t \t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd> \n  Geometrically based methods for various tasks of data analysis\n  have attracted considerable attention over the last few years.\n  In many of these algorithms, a central role is played by the\n  eigenvectors of the graph Laplacian of a \n  data-derived graph. In this paper, we show that if points are\n  sampled uniformly at random from an unknown submanifold ${\\cal M}$\n  of $\\R^N$, then the eigenvectors of a suitably constructed\n  graph Laplacian converge to the eigenfunctions of the Laplace\n  Beltrami operator on ${\\cal M}$. This basic result directly\n  establishes the convergence of spectral manifold learning\n  algorithms such as Laplacian Eigenmaps and Diffusion Maps.\n  It also has implications for the understanding of geometric algorithms\n  in data analysis, computational harmonic analysis, geometric\n  random graphs, and graphics.\n \t\t\t\t </dd>\n        </dl>\n\t\t\t\t</li>\n<li>\n        <b> On Learning with Integral Operators\n        </b> [<a href=\"./papers/LIO_JMLR_10.pdf\">pdf</a>, <a href=\"./papers/bib.html#LIO_JMLR_10\">bib</a>]<br>\n        L. Rosasco, M.Belkin, E. De Vito,<br> Journal of Machine Learning Research, vol.11, pp.905-934, 2010.\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n        A large number of learning algorithms, for example,\n\t\t\t\tspectral clustering, kernel Principal Components\n\t\t\t\tAnalysis and many manifold methods are\n\t\t\t\tbased on estimating eigenvalues and eigenfunctions\n\t\t\t\tof operators defined by a similarity function or a\n\t\t\t\tkernel, given empirical data. Thus for the analysis\n\t\t\t\tof algorithms, it is an important problem to\n\t\t\t\tbe able to assess the quality of such approximations.\n\t\t\t\tThe contribution of our paper is two-fold.\n\t\t\t\tFirst, we use a technique based on a concentration\n\t\t\t\tinequality for Hilbert spaces to provide new simplified\n\t\t\t\tproofs for a number of results in spectral\n\t\t\t\tapproximation. Second, using these methods we\n\t\t\t\tprovide several new results for estimating spectral\n\t\t\t\tproperties of the graph Laplacian operator extending\n\t\t\t\tand strengthening results from [28].\n          </dd> \n        </dl>\n      </li>\n\n\t\t\t<li>\n <b>Data spectroscopy: eigenspaces of convolution operators and clustering</b> [<a href=\"./papers/DS_AOS_09.pdf\">pdf</a>, <a href=\"bib.html#DS_AOS_09\">bib</a>]<br>\n        Tao Shi, Mikhail Belkin, Bin Yu<br>\n        The Annals of Statistics, vol. 37, Number 6B (2009), 3960-3984. <br>\n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            This paper focuses on obtaining clustering information about a\ndistribution from its i.i.d. samples. We develop theoretical results to\nunderstand and use clustering information contained in the eigenvectors\nof data adjacency matrices based on a radial kernel function\nwith a sufficiently fast tail decay. In particular, we provide population\nanalyses to gain insights into which eigenvectors should be used and\nwhen the clustering information for the distribution can be recovered\nfrom the sample. We learn that a fixed number of top eigenvectors\nmight at the same time contain redundant clustering information and\nmiss relevant clustering information. We use this insight to design\nthe Data Spectroscopic clustering (DaSpec) algorithm that utilizes\nproperly selected eigenvectors to determine the number of clusters\nautomatically and to group the data accordingly. Our findings extend\nthe intuitions underlying existing spectral techniques such as\nspectral clustering and Kernel Principal Components Analysis, and\nprovide new understanding into their usability and modes of failure.\nSimulation studies and experiments on real world data are conducted\nto show the potential of our algorithm. In particular, DaSpec is found\nto handle unbalanced groups and recover clusters of different shapes\nbetter than the competing methods.\n          </dd>\n        </dl>\n      </li>\n<li><b>Towards a Theoretical Foundation for Laplacian-Based\n        Manifold Methods</b> [<a href=\"./papers/TT_JCSS_08.pdf\">pdf</a>, <a href=\"./papers/bib.html#TT_JCSS_08\">bib</a>]<br>\n        M. Belkin, P. Niyogi<br>\n        Journal of Computer and System Sciences, 2008. <br>\n\t\t\t\tVolume 74, Issue 8, pp. 1289-1308.\n        Special Issue on Learning Theory, invited (short version in COLT 2005).<br>\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            In recent years manifold methods have attracted a considerable amount of atten-\ntion in machine learning. However most algorithms in that class may be termed\n\"manifold-motivated\" as they lack any explicit theoretical guarantees. In this pa-\nper we take a step towards closing the gap between theory and practice for a class\nof Laplacian-based manifold methods. These methods utilize the graph Laplacian\nassociated to a data set for a variety of applications in semi-supervised learning,\nclustering, data representation.\nWe show that under certain conditions the graph Laplacian of a point cloud of\ndata samples converges to the Laplace-Beltrami operator on the underlying mani-\nfold. Theorem 3.1 contains the .rst result showing convergence of a random graph\nLaplacian to the manifold Laplacian in the context of machine learning.\n          </dd>\n        </dl>\n      </li>\t\t\t\n\t\t\t\t<li>\n      <b> Discrete Laplace Operator for Meshed Surfaces \n        </b> [<a href=\"./papers/DLMS_SOCG_08.pdf\">pdf</a>, <a href=\"http://www.cs.princeton.edu/~jiansun/software/meshlp.html\">code</a>, <a href=\"./papers/bib.html#DLMS_SOCG_08\">bib</a>]<br>\n        M. Belkin, J. Sun, Y. Wang, 24th Annual Symposium on Computational Geometry (SOCG) 2008.\n\t\t\t\t<dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            In recent years a considerable amount of work in graphics and geometric optimization used tools\nbased on the Laplace-Beltrami operator on a surface. The applications of the Laplacian include mesh\nediting, surface smoothing, and shape interpolations among others. However, it has been shown [12, 23,\n25] that the popular cotangent approximation schemes do not provide convergent point-wise (or even\nL^2) estimates, while many applications rely on point-wise estimation. Existence of such schemes has\nbeen an open question [12].\nIn this paper we propose the first algorithm for approximating the Laplace operator of a surface from\na mesh with point-wise convergence guarantees applicable to arbitrary meshed surfaces. We show that\nfor a sufficiently fine mesh over an arbitrary surface, our mesh Laplacian is close to the Laplace-Beltrami\noperator on the surface at every point of the surface.\nMoreover, the proposed algorithm is simple and easily implementable. Experimental evidence shows\nthat our algorithm exhibits convergence empirically and outperforms cotangent-based methods in providing\naccurate approximation of the Laplace operator for various meshes.\n          </dd> \n        </dl>\n      </li>\n\t\t\n\t\t\n\t\t<li>\n        <b>Consistency of Spectral Clustering</b> [<a href=\"./papers/SC_AOS_08.pdf\">pdf</a>, <a href=\"./papers/bib.html#SC_AOS_08\">bib</a>]<br>\n        U. von Luxburg, M. Belkin, O. Bousquet,<br>\n        The Annals of Statistics 2008, Vol. 36, No. 2, 555-586. <br>\n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            Consistency is a key property of statistical\n            algorithms when the data is drawn from some\n            underlying probability distribution. Surprisingly,\n            despite decades of work, little is known about\n            consistency of most clustering algorithms. In this\n            paper we investigate consistency of the popular\n            family of spectral clustering algorithms, which\n            clusters the data with the help of eigenvectors of\n            graph Laplacian matrices. We develop new methods to\n            establish that for increasing sample size, those\n            eigenvectors converge to the eigenvectors of certain\n            limit operators. As a result we can prove that one of\n            the two major classes of spectral clustering\n            (normalized clustering) converges under very general\n            conditions, while the other (unnormalized clustering)\n            is only consistent under strong additional\n            assumptions, which are not always satisfied in real\n            data. We conclude that our analysis provides strong\n            evidence for the superiority of normalized spectral\n            clustering.\n          </dd>\n        </dl>\n      </li>\n\t\t\t\n\t\t\n\t\t\t\n\t\t\t\t     <li>\n       <b>Manifold Regularization:\n        a Geometric Framework for Learning from Labeled and\n        Unlabeled Examples</b> [<a href=\"./papers/MR_JMLR_06.pdf\">pdf</a>, <a href=\"./papers/bib.html#MR_JMLR_06\">bib</a>]<br>\n        M. Belkin, P. Niyogi, V. Sindhwani<br>\n        Journal of Machine Learning Research, 7(Nov):2399-2434,\n        2006.<br>\t \t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd> We propose a family of learning algorithms based on a\n            new form of regularization that allows us to exploit\n            the geometry of the marginal distribution. We focus\n            on a semi-supervised framework that incorporates\n            labeled and unlabeled data in a general-purpose\n            learner. Some transductive graph learning algorithms\n            and standard methods including support vector\n            machines and regularized least squares can be\n            obtained as special cases. We use properties of\n            reproducing kernel Hilbert spaces to prove new\n            Representer theorems that provide theoretical basis\n            for the algorithms. As a result (in contrast to\n            purely graph-based approaches) we obtain a natural\n            out-of-sample extension to novel examples and so are\n            able to handle both transductive and truly\n            semi-supervised settings. We present experimental\n            evidence suggesting that our semi-supervised\n            algorithms are able to use unlabeled data\n            effectively. Finally we have a brief discussion of\n            unsupervised and fully supervised learning within our\n            general framework.\n\t\t\t\t </dd>\n        </dl>\n\t\t\t\t\n\t\t\t\t</li><li>\n        <b>Heat Flow and a Faster Algorithm to Compute the\n        Surface Area of a Convex Body</b> [<a href=\"./papers/HF_FOCS_06.pdf\">pdf</a>, <a href=\"bib.html#HF_FOCS_06\">bib</a>]<br>\n        M. Belkin, H. Narayanan, P. Niyogi, FOCS 2006.<br>\n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            We draw on the observation that the amount of heat\n            diffusing outside of a heated body in a short period\n            of time is proportional to its surface area, to\n            design a simple algorithm for approximating the\n            surface area of a convex body given by a membership\n            oracle. Our method has a complexity of O*(n^4), where\n            n is the dimension, compared to O*(n^8.5) for the\n            previous best algorithm. We show that our complexity\n            cannot be improved given the current state-of-the-art\n            in volume estimation.\n          </dd>\n         \n        </dl>\n      </li>\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t<li>\n        <b>Semi-supervised Learning on Riemannian Manifolds</b>\n        [<a href=\"./papers/SSL_ML_04.pdf\">pdf</a>, <a href=\"./papers/bib.html#SSL_ML_04\">bib</a>]<br>\n        M. Belkin, P. Niyogi<br>\n        Machine Learning, 56 (invited, special Issue on\n        clustering), 209-239, 2004 (short version in NIPS 2002).<br>\n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            We consider the general problem of utilizing both\n            labeled and unlabeled data to improve classification\n            accuracy. Under the assumption that the data lie on a\n            submanifold in a high dimensional space, we develop\n            an algorithmic framework to classify a partially\n            labeled data set in a principled manner. The central\n            idea of our approach is that classification functions\n            are naturally defined only on the submanifold in\n            question rather than the total ambient space. Using\n            the Laplace-Beltrami operator one produces a basis\n            (the Laplacian Eigenmaps) for a Hilbert space of\n            square integrable functions on the submanifold. To\n            recover such a basis, only unlabeled examples are\n            required. Once such a basis is obtained, training can\n            be performed using the labeled data set. Our\n            algorithm models the manifold using the adjacency\n            graph for the data and approximates the Laplace-\n            Beltrami operator by the graph Laplacian. We provide\n            details of the algorithm, its theoretical\n            justification, and several practical applications for\n            image, speech, and text classification.\n          </dd>\n        </dl>\n       </li><li>\n        <b>Laplacian Eigenmaps for Dimensionality Reduction and\n        Data Representation</b> [<a href=\"./papers/LEM_NC_03.pdf\">pdf</a>, <a href=\"./papers/bib.html#LEM_NC_03\">bib</a>]<br>\n        M. Belkin, P. Niyogi<br>\n        Neural Computation, June 2003; 15 (6):1373-1396 (short version in NIPS 2001).<br>\n        <dl class=\"togList\">\n          <dt onclick=\"tog(this)\">\n            <span>+</span> <font color=\"blue\">abstract</font>\n          </dt>\n          <dd>\n            One of the central problems in machine learning and\n            pattern recognition is to develop appropriate\n            representations for complex data. We consider the\n            problem of constructing a representation for data\n            lying on a lowdimensional manifold embedded in a\n            high-dimensional space. Drawing on the correspondence\n            between the graph Laplacian, the Laplace Beltrami\n            operator on the manifold, and the connections to the\n            heat equation, we propose a geometrically motivated\n            algorithm for representing the highdimensional data.\n            The algorithm provides a computationally efficient\n            approach to non\n\t\t\t\t\t\t dimensionality reduction that\n            has locality-preserving properties and a natural\n            connection to clustering. Some potential applications\n            and illustrative examples are discussed.\n          </dd>\n        </dl>\n      </li>\n</ul>\t\t\n</td></tr>\n\n</tbody></table>\n\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"75%\">\n<tbody><tr>\n<td style=\"text-align: left; width: 20%;\"> \n\n<b> <font color=\"#993300\"> Algorithms. </font></b> \n<p>\n <a href=\"./algorithms/algorithms.html\">Links to some implementations.</a>\n</p></td>\n\n<td valign=\"top\" style=\"text-align: left; width: 30%;\"> \n\n<b> <font color=\"#993300\"> Talks. </font></b> \n<p>\n<a href=\"talks.html\">Slides and videos for some of my talks.</a> <br>\n \n</p></td>\n</tr>\n\n</tbody></table>\n\n\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"75%\">\n<tbody><tr>\n<td style=\"text-align: left; width: 20%;\"> \n</td>\n</tr>\n\n</tbody></table>\n<table border=\"0\" cellpadding=\"10%\" cellspacing=\"10%\" width=\"65%\">\n<tbody><tr>\n<td style=\"text-align: left; width: 20%;\"> \n<font color=\"#993300\"> My wonderful wife and colleague  </font> \n<a href=\"http://www.cse.ohio-state.edu/~yusu/\">Yusu Wang</a>. <br>             \n</td>\n</tr>\n\n<tr><td style=\"text-align: left; width: 20%;\"> \nAlways thoughtful advisor and dear friend   \n<a href=\"http://parthaniyogiconference.cs.uchicago.edu/\">Partha Niyogi</a>, no longer with us... <br>  \n</td>\n\n\n\n\n</tr>\n\n</tbody></table>\n\n\n\n\n\n\n\n<!-- A nice online <a href=\"http://people.cs.uchicago.edu/%7Emrainey/jlapvis/JLapVis.html\">Java\ndemo</a> for Manifold Regularization by Mike Rainey.-->\n\n\n\n\n</p></body></html>"